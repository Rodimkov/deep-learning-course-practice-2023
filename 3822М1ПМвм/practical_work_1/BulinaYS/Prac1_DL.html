<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>68c5ef2830f04176aa6098762deeaaad</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="cjmvWEK3plPp" data-outputId="c9629deb-7f9b-4368-a50e-5a0fda10108b">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pip install torchviz</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting torchviz
  Downloading torchviz-0.0.2.tar.gz (4.9 kB)
  Preparing metadata (setup.py) ... ent already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu118)
Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.13.1)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (1.12)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.2.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1.2)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2023.6.0)
Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2.1.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-&gt;torchviz) (2.1.3)
Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch-&gt;torchviz) (1.3.0)
Building wheels for collected packages: torchviz
  Building wheel for torchviz (setup.py) ... e=torchviz-0.0.2-py3-none-any.whl size=4133 sha256=51e5626dead3bba38cbc3eff5654eee3b6fa4e88a4677cfd35f77fa1e5b058f4
  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094
Successfully built torchviz
Installing collected packages: torchviz
Successfully installed torchviz-0.0.2
</code></pre>
</div>
</div>
<div class="cell code" id="IaHduMBQoTk1">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.datasets</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plot</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchsummary <span class="im">import</span> summary</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="FdUeHhUHs0-t" data-outputId="57273d07-d63d-4429-e9c5-c500d4bc30c6">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div>
<div class="output execute_result" data-execution_count="8">
<pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre>
</div>
</div>
<div class="cell code" id="jcfSvHP3rL-X">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>CLASSES <span class="op">=</span> [<span class="st">&#39;plane&#39;</span>, <span class="st">&#39;car&#39;</span>, <span class="st">&#39;bird&#39;</span>, <span class="st">&#39;cat&#39;</span>, <span class="st">&#39;deer&#39;</span>, <span class="st">&#39;dog&#39;</span>, <span class="st">&#39;frog&#39;</span>, <span class="st">&#39;horse&#39;</span>, <span class="st">&#39;ship&#39;</span>, <span class="st">&#39;truck&#39;</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>N_CLASSES <span class="op">=</span> <span class="bu">len</span>(CLASSES)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_images(images, title):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    num_showed_imgs_x <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    num_showed_imgs_y <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plot.subplots(num_showed_imgs_y, num_showed_imgs_x, figsize <span class="op">=</span> figsize)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(title)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    plot.setp(plot.gcf().get_axes(), xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    dataiter <span class="op">=</span> <span class="bu">iter</span>(images)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> images[i][<span class="dv">0</span>].numpy().transpose(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        ax.text(<span class="dv">1</span>, <span class="dv">1</span>, CLASSES[images[i][<span class="dv">1</span>]], bbox<span class="op">=</span><span class="bu">dict</span>(fill<span class="op">=</span><span class="va">False</span>, edgecolor<span class="op">=</span><span class="st">&#39;red&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>))</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="t8-bTcbHqDSm" data-outputId="2e666ec3-8e84-4e39-f5f1-fc24bfc866f5">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="tdKE80FZqL3n" data-outputId="7efa0b26-d51b-4efc-a788-12b8edd5b09c">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_dataset[<span class="dv">6</span>][<span class="dv">0</span>].shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="6">
<pre><code>torch.Size([3, 32, 32])</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Kd72p0Tdu06A" data-outputId="9e57840c-9722-4f2d-c2c3-02c283ed30e4">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>test_dataset[<span class="dv">6</span>][<span class="dv">0</span>].shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="7">
<pre><code>torch.Size([3, 32, 32])</code></pre>
</div>
</div>
<div class="cell code" id="JhTIn62ICBE-">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvBlock(torch.nn.Sequential):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Standard convolution block with Batch normalization and activation.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                 ksize,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                 padding,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                 groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>                 use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>                 bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                 activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activate <span class="op">=</span> (activation <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_bn <span class="op">=</span> use_bn</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># with self.init_scope():</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>ksize,</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span>padding,</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span>groups)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_bn:</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bn <span class="op">=</span> torch.nn.BatchNorm2d(out_channels, eps<span class="op">=</span>bn_eps)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activate:</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activ <span class="op">=</span> activation</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv(x)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_bn:</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.bn(x)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activate:</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.activ(x)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="RWQWbY5dEJ79">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv1x1_block(in_channels,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                  out_channels,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                  stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>                  padding<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                  groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                  use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                  bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                  activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ConvBlock(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        ksize<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        stride<span class="op">=</span>stride,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        use_bn<span class="op">=</span>use_bn,</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        bn_eps<span class="op">=</span>bn_eps,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span>activation)</span></code></pre></div>
</div>
<div class="cell code" id="-mC7q7obBcZp">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv3x3_block(in_channels,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>                  out_channels,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                  stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                  padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                  groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                  use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                  bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                  activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ConvBlock(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        ksize<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        stride<span class="op">=</span>stride,</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        use_bn<span class="op">=</span>use_bn,</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        bn_eps<span class="op">=</span>bn_eps,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span>activation)</span></code></pre></div>
</div>
<div class="cell code" id="mvNPybk8AFuL">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEInitBlock(torch.nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    SENet specific initial block.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                 out_channels):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEInitBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        mid_channels <span class="op">=</span> out_channels <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>mid_channels)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> torch.nn.MaxPool2d(</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SEInitBlock&quot;)</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="Ah_OYMAndkOz">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> round_channels(channels,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                   divisor<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Round weighted channel number (make divisible operation).</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    rounded_channels <span class="op">=</span> <span class="bu">max</span>(<span class="bu">int</span>(channels <span class="op">+</span> divisor <span class="op">/</span> <span class="fl">2.0</span>) <span class="op">//</span> divisor <span class="op">*</span> divisor, divisor)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">float</span>(rounded_channels) <span class="op">&lt;</span> <span class="fl">0.9</span> <span class="op">*</span> channels:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        rounded_channels <span class="op">+=</span> divisor</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rounded_channels</span></code></pre></div>
</div>
<div class="cell code" id="ZUhqp0ehcD-I">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEBlock(torch.nn.Sequential):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Squeeze-and-Excitation block</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                 channels,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                 reduction<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                 mid_channels<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                 round_mid<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                 use_conv<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                 mid_activation<span class="op">=</span>torch.nn.ReLU(),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                 out_activation<span class="op">=</span>torch.nn.Sigmoid()):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_conv <span class="op">=</span> use_conv</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mid_channels <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            mid_channels <span class="op">=</span> channels <span class="op">//</span> reduction <span class="cf">if</span> <span class="kw">not</span> round_mid <span class="cf">else</span> round_channels(<span class="bu">float</span>(channels) <span class="op">/</span> reduction)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_conv:</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span>channels,</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span>mid_channels,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fc1 <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>                in_features<span class="op">=</span>channels,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>                out_features<span class="op">=</span>mid_channels)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activ <span class="op">=</span> mid_activation</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_conv:</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span>channels,</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fc2 <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>                in_features<span class="op">=</span>mid_channels,</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>                out_features<span class="op">=</span>channels)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> out_activation</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SEBlock&quot;)</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        avgpool <span class="op">=</span> torch.nn.AvgPool2d(kernel_size<span class="op">=</span>x.shape[<span class="dv">2</span>:])</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> avgpool(x)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.use_conv:</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> torch.reshape(w, shape<span class="op">=</span>(w.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.conv1(w) <span class="cf">if</span> <span class="va">self</span>.use_conv <span class="cf">else</span> <span class="va">self</span>.fc1(w)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.activ(w)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.conv2(w) <span class="cf">if</span> <span class="va">self</span>.use_conv <span class="cf">else</span> <span class="va">self</span>.fc2(w)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.sigmoid(w)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.use_conv:</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> torch.unsqueeze(torch.unsqueeze(w, axis<span class="op">=</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">*</span> w</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="YVTcnbE3aLta">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SENetBottleneck(torch.nn.Sequential):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    SENet bottleneck block for residual path in SENet unit.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SENetBottleneck, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        mid_channels <span class="op">=</span> out_channels <span class="op">//</span> <span class="dv">4</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> <span class="bu">int</span>(math.floor(mid_channels <span class="op">*</span> (bottleneck_width <span class="op">/</span> <span class="fl">64.0</span>)))</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        group_width <span class="op">=</span> number_of_groups <span class="op">*</span> D</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        group_width2 <span class="op">=</span> group_width <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1_block(</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>group_width2)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3_block(</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>group_width2,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>group_width,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span>number_of_groups)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1_block(</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>group_width,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SENetBottleneck&quot;)</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="BI07To1nEGjS">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SENetUnit(torch.nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>                 identity_conv3x3):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SENetUnit, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resize_identity <span class="op">=</span> (in_channels <span class="op">!=</span> out_channels) <span class="kw">or</span> (stride <span class="op">!=</span> <span class="dv">1</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.body <span class="op">=</span> SENetBottleneck(</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>            bottleneck_width<span class="op">=</span>bottleneck_width)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.se <span class="op">=</span> SEBlock(channels<span class="op">=</span>out_channels)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.resize_identity:</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> identity_conv3x3:</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.identity_conv <span class="op">=</span> conv3x3_block(</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.identity_conv <span class="op">=</span> conv1x1_block(</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activ <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.resize_identity:</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.identity_conv(x)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> x</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.body(x)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.se(x)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> identity</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activ(x)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="SFGDgCXpv86s">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySENet(torch.nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                 channels,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                 init_block_channels,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>                 in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>                 in_size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>),</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span>N_CLASSES):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MySENet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_size <span class="op">=</span> in_size</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_classes <span class="op">=</span> num_classes</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.add_module(<span class="st">&quot;init_block&quot;</span>, SEInitBlock(</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>init_block_channels))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        in_channels <span class="op">=</span> init_block_channels</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, channels_per_stage <span class="kw">in</span> <span class="bu">enumerate</span>(channels):</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            stage <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>            identity_conv3x3 <span class="op">=</span> (i <span class="op">!=</span> <span class="dv">0</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j, out_channels <span class="kw">in</span> <span class="bu">enumerate</span>(channels_per_stage):</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>                stride <span class="op">=</span> <span class="dv">2</span> <span class="cf">if</span> (j <span class="op">==</span> <span class="dv">0</span>) <span class="kw">and</span> (i <span class="op">!=</span> <span class="dv">0</span>) <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>                stage.add_module(<span class="st">&quot;unit</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(j <span class="op">+</span> <span class="dv">1</span>), SENetUnit(</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>                    number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>                    bottleneck_width<span class="op">=</span>bottleneck_width,</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>                    identity_conv3x3<span class="op">=</span>identity_conv3x3))</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>                in_channels <span class="op">=</span> out_channels</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.features.add_module(<span class="st">&quot;stage</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(i <span class="op">+</span> <span class="dv">1</span>), stage)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.add_module(<span class="st">&quot;final_pool&quot;</span>, torch.nn.AdaptiveAvgPool2d(<span class="dv">1</span>))</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output.add_module(<span class="st">&quot;dropout&quot;</span>, torch.nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>))</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output.add_module(<span class="st">&quot;fc&quot;</span>, torch.nn.Linear(</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>in_channels,</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>            out_features<span class="op">=</span>num_classes))</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_params()</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_params(<span class="va">self</span>):</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.named_modules():</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(module, torch.nn.Conv2d):</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.kaiming_uniform_(module.weight)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>                    torch.nn.init.constant_(module.bias, <span class="dv">0</span>)</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.features(x)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" id="gnr-LlF_hcxY">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_senet(blocks,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>              model_name<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>              <span class="op">**</span>kwargs):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> blocks <span class="op">==</span> <span class="dv">16</span>:</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> blocks <span class="op">==</span> <span class="dv">28</span>:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> blocks <span class="op">==</span> <span class="dv">40</span>:</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&quot;Unsupported SENet with number of blocks: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(blocks))</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    bottleneck_width <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    init_block_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    channels_per_layers <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">1024</span>]</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    channels <span class="op">=</span> [[ci] <span class="op">*</span> li <span class="cf">for</span> (ci, li) <span class="kw">in</span> <span class="bu">zip</span>(channels_per_layers, layers)]</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    net <span class="op">=</span> MySENet(</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        channels<span class="op">=</span>channels,</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        init_block_channels<span class="op">=</span>init_block_channels,</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        bottleneck_width<span class="op">=</span>bottleneck_width,</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> net</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet16(<span class="op">**</span>kwargs):</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">16</span>, model_name<span class="op">=</span><span class="st">&quot;senet16&quot;</span>)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet28(<span class="op">**</span>kwargs):</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">28</span>, model_name<span class="op">=</span><span class="st">&quot;senet28&quot;</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet40(<span class="op">**</span>kwargs):</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">40</span>, model_name<span class="op">=</span><span class="st">&quot;senet40&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" id="pxVuxNbKicZo">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span></code></pre></div>
</div>
<div class="cell code" id="f_eqcE00hcmJ">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_accuracy(data_loader, model):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    tp <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> data_loader:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.to(device)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>            n <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            tp <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tp <span class="op">/</span> n</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="n9pOQa9-pawM" data-outputId="506381d7-78ed-4d99-9ca4-d5c274cb4ab6">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">#      </span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot; </span><span class="ch">\&quot;</span><span class="st">CNN</span><span class="ch">\&quot;</span><span class="st">:&quot;</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>summary(cnn_model.to(device), input_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot; :&quot;</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>display(make_dot(cnn_model(torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>).to(device)), params<span class="op">=</span><span class="bu">dict</span>(cnn_model.named_parameters())))</span></code></pre></div>
<div class="output stream stdout">
<pre><code> &quot;CNN&quot;:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 16, 16]           1,792
       BatchNorm2d-2           [-1, 64, 16, 16]             128
              ReLU-3           [-1, 64, 16, 16]               0
              ReLU-4           [-1, 64, 16, 16]               0
              ReLU-5           [-1, 64, 16, 16]               0
              ReLU-6           [-1, 64, 16, 16]               0
              ReLU-7           [-1, 64, 16, 16]               0
              ReLU-8           [-1, 64, 16, 16]               0
            Conv2d-9           [-1, 64, 16, 16]          36,928
      BatchNorm2d-10           [-1, 64, 16, 16]             128
             ReLU-11           [-1, 64, 16, 16]               0
             ReLU-12           [-1, 64, 16, 16]               0
             ReLU-13           [-1, 64, 16, 16]               0
             ReLU-14           [-1, 64, 16, 16]               0
             ReLU-15           [-1, 64, 16, 16]               0
             ReLU-16           [-1, 64, 16, 16]               0
           Conv2d-17          [-1, 128, 16, 16]          73,856
      BatchNorm2d-18          [-1, 128, 16, 16]             256
             ReLU-19          [-1, 128, 16, 16]               0
             ReLU-20          [-1, 128, 16, 16]               0
             ReLU-21          [-1, 128, 16, 16]               0
             ReLU-22          [-1, 128, 16, 16]               0
             ReLU-23          [-1, 128, 16, 16]               0
             ReLU-24          [-1, 128, 16, 16]               0
        MaxPool2d-25            [-1, 128, 8, 8]               0
      SEInitBlock-26            [-1, 128, 8, 8]               0
           Conv2d-27            [-1, 256, 8, 8]          33,024
      BatchNorm2d-28            [-1, 256, 8, 8]             512
           Conv2d-29             [-1, 64, 8, 8]           8,256
      BatchNorm2d-30             [-1, 64, 8, 8]             128
             ReLU-31             [-1, 64, 8, 8]               0
             ReLU-32             [-1, 64, 8, 8]               0
             ReLU-33             [-1, 64, 8, 8]               0
           Conv2d-34            [-1, 128, 8, 8]          73,856
      BatchNorm2d-35            [-1, 128, 8, 8]             256
             ReLU-36            [-1, 128, 8, 8]               0
             ReLU-37            [-1, 128, 8, 8]               0
             ReLU-38            [-1, 128, 8, 8]               0
             ReLU-39            [-1, 128, 8, 8]               0
             ReLU-40            [-1, 128, 8, 8]               0
             ReLU-41            [-1, 128, 8, 8]               0
           Conv2d-42            [-1, 256, 8, 8]          33,024
      BatchNorm2d-43            [-1, 256, 8, 8]             512
           Conv2d-44             [-1, 16, 1, 1]           4,112
             ReLU-45             [-1, 16, 1, 1]               0
             ReLU-46             [-1, 16, 1, 1]               0
             ReLU-47             [-1, 16, 1, 1]               0
           Conv2d-48            [-1, 256, 1, 1]           4,352
          Sigmoid-49            [-1, 256, 1, 1]               0
          Sigmoid-50            [-1, 256, 1, 1]               0
          Sigmoid-51            [-1, 256, 1, 1]               0
             ReLU-52            [-1, 256, 8, 8]               0
           Conv2d-53            [-1, 512, 4, 4]       1,180,160
      BatchNorm2d-54            [-1, 512, 4, 4]           1,024
           Conv2d-55            [-1, 128, 8, 8]          32,896
      BatchNorm2d-56            [-1, 128, 8, 8]             256
             ReLU-57            [-1, 128, 8, 8]               0
             ReLU-58            [-1, 128, 8, 8]               0
             ReLU-59            [-1, 128, 8, 8]               0
           Conv2d-60            [-1, 256, 4, 4]         295,168
      BatchNorm2d-61            [-1, 256, 4, 4]             512
             ReLU-62            [-1, 256, 4, 4]               0
             ReLU-63            [-1, 256, 4, 4]               0
             ReLU-64            [-1, 256, 4, 4]               0
             ReLU-65            [-1, 256, 4, 4]               0
             ReLU-66            [-1, 256, 4, 4]               0
             ReLU-67            [-1, 256, 4, 4]               0
           Conv2d-68            [-1, 512, 4, 4]         131,584
      BatchNorm2d-69            [-1, 512, 4, 4]           1,024
           Conv2d-70             [-1, 32, 1, 1]          16,416
             ReLU-71             [-1, 32, 1, 1]               0
             ReLU-72             [-1, 32, 1, 1]               0
             ReLU-73             [-1, 32, 1, 1]               0
           Conv2d-74            [-1, 512, 1, 1]          16,896
          Sigmoid-75            [-1, 512, 1, 1]               0
          Sigmoid-76            [-1, 512, 1, 1]               0
          Sigmoid-77            [-1, 512, 1, 1]               0
             ReLU-78            [-1, 512, 4, 4]               0
           Conv2d-79           [-1, 1024, 2, 2]       4,719,616
      BatchNorm2d-80           [-1, 1024, 2, 2]           2,048
           Conv2d-81            [-1, 256, 4, 4]         131,328
      BatchNorm2d-82            [-1, 256, 4, 4]             512
             ReLU-83            [-1, 256, 4, 4]               0
             ReLU-84            [-1, 256, 4, 4]               0
             ReLU-85            [-1, 256, 4, 4]               0
           Conv2d-86            [-1, 512, 2, 2]       1,180,160
      BatchNorm2d-87            [-1, 512, 2, 2]           1,024
             ReLU-88            [-1, 512, 2, 2]               0
             ReLU-89            [-1, 512, 2, 2]               0
             ReLU-90            [-1, 512, 2, 2]               0
             ReLU-91            [-1, 512, 2, 2]               0
             ReLU-92            [-1, 512, 2, 2]               0
             ReLU-93            [-1, 512, 2, 2]               0
           Conv2d-94           [-1, 1024, 2, 2]         525,312
      BatchNorm2d-95           [-1, 1024, 2, 2]           2,048
           Conv2d-96             [-1, 64, 1, 1]          65,600
             ReLU-97             [-1, 64, 1, 1]               0
             ReLU-98             [-1, 64, 1, 1]               0
             ReLU-99             [-1, 64, 1, 1]               0
          Conv2d-100           [-1, 1024, 1, 1]          66,560
         Sigmoid-101           [-1, 1024, 1, 1]               0
         Sigmoid-102           [-1, 1024, 1, 1]               0
         Sigmoid-103           [-1, 1024, 1, 1]               0
            ReLU-104           [-1, 1024, 2, 2]               0
AdaptiveAvgPool2d-105           [-1, 1024, 1, 1]               0
         Dropout-106                 [-1, 1024]               0
          Linear-107                   [-1, 10]          10,250
================================================================
Total params: 8,651,514
Trainable params: 8,651,514
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 6.79
Params size (MB): 33.00
Estimated Total Size (MB): 39.81
----------------------------------------------------------------
 :
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/63df40b3677df615e44398b38375e0c5f6c546e8.svg" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ajEsLLCyjYd2" data-outputId="d83b7414-5212-45f7-fba1-c06813a045a2">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6835799813270569, time = 38.34700584411621
Epoch[1]: accuracy = 0.7913399934768677, time = 37.3702495098114
Epoch[2]: accuracy = 0.8375200033187866, time = 37.933491468429565
Epoch[3]: accuracy = 0.8983599543571472, time = 37.171271324157715
Epoch[4]: accuracy = 0.9249799847602844, time = 37.8235969543457
Epoch[5]: accuracy = 0.9432799816131592, time = 37.87195301055908
Epoch[6]: accuracy = 0.9565399885177612, time = 37.62522339820862
Epoch[7]: accuracy = 0.9709799885749817, time = 37.49402189254761
Epoch[8]: accuracy = 0.9698799848556519, time = 37.93991184234619
Epoch[9]: accuracy = 0.9809999465942383, time = 37.350093603134155
Epoch[10]: accuracy = 0.9861199855804443, time = 37.438411235809326
Epoch[11]: accuracy = 0.9856599569320679, time = 37.6477108001709
Epoch[12]: accuracy = 0.9884600043296814, time = 37.56942677497864
Epoch[13]: accuracy = 0.9910999536514282, time = 37.77301216125488
Epoch[14]: accuracy = 0.9917399883270264, time = 37.387455463409424
Epoch[15]: accuracy = 0.9926999807357788, time = 37.198869705200195
Epoch[16]: accuracy = 0.9943599700927734, time = 36.868306398391724
Epoch[17]: accuracy = 0.9939199686050415, time = 37.069987058639526
Epoch[18]: accuracy = 0.9953999519348145, time = 36.89101576805115
Epoch[19]: accuracy = 0.9947800040245056, time = 37.30176520347595
train time = 1165.9577128887177
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="D2nJs1Y5kuuG" data-outputId="b0cba7bd-770e-4834-f3f5-d0acc8bca94b">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7863999605178833
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="8HUhxYrNCjUe" data-outputId="0cf162b6-2d31-448d-faa8-15f119669b17">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>pip install ray</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting ray
  Downloading ray-2.7.1-cp310-cp310-manylinux2014_x86_64.whl (62.4 MB)
 62.4/62.4 MB 12.5 MB/s eta 0:00:00
ent already satisfied: click&gt;=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.12.4)
Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.1)
Requirement already satisfied: msgpack&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.7)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (23.2)
Requirement already satisfied: protobuf!=3.19.5,&gt;=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)
Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)
Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.0)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)
Requirement already satisfied: numpy&gt;=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray) (1.23.5)
Requirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (23.1.0)
Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (2023.7.1)
Requirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (0.30.2)
Requirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (0.10.6)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (3.3.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (2.0.7)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (2023.7.22)
Installing collected packages: ray
Successfully installed ray-2.7.1
</code></pre>
</div>
</div>
<div class="cell code" id="AUV8n55OChmT">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ray</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.tune <span class="im">import</span> CLIReporter</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.tune.schedulers <span class="im">import</span> ASHAScheduler</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> filelock <span class="im">import</span> FileLock</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray <span class="im">import</span> train, tune</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.train <span class="im">import</span> Checkpoint</span></code></pre></div>
</div>
<div class="cell code" id="1ddXjr_fEiA2">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_cifar(config):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  learning_rate <span class="op">=</span> config[<span class="st">&quot;learning_rate&quot;</span>]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  batch_size <span class="op">=</span> config[<span class="st">&quot;batch_size&quot;</span>]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>      root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>      transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>  test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>      root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>      transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(&#39;Number of train samples: {}&#39;.format(len(train_dataset)))</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show_images(train_dataset, &#39;Train samples&#39;)</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(&#39;Number of test samples: {}&#39;.format(len(test_dataset)))</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show_images(test_dataset, &#39;Test samples&#39;)</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>  train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>      train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>  test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>      test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>  cnn_model<span class="op">=</span>senet16()</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>  cnn_model.to(device)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>  loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>  start_all <span class="op">=</span> time.time()</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>      start <span class="op">=</span> time.time()</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>          images <span class="op">=</span> images.to(device)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>          labels <span class="op">=</span> labels.to(device)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>          outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>          loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>          optimizer.zero_grad()</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>          loss.backward()</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>          optimizer.step()</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>      end <span class="op">=</span> time.time()</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>      accuracy <span class="op">=</span> get_accuracy(train_data_loader, cnn_model)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, accuracy, (end <span class="op">-</span> start)))</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>  end_all <span class="op">=</span> time.time()</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>  os.makedirs(<span class="st">&quot;my_model&quot;</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>  torch.save(</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>      (cnn_model.state_dict(), optimizer.state_dict()), <span class="st">&quot;my_model/checkpoint.pt&quot;</span>)</span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>  checkpoint <span class="op">=</span> Checkpoint.from_directory(<span class="st">&quot;my_model&quot;</span>)</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>  train.report({<span class="st">&quot;accuracy&quot;</span>: accuracy.item()}, checkpoint<span class="op">=</span>checkpoint)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="kwUePJ_zN4Tz" data-outputId="268ac626-3e83-4efd-9736-0e9fa3c8e6c1">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;learning_rate&quot;</span>: tune.choice([<span class="fl">1e-3</span>, <span class="fl">1e-1</span>]),</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;batch_size&quot;</span>: tune.choice([<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>])</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>max_num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ASHAScheduler(</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># metric=&quot;loss&quot;,</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mode=&quot;min&quot;,</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        max_t<span class="op">=</span>max_num_epochs,</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        grace_period<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>tuner <span class="op">=</span> tune.Tuner(</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    tune.with_resources(</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        tune.with_parameters(train_cifar),</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># resources={&quot;cpu&quot;: 2, &quot;gpu&quot;: 0}</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        resources<span class="op">=</span>{<span class="st">&quot;cpu&quot;</span>: <span class="dv">2</span>, <span class="st">&quot;gpu&quot;</span>: <span class="dv">1</span>}</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    tune_config<span class="op">=</span>tune.TuneConfig(</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        metric<span class="op">=</span><span class="st">&quot;accuracy&quot;</span>,</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">&quot;min&quot;</span>,</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        scheduler<span class="op">=</span>scheduler,</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>        num_samples<span class="op">=</span>num_samples,</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    param_space<span class="op">=</span>config,</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> tuner.fit()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>best_result <span class="op">=</span> results.get_best_result(<span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;min&quot;</span>, <span class="st">&quot;last&quot;</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial config: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(best_result .config))</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial final validation accuracy: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    best_result .last_result[<span class="st">&quot;accuracy&quot;</span>]))</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial final validation accuracy: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    best_result .last_result[<span class="st">&quot;accuracy&quot;</span>]))</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a><span class="co"># best_trained_model = cnn_model(best_trial.config[&quot;l1&quot;], best_trial.config[&quot;l2&quot;])</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="co"># best_trained_model.to(device)</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Test accuracy: {}&#39;.format(get_accuracy(test_data_loader, best_trained_model)))</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>2023-10-31 15:54:42,829	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-31 15:54:42,864	WARNING callback.py:137 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it&#39;s dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>+--------------------------------------------------------------------+
| Configuration for experiment     train_cifar_2023-10-31_15-54-42   |
+--------------------------------------------------------------------+
| Search algorithm                 BasicVariantGenerator             |
| Scheduler                        AsyncHyperBandScheduler           |
| Number of trials                 10                                |
+--------------------------------------------------------------------+

View detailed results here: /root/ray_results/train_cifar_2023-10-31_15-54-42

Trial status: 10 PENDING
Current time: 2023-10-31 15:54:43. Total running time: 0s
Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   PENDING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+

Trial train_cifar_ce6f6_00000 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00000 config         |
+----------------------------------------------+
| batch_size                                 4 |
| learning_rate                            0.1 |
+----------------------------------------------+
(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:49, 362849.74it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 675969.40it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2084945.92it/s]
  2%|         | 2719744/170498071 [00:00&lt;00:26, 6272409.30it/s]
  3%|         | 5636096/170498071 [00:00&lt;00:13, 12513233.51it/s]
  5%|         | 9273344/170498071 [00:00&lt;00:08, 19193316.41it/s]
  8%|         | 13008896/170498071 [00:00&lt;00:06, 24368777.35it/s]
 10%|         | 16744448/170498071 [00:01&lt;00:05, 28105997.70it/s]
 12%|        | 20480000/170498071 [00:01&lt;00:04, 30789564.19it/s]
 14%|        | 24084480/170498071 [00:01&lt;00:05, 27042541.53it/s]
 16%|        | 27820032/170498071 [00:01&lt;00:04, 29707288.73it/s]
 19%|        | 31555584/170498071 [00:01&lt;00:04, 31768512.66it/s]
 21%|        | 35291136/170498071 [00:01&lt;00:04, 33156681.78it/s]
 23%|       | 38993920/170498071 [00:01&lt;00:03, 34252775.22it/s]
 25%|       | 42663936/170498071 [00:01&lt;00:03, 34940719.76it/s]
 27%|       | 46432256/170498071 [00:01&lt;00:03, 35708780.86it/s]
 29%|       | 50069504/170498071 [00:02&lt;00:04, 30098587.32it/s]
 32%|      | 53772288/170498071 [00:02&lt;00:03, 31886902.60it/s]
 34%|      | 57475072/170498071 [00:02&lt;00:03, 33263436.57it/s]
 36%|      | 61014016/170498071 [00:02&lt;00:03, 33837884.73it/s]
 38%|      | 64749568/170498071 [00:02&lt;00:03, 34813460.61it/s]
 40%|      | 68452352/170498071 [00:02&lt;00:02, 35432752.67it/s]
 42%|     | 72056832/170498071 [00:02&lt;00:02, 35608333.68it/s]
 44%|     | 75661312/170498071 [00:02&lt;00:03, 30041866.34it/s]
 47%|     | 79331328/170498071 [00:02&lt;00:02, 31738001.93it/s]
 49%|     | 83165184/170498071 [00:03&lt;00:02, 33504554.62it/s]
 51%|     | 86638592/170498071 [00:03&lt;00:02, 33670223.58it/s]
 53%|    | 90112000/170498071 [00:03&lt;00:02, 33918612.01it/s]
 55%|    | 93585408/170498071 [00:03&lt;00:02, 33433299.85it/s]
 57%|    | 96993280/170498071 [00:03&lt;00:02, 33563191.04it/s]
 59%|    | 100401152/170498071 [00:03&lt;00:02, 33259433.39it/s]
 61%|    | 103776256/170498071 [00:03&lt;00:02, 29573400.60it/s]
 63%|   | 107184128/170498071 [00:03&lt;00:02, 30688039.70it/s]
 65%|   | 110559232/170498071 [00:03&lt;00:01, 31527394.86it/s]
 67%|   | 114393088/170498071 [00:04&lt;00:01, 32223817.20it/s]
 69%|   | 117866496/170498071 [00:04&lt;00:01, 32906643.40it/s]
 71%|   | 121208832/170498071 [00:04&lt;00:01, 32837610.33it/s]
 73%|  | 124911616/170498071 [00:04&lt;00:01, 34023762.52it/s]
 75%|  | 128352256/170498071 [00:04&lt;00:01, 21855036.71it/s]
 77%|  | 132055040/170498071 [00:04&lt;00:01, 25044305.51it/s]
 80%|  | 135823360/170498071 [00:04&lt;00:01, 27546352.43it/s]
 82%| | 139526144/170498071 [00:04&lt;00:01, 29872205.91it/s]
 84%| | 142966784/170498071 [00:05&lt;00:00, 31016002.71it/s]
 86%| | 146341888/170498071 [00:05&lt;00:00, 31309366.15it/s]
 88%| | 149716992/170498071 [00:05&lt;00:00, 28266610.61it/s]
 90%| | 152731648/170498071 [00:05&lt;00:00, 21611515.63it/s]
 92%|| 156794880/170498071 [00:05&lt;00:00, 25740279.80it/s]
 94%|| 160235520/170498071 [00:05&lt;00:00, 27786371.57it/s]
 96%|| 163545088/170498071 [00:05&lt;00:00, 29118796.98it/s]
 98%|| 167247872/170498071 [00:05&lt;00:00, 31191099.14it/s]
100%|| 170498071/170498071 [00:06&lt;00:00, 28310477.68it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:55:13. Total running time: 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:55:43. Total running time: 1min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:56:13. Total running time: 1min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:56:43. Total running time: 2min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:57:13. Total running time: 2min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:57:43. Total running time: 3min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:58:13. Total running time: 3min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.670479953289032, time = 132.9324929714203
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:58:43. Total running time: 4min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:59:13. Total running time: 4min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:59:43. Total running time: 5min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:00:14. Total running time: 5min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:00:44. Total running time: 6min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:01:14. Total running time: 6min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7766000032424927, time = 128.77364206314087
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:01:44. Total running time: 7min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:02:14. Total running time: 7min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:02:44. Total running time: 8min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:03:14. Total running time: 8min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:03:44. Total running time: 9min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:04:14. Total running time: 9min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.830839991569519, time = 127.71064853668213
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:04:44. Total running time: 10min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:05:14. Total running time: 10min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:05:44. Total running time: 11min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:06:14. Total running time: 11min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:06:45. Total running time: 12min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:07:15. Total running time: 12min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.874019980430603, time = 129.5284743309021
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:07:45. Total running time: 13min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:08:15. Total running time: 13min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:08:45. Total running time: 14min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:09:15. Total running time: 14min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:09:45. Total running time: 15min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:10:15. Total running time: 15min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:10:45. Total running time: 16min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9064599871635437, time = 127.27779459953308
(train_cifar pid=3909) train time = 946.7944233417511

Trial train_cifar_ce6f6_00000 completed after 1 iterations at 2023-10-31 16:10:48. Total running time: 16min 5s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00000 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 959.95732 |
| time_total_s                                     959.95732 |
| training_iteration                                       1 |
| accuracy                                           0.90646 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00001 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00001 config           |
+------------------------------------------------+
| batch_size                                  16 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:53, 359828.34it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 676505.04it/s]
  0%|          | 720896/170498071 [00:00&lt;01:24, 2011566.63it/s]
  1%|          | 1835008/170498071 [00:00&lt;00:42, 3947702.43it/s]
  3%|         | 4685824/170498071 [00:00&lt;00:15, 10458543.03it/s]
  4%|         | 7667712/170498071 [00:00&lt;00:10, 15715886.24it/s]
  6%|         | 9863168/170498071 [00:01&lt;00:10, 14822876.50it/s]
  7%|         | 12419072/170498071 [00:01&lt;00:09, 17531852.90it/s]
  9%|         | 15368192/170498071 [00:01&lt;00:07, 20713427.55it/s]
 11%|         | 18087936/170498071 [00:01&lt;00:07, 20401821.21it/s]
 12%|        | 20283392/170498071 [00:01&lt;00:07, 19566230.98it/s]
 13%|        | 22577152/170498071 [00:01&lt;00:07, 20439571.47it/s]
 15%|        | 25100288/170498071 [00:01&lt;00:06, 21734468.47it/s]
 16%|        | 27983872/170498071 [00:01&lt;00:06, 23705055.56it/s]
 18%|        | 30441472/170498071 [00:01&lt;00:06, 20879331.12it/s]
 19%|        | 32702464/170498071 [00:02&lt;00:06, 21147766.08it/s]
 21%|        | 35225600/170498071 [00:02&lt;00:06, 22160088.27it/s]
 22%|       | 38141952/170498071 [00:02&lt;00:05, 24103625.96it/s]
 24%|       | 40632320/170498071 [00:02&lt;00:06, 21232908.74it/s]
 25%|       | 42893312/170498071 [00:02&lt;00:05, 21578514.89it/s]
 27%|       | 45187072/170498071 [00:02&lt;00:05, 21874585.36it/s]
 28%|       | 47710208/170498071 [00:02&lt;00:05, 22799359.82it/s]
 30%|       | 50462720/170498071 [00:02&lt;00:05, 22523861.34it/s]
 31%|       | 52789248/170498071 [00:02&lt;00:05, 21557655.18it/s]
 32%|      | 55181312/170498071 [00:03&lt;00:05, 22004122.31it/s]
 34%|      | 57540608/170498071 [00:03&lt;00:05, 22314926.47it/s]
 35%|      | 60129280/170498071 [00:03&lt;00:04, 23281981.42it/s]
 37%|      | 62652416/170498071 [00:03&lt;00:04, 22387352.17it/s]
 38%|      | 65011712/170498071 [00:03&lt;00:04, 22640718.31it/s]
 39%|      | 67305472/170498071 [00:03&lt;00:04, 21920632.33it/s]
 41%|      | 69566464/170498071 [00:03&lt;00:04, 22098699.18it/s]
 42%|     | 71860224/170498071 [00:03&lt;00:04, 22319652.14it/s]
 44%|     | 74219520/170498071 [00:03&lt;00:04, 22646936.91it/s]
 45%|     | 76906496/170498071 [00:03&lt;00:03, 23451667.06it/s]
 46%|     | 79265792/170498071 [00:04&lt;00:04, 21735099.17it/s]
 48%|     | 81494016/170498071 [00:04&lt;00:04, 21598772.91it/s]
 49%|     | 83722240/170498071 [00:04&lt;00:03, 21743865.57it/s]
 50%|     | 85917696/170498071 [00:04&lt;00:03, 21778149.28it/s]
 52%|    | 88408064/170498071 [00:04&lt;00:03, 22678125.01it/s]
 53%|    | 91062272/170498071 [00:04&lt;00:03, 23763478.14it/s]
 55%|    | 93454336/170498071 [00:04&lt;00:03, 22010013.05it/s]
 56%|    | 95748096/170498071 [00:04&lt;00:03, 21894977.20it/s]
 57%|    | 97976320/170498071 [00:04&lt;00:03, 21572439.63it/s]
 59%|    | 100171776/170498071 [00:05&lt;00:03, 21573294.08it/s]
 60%|    | 102367232/170498071 [00:05&lt;00:03, 21647755.36it/s]
 61%|   | 104660992/170498071 [00:05&lt;00:03, 21901388.84it/s]
 63%|   | 106987520/170498071 [00:05&lt;00:02, 22186418.64it/s]
 64%|   | 109346816/170498071 [00:05&lt;00:02, 22586902.80it/s]
 65%|   | 111640576/170498071 [00:05&lt;00:02, 22612567.39it/s]
 67%|   | 114196480/170498071 [00:05&lt;00:02, 23236842.09it/s]
 68%|   | 116523008/170498071 [00:05&lt;00:02, 22446526.62it/s]
 70%|   | 118784000/170498071 [00:05&lt;00:02, 22475329.31it/s]
 71%|   | 121044992/170498071 [00:05&lt;00:02, 21624456.61it/s]
 72%|  | 123240448/170498071 [00:06&lt;00:02, 21138880.01it/s]
 74%|  | 125403136/170498071 [00:06&lt;00:02, 21247439.04it/s]
 75%|  | 127631360/170498071 [00:06&lt;00:01, 21512765.03it/s]
 76%|  | 130056192/170498071 [00:06&lt;00:01, 22269551.62it/s]
 78%|  | 132579328/170498071 [00:06&lt;00:01, 23033972.62it/s]
 79%|  | 134905856/170498071 [00:06&lt;00:01, 22743234.72it/s]
 81%|  | 137330688/170498071 [00:06&lt;00:01, 23126840.93it/s]
 82%| | 139657216/170498071 [00:06&lt;00:01, 22329021.22it/s]
 83%| | 141918208/170498071 [00:06&lt;00:01, 22036667.70it/s]
 85%| | 144146432/170498071 [00:07&lt;00:01, 21022787.80it/s]
 86%| | 146341888/170498071 [00:07&lt;00:01, 21141541.59it/s]
 87%| | 148570112/170498071 [00:07&lt;00:01, 21431320.80it/s]
 89%| | 151027712/170498071 [00:07&lt;00:00, 22300502.10it/s]
 90%| | 153485312/170498071 [00:07&lt;00:00, 22946330.28it/s]
 91%|| 155844608/170498071 [00:07&lt;00:00, 22882840.56it/s]
 93%|| 158171136/170498071 [00:07&lt;00:00, 22839263.48it/s]
 94%|| 160464896/170498071 [00:07&lt;00:00, 22757572.10it/s]
 95%|| 162758656/170498071 [00:07&lt;00:00, 22497712.21it/s]
 97%|| 165019648/170498071 [00:07&lt;00:00, 21335096.76it/s]
 98%|| 167280640/170498071 [00:08&lt;00:00, 21557412.36it/s]
 99%|| 169541632/170498071 [00:08&lt;00:00, 21849552.71it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20776756.24it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:11:15. Total running time: 16min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:11:45. Total running time: 17min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.3935999870300293, time = 39.21790838241577
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:12:15. Total running time: 17min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:12:45. Total running time: 18min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4590199887752533, time = 38.98495864868164
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:13:15. Total running time: 18min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:13:45. Total running time: 19min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.4958999752998352, time = 38.62439441680908
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:14:15. Total running time: 19min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:14:46. Total running time: 20min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5320999622344971, time = 38.550172567367554
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:15:16. Total running time: 20min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:15:46. Total running time: 21min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.5552600026130676, time = 39.06446933746338
(train_cifar pid=3909) train time = 294.28418612480164

Trial train_cifar_ce6f6_00001 completed after 1 iterations at 2023-10-31 16:15:55. Total running time: 21min 12s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00001 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 306.70894 |
| time_total_s                                     306.70894 |
| training_iteration                                       1 |
| accuracy                                           0.55526 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00002 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00002 config         |
+----------------------------------------------+
| batch_size                                32 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:55, 358722.47it/s]
  0%|          | 229376/170498071 [00:00&lt;04:12, 674576.34it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2084334.08it/s]
  2%|         | 2850816/170498071 [00:00&lt;00:25, 6617514.85it/s]
  3%|         | 5472256/170498071 [00:00&lt;00:13, 11959497.44it/s]
  5%|         | 7864320/170498071 [00:00&lt;00:12, 12910891.47it/s]
  6%|         | 10715136/170498071 [00:01&lt;00:09, 16846600.07it/s]
  8%|         | 13500416/170498071 [00:01&lt;00:07, 19748671.61it/s]
  9%|         | 16121856/170498071 [00:01&lt;00:08, 18148842.99it/s]
 11%|         | 19038208/170498071 [00:01&lt;00:07, 20871137.16it/s]
 13%|        | 21790720/170498071 [00:01&lt;00:06, 22610627.23it/s]
 14%|        | 24346624/170498071 [00:01&lt;00:07, 20044921.90it/s]
 16%|        | 26640384/170498071 [00:01&lt;00:06, 20752543.40it/s]
 17%|        | 29065216/170498071 [00:01&lt;00:06, 21664663.84it/s]
 19%|        | 31883264/170498071 [00:01&lt;00:05, 23437302.49it/s]
 20%|        | 34701312/170498071 [00:02&lt;00:06, 21181602.73it/s]
 22%|       | 36962304/170498071 [00:02&lt;00:06, 21359227.26it/s]
 23%|       | 39813120/170498071 [00:02&lt;00:05, 23256855.87it/s]
 25%|       | 42532864/170498071 [00:02&lt;00:05, 24328187.35it/s]
 26%|       | 45056000/170498071 [00:02&lt;00:05, 21011677.48it/s]
 28%|       | 47415296/170498071 [00:02&lt;00:05, 21671655.72it/s]
 29%|       | 50233344/170498071 [00:02&lt;00:05, 23361423.42it/s]
 31%|       | 52854784/170498071 [00:02&lt;00:04, 24099727.98it/s]
 32%|      | 55345152/170498071 [00:03&lt;00:05, 21315674.30it/s]
 34%|      | 57606144/170498071 [00:03&lt;00:05, 21299287.28it/s]
 35%|      | 60325888/170498071 [00:03&lt;00:04, 22877108.05it/s]
 37%|      | 63012864/170498071 [00:03&lt;00:04, 23979188.17it/s]
 38%|      | 65470464/170498071 [00:03&lt;00:04, 22190742.33it/s]
 40%|      | 67764224/170498071 [00:03&lt;00:04, 21383502.26it/s]
 41%|      | 70025216/170498071 [00:03&lt;00:04, 21381796.03it/s]
 43%|     | 72646656/170498071 [00:03&lt;00:04, 22689615.22it/s]
 44%|     | 75235328/170498071 [00:03&lt;00:04, 23524570.65it/s]
 46%|     | 77627392/170498071 [00:04&lt;00:04, 22191936.07it/s]
 47%|     | 79888384/170498071 [00:04&lt;00:04, 21796060.39it/s]
 48%|     | 82149376/170498071 [00:04&lt;00:04, 21955721.55it/s]
 49%|     | 84377600/170498071 [00:04&lt;00:03, 21950975.79it/s]
 51%|     | 86638592/170498071 [00:04&lt;00:03, 22016900.15it/s]
 52%|    | 89227264/170498071 [00:04&lt;00:03, 23107200.43it/s]
 54%|    | 91684864/170498071 [00:04&lt;00:03, 21775014.88it/s]
 55%|    | 94175232/170498071 [00:04&lt;00:03, 22285023.32it/s]
 57%|    | 96501760/170498071 [00:04&lt;00:03, 22522415.40it/s]
 58%|    | 98795520/170498071 [00:04&lt;00:03, 22414092.81it/s]
 59%|    | 101089280/170498071 [00:05&lt;00:03, 22481292.82it/s]
 61%|    | 103415808/170498071 [00:05&lt;00:02, 22709278.20it/s]
 62%|   | 105906176/170498071 [00:05&lt;00:02, 23324798.23it/s]
 63%|   | 108265472/170498071 [00:05&lt;00:02, 21819538.92it/s]
 65%|   | 110493696/170498071 [00:05&lt;00:02, 21810226.61it/s]
 66%|   | 112951296/170498071 [00:05&lt;00:02, 22592391.32it/s]
 68%|   | 115245056/170498071 [00:05&lt;00:02, 22524215.35it/s]
 69%|   | 117538816/170498071 [00:05&lt;00:02, 22532470.77it/s]
 70%|   | 119799808/170498071 [00:05&lt;00:02, 22546581.03it/s]
 72%|  | 122257408/170498071 [00:05&lt;00:02, 23140263.78it/s]
 73%|  | 124583936/170498071 [00:06&lt;00:02, 21568046.67it/s]
 74%|  | 126779392/170498071 [00:06&lt;00:02, 21654737.02it/s]
 76%|  | 129007616/170498071 [00:06&lt;00:01, 21268982.99it/s]
 77%|  | 131760128/170498071 [00:06&lt;00:01, 23013426.59it/s]
 79%|  | 134086656/170498071 [00:06&lt;00:01, 22332112.98it/s]
 80%|  | 136577024/170498071 [00:06&lt;00:01, 21219015.47it/s]
 82%| | 139165696/170498071 [00:06&lt;00:01, 22492405.03it/s]
 83%| | 141459456/170498071 [00:06&lt;00:01, 21931948.56it/s]
 84%| | 143785984/170498071 [00:06&lt;00:01, 22291698.25it/s]
 86%| | 146341888/170498071 [00:07&lt;00:01, 22793209.03it/s]
 87%| | 148865024/170498071 [00:07&lt;00:01, 21600806.60it/s]
 89%| | 151289856/170498071 [00:07&lt;00:00, 22306690.28it/s]
 90%| | 153550848/170498071 [00:07&lt;00:00, 21306230.26it/s]
 92%|| 156008448/170498071 [00:07&lt;00:00, 22169300.47it/s]
 93%|| 158433280/170498071 [00:07&lt;00:00, 22724949.81it/s]
 94%|| 160825344/170498071 [00:07&lt;00:00, 22851248.95it/s]
 96%|| 163151872/170498071 [00:07&lt;00:00, 21218293.51it/s]
 97%|| 165511168/170498071 [00:07&lt;00:00, 21840124.16it/s]
 98%|| 167739392/170498071 [00:08&lt;00:00, 21507444.49it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20816163.25it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:16:16. Total running time: 21min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:16:46. Total running time: 22min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6808599829673767, time = 25.51612639427185
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:17:16. Total running time: 22min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.765779972076416, time = 24.523948669433594
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:17:46. Total running time: 23min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8193999528884888, time = 25.170745372772217
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:18:16. Total running time: 23min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8781999945640564, time = 25.780768156051636
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:18:46. Total running time: 24min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:19:16. Total running time: 24min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9199399948120117, time = 24.40403652191162
(train_cifar pid=3909) train time = 195.2490997314453

Trial train_cifar_ce6f6_00002 completed after 1 iterations at 2023-10-31 16:19:22. Total running time: 24min 39s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00002 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 207.59839 |
| time_total_s                                     207.59839 |
| training_iteration                                       1 |
| accuracy                                           0.91994 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00003 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00003 config           |
+------------------------------------------------+
| batch_size                                   4 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:07, 349900.85it/s]
  0%|          | 229376/170498071 [00:00&lt;04:12, 673769.59it/s]
  0%|          | 524288/170498071 [00:00&lt;02:02, 1384727.51it/s]
  1%|          | 1212416/170498071 [00:00&lt;00:55, 3065007.00it/s]
  2%|         | 2621440/170498071 [00:00&lt;00:26, 6351286.76it/s]
  3%|         | 4489216/170498071 [00:00&lt;00:16, 10040763.61it/s]
  4%|         | 6520832/170498071 [00:00&lt;00:12, 12985970.09it/s]
  5%|         | 9142272/170498071 [00:00&lt;00:09, 16898840.53it/s]
  7%|         | 11501568/170498071 [00:01&lt;00:08, 18887535.82it/s]
  8%|         | 13664256/170498071 [00:01&lt;00:08, 18798718.29it/s]
  9%|         | 15892480/170498071 [00:01&lt;00:07, 19767548.93it/s]
 11%|         | 18186240/170498071 [00:01&lt;00:07, 20583997.31it/s]
 12%|        | 20512768/170498071 [00:01&lt;00:07, 21324141.42it/s]
 13%|        | 22806528/170498071 [00:01&lt;00:06, 21788371.08it/s]
 15%|        | 25034752/170498071 [00:01&lt;00:06, 21924534.52it/s]
 18%|        | 29917184/170498071 [00:01&lt;00:06, 23259614.00it/s]
 19%|        | 32276480/170498071 [00:02&lt;00:06, 21935304.94it/s]
 20%|        | 34504704/170498071 [00:02&lt;00:06, 21968040.04it/s]
 22%|       | 36765696/170498071 [00:02&lt;00:06, 22060329.62it/s]
 23%|       | 39092224/170498071 [00:02&lt;00:05, 22384294.64it/s]
 24%|       | 41385984/170498071 [00:02&lt;00:05, 22484887.81it/s]
 26%|       | 43679744/170498071 [00:02&lt;00:05, 22456293.72it/s]
 27%|       | 46039040/170498071 [00:02&lt;00:05, 22782035.14it/s]
 29%|       | 48594944/170498071 [00:02&lt;00:05, 23575115.80it/s]
 30%|       | 50987008/170498071 [00:02&lt;00:05, 22030625.38it/s]
 31%|       | 53215232/170498071 [00:02&lt;00:05, 21985317.33it/s]
 33%|      | 55574528/170498071 [00:03&lt;00:05, 22439349.53it/s]
 34%|      | 57835520/170498071 [00:03&lt;00:05, 22487720.36it/s]
 35%|      | 60096512/170498071 [00:03&lt;00:04, 22421147.91it/s]
 37%|      | 62390272/170498071 [00:03&lt;00:04, 22551290.79it/s]
 38%|      | 64782336/170498071 [00:03&lt;00:04, 22910257.30it/s]
 39%|      | 67305472/170498071 [00:03&lt;00:04, 22360752.81it/s]
 41%|      | 69632000/170498071 [00:03&lt;00:04, 22481643.05it/s]
 42%|     | 71892992/170498071 [00:03&lt;00:04, 22189900.99it/s]
 44%|     | 74252288/170498071 [00:03&lt;00:04, 22579042.88it/s]
 45%|     | 76546048/170498071 [00:03&lt;00:04, 22440558.14it/s]
 46%|     | 78807040/170498071 [00:04&lt;00:04, 22439136.54it/s]
 48%|     | 81264640/170498071 [00:04&lt;00:03, 22780009.36it/s]
 49%|     | 83820544/170498071 [00:04&lt;00:03, 22476312.49it/s]
 50%|     | 86081536/170498071 [00:04&lt;00:03, 22502094.20it/s]
 52%|    | 88375296/170498071 [00:04&lt;00:03, 22533867.94it/s]
 53%|    | 90636288/170498071 [00:04&lt;00:03, 22478873.53it/s]
 55%|    | 92930048/170498071 [00:04&lt;00:03, 22517306.35it/s]
 56%|    | 95191040/170498071 [00:04&lt;00:03, 22480162.00it/s]
 57%|    | 97484800/170498071 [00:04&lt;00:03, 22606771.99it/s]
 59%|    | 99844096/170498071 [00:05&lt;00:03, 22744515.64it/s]
 60%|    | 102400000/170498071 [00:05&lt;00:03, 22269632.13it/s]
 61%|   | 104792064/170498071 [00:05&lt;00:02, 22617050.10it/s]
 63%|   | 107085824/170498071 [00:05&lt;00:02, 22363402.95it/s]
 64%|   | 109412352/170498071 [00:05&lt;00:02, 22341358.47it/s]
 66%|   | 111706112/170498071 [00:05&lt;00:02, 22513110.77it/s]
 67%|   | 114032640/170498071 [00:05&lt;00:02, 22712269.54it/s]
 68%|   | 116359168/170498071 [00:05&lt;00:02, 22812319.65it/s]
 70%|   | 118784000/170498071 [00:05&lt;00:02, 23233460.06it/s]
 71%|   | 121110528/170498071 [00:05&lt;00:02, 22216851.30it/s]
 72%|  | 123371520/170498071 [00:06&lt;00:02, 22206323.54it/s]
 74%|  | 125632512/170498071 [00:06&lt;00:02, 22314473.46it/s]
 75%|  | 127991808/170498071 [00:06&lt;00:01, 22300443.47it/s]
 76%|  | 130285568/170498071 [00:06&lt;00:01, 22465845.52it/s]
 78%|  | 132644864/170498071 [00:06&lt;00:01, 22703298.21it/s]
 79%|  | 134971392/170498071 [00:06&lt;00:01, 22769580.84it/s]
 81%|  | 137265152/170498071 [00:06&lt;00:01, 22628963.30it/s]
 82%| | 139558912/170498071 [00:06&lt;00:01, 22154058.17it/s]
 83%| | 141885440/170498071 [00:06&lt;00:01, 22310546.25it/s]
 85%| | 144179200/170498071 [00:06&lt;00:01, 22479389.81it/s]
 86%| | 146440192/170498071 [00:07&lt;00:01, 22514432.03it/s]
 87%| | 148701184/170498071 [00:07&lt;00:00, 22406628.15it/s]
 89%| | 151060480/170498071 [00:07&lt;00:00, 22703935.91it/s]
 90%| | 153354240/170498071 [00:07&lt;00:00, 22640300.09it/s]
 91%|| 155713536/170498071 [00:07&lt;00:00, 22848139.72it/s]
 93%|| 158007296/170498071 [00:07&lt;00:00, 22644962.90it/s]
 94%|| 160301056/170498071 [00:07&lt;00:00, 22302305.46it/s]
 95%|| 162594816/170498071 [00:07&lt;00:00, 22389210.38it/s]
 97%|| 164888576/170498071 [00:07&lt;00:00, 22525669.39it/s]
 98%|| 167215104/170498071 [00:08&lt;00:00, 22738645.24it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20884540.23it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:19:46. Total running time: 25min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:20:16. Total running time: 25min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:20:46. Total running time: 26min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:21:16. Total running time: 26min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:21:46. Total running time: 27min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:22:17. Total running time: 27min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.4229999780654907, time = 127.65873551368713
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:22:47. Total running time: 28min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:23:17. Total running time: 28min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:23:47. Total running time: 29min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:24:17. Total running time: 29min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:24:47. Total running time: 30min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:25:17. Total running time: 30min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:25:47. Total running time: 31min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.49333998560905457, time = 128.40579271316528
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:26:17. Total running time: 31min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:26:47. Total running time: 32min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:27:17. Total running time: 32min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:27:47. Total running time: 33min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:28:17. Total running time: 33min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:28:47. Total running time: 34min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5471999645233154, time = 129.01236128807068
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:29:17. Total running time: 34min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:29:47. Total running time: 35min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:30:17. Total running time: 35min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:30:48. Total running time: 36min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:31:18. Total running time: 36min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:31:48. Total running time: 37min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5875200033187866, time = 127.10114216804504
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:32:18. Total running time: 37min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:32:48. Total running time: 38min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:33:18. Total running time: 38min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:33:48. Total running time: 39min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:34:18. Total running time: 39min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:34:48. Total running time: 40min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.6218799948692322, time = 127.78192067146301
(train_cifar pid=3909) train time = 933.7627487182617

Trial train_cifar_ce6f6_00003 completed after 1 iterations at 2023-10-31 16:35:08. Total running time: 40min 25s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00003 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 945.97803 |
| time_total_s                                     945.97803 |
| training_iteration                                       1 |
| accuracy                                           0.62188 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00004 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00004 config           |
+------------------------------------------------+
| batch_size                                   4 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:55, 358355.82it/s]
  0%|          | 196608/170498071 [00:00&lt;03:39, 775522.39it/s]
  0%|          | 327680/170498071 [00:00&lt;02:53, 980584.28it/s]
  1%|          | 917504/170498071 [00:00&lt;01:16, 2212208.86it/s]
  1%|         | 2162688/170498071 [00:00&lt;00:32, 5173222.05it/s]
  3%|         | 4554752/170498071 [00:00&lt;00:15, 10657919.44it/s]
  5%|         | 7766016/170498071 [00:00&lt;00:09, 16980632.51it/s]
  7%|         | 11272192/170498071 [00:00&lt;00:07, 22299149.25it/s]
  9%|         | 15073280/170498071 [00:01&lt;00:05, 26964064.07it/s]
 11%|         | 18120704/170498071 [00:01&lt;00:05, 27997765.62it/s]
 13%|        | 21331968/170498071 [00:01&lt;00:05, 28941267.43it/s]
 14%|        | 24510464/170498071 [00:01&lt;00:04, 29517516.37it/s]
 17%|        | 28475392/170498071 [00:01&lt;00:04, 31944281.61it/s]
 19%|        | 31850496/170498071 [00:01&lt;00:04, 32472738.84it/s]
 21%|        | 35520512/170498071 [00:01&lt;00:04, 33711131.37it/s]
 23%|       | 39256064/170498071 [00:01&lt;00:04, 30193849.42it/s]
 25%|       | 42958848/170498071 [00:01&lt;00:04, 31864110.09it/s]
 27%|       | 46858240/170498071 [00:02&lt;00:03, 33831096.25it/s]
 30%|       | 50364416/170498071 [00:02&lt;00:03, 32745718.96it/s]
 32%|      | 54263808/170498071 [00:02&lt;00:03, 34478257.16it/s]
 34%|      | 57769984/170498071 [00:02&lt;00:03, 34526436.52it/s]
 36%|      | 61276160/170498071 [00:02&lt;00:03, 30108834.61it/s]
 38%|      | 64520192/170498071 [00:02&lt;00:03, 30682201.49it/s]
 40%|      | 67731456/170498071 [00:02&lt;00:03, 31057740.59it/s]
 42%|     | 70909952/170498071 [00:02&lt;00:03, 31209196.63it/s]
 43%|     | 74088448/170498071 [00:02&lt;00:03, 31347810.32it/s]
 45%|     | 77266944/170498071 [00:02&lt;00:02, 31336066.19it/s]
 47%|     | 80510976/170498071 [00:03&lt;00:02, 31483268.53it/s]
 49%|     | 83755008/170498071 [00:03&lt;00:02, 31650869.22it/s]
 51%|     | 86933504/170498071 [00:03&lt;00:02, 31603153.39it/s]
 53%|    | 90112000/170498071 [00:03&lt;00:02, 31637983.57it/s]
 55%|    | 93290496/170498071 [00:03&lt;00:02, 31610447.62it/s]
 57%|    | 96534528/170498071 [00:03&lt;00:02, 31791124.72it/s]
 59%|    | 99745792/170498071 [00:03&lt;00:02, 31839603.05it/s]
 60%|    | 102957056/170498071 [00:03&lt;00:02, 28461435.26it/s]
 62%|   | 106135552/170498071 [00:03&lt;00:02, 29310649.89it/s]
 64%|   | 109314048/170498071 [00:04&lt;00:02, 29965092.08it/s]
 66%|   | 112361472/170498071 [00:04&lt;00:01, 30073381.10it/s]
 68%|   | 115572736/170498071 [00:04&lt;00:01, 30616977.91it/s]
 70%|   | 118784000/170498071 [00:04&lt;00:01, 31029471.68it/s]
 72%|  | 121995264/170498071 [00:04&lt;00:01, 31340954.79it/s]
 73%|  | 125173760/170498071 [00:04&lt;00:01, 31184111.72it/s]
 76%|  | 128745472/170498071 [00:04&lt;00:01, 32516143.67it/s]
 78%|  | 132513792/170498071 [00:04&lt;00:01, 34029133.99it/s]
 80%|  | 135954432/170498071 [00:04&lt;00:01, 29940752.57it/s]
 82%| | 139689984/170498071 [00:04&lt;00:00, 31938066.23it/s]
 84%| | 143458304/170498071 [00:05&lt;00:00, 33528655.04it/s]
 86%| | 146898944/170498071 [00:05&lt;00:00, 32524673.33it/s]
 88%| | 150568960/170498071 [00:05&lt;00:00, 33682677.33it/s]
 91%| | 154337280/170498071 [00:05&lt;00:00, 34816496.13it/s]
 93%|| 157876224/170498071 [00:05&lt;00:00, 34915346.75it/s]
 95%|| 161415168/170498071 [00:05&lt;00:00, 30895319.20it/s]
 97%|| 165183488/170498071 [00:05&lt;00:00, 32701603.94it/s]
100%|| 170498071/170498071 [00:05&lt;00:00, 28985460.92it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:35:18. Total running time: 40min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:35:48. Total running time: 41min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:36:18. Total running time: 41min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:36:48. Total running time: 42min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:37:18. Total running time: 42min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:37:48. Total running time: 43min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:38:18. Total running time: 43min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.4318599998950958, time = 126.27536916732788
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:38:49. Total running time: 44min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:39:19. Total running time: 44min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:39:49. Total running time: 45min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:40:19. Total running time: 45min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:40:49. Total running time: 46min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:41:19. Total running time: 46min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4997199773788452, time = 125.57496285438538
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:41:49. Total running time: 47min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:42:19. Total running time: 47min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:42:49. Total running time: 48min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:43:19. Total running time: 48min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:43:49. Total running time: 49min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:44:19. Total running time: 49min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5493999719619751, time = 128.38473892211914
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:44:49. Total running time: 50min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:45:19. Total running time: 50min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:45:49. Total running time: 51min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:46:19. Total running time: 51min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:46:49. Total running time: 52min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:47:19. Total running time: 52min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5884599685668945, time = 128.52797555923462
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:47:49. Total running time: 53min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:48:20. Total running time: 53min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:48:50. Total running time: 54min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:49:20. Total running time: 54min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:49:50. Total running time: 55min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:50:20. Total running time: 55min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.6177799701690674, time = 127.81025004386902
(train_cifar pid=3909) train time = 930.2581012248993

Trial train_cifar_ce6f6_00004 completed after 1 iterations at 2023-10-31 16:50:49. Total running time: 56min 6s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00004 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 940.36313 |
| time_total_s                                     940.36313 |
| training_iteration                                       1 |
| accuracy                                           0.61778 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00005 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00005 config         |
+----------------------------------------------+
| batch_size                                16 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:52, 360445.51it/s]
  0%|          | 229376/170498071 [00:00&lt;04:10, 680851.57it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:50:50. Total running time: 56min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 589824/170498071 [00:00&lt;01:45, 1605238.32it/s]
  1%|          | 1441792/170498071 [00:00&lt;00:45, 3749338.56it/s]
  2%|         | 3276800/170498071 [00:00&lt;00:20, 8222975.15it/s]
  3%|         | 5668864/170498071 [00:00&lt;00:12, 13005952.96it/s]
  5%|         | 7831552/170498071 [00:00&lt;00:11, 13663453.41it/s]
  6%|         | 10354688/170498071 [00:01&lt;00:09, 16836990.60it/s]
  7%|         | 12648448/170498071 [00:01&lt;00:08, 18554203.03it/s]
  9%|         | 15073280/170498071 [00:01&lt;00:07, 20177789.30it/s]
 12%|        | 19693568/170498071 [00:01&lt;00:06, 21670248.20it/s]
 13%|        | 22151168/170498071 [00:01&lt;00:06, 22440894.36it/s]
 14%|        | 24444928/170498071 [00:01&lt;00:07, 20681322.15it/s]
 16%|        | 26771456/170498071 [00:01&lt;00:06, 21387499.18it/s]
 17%|        | 29032448/170498071 [00:01&lt;00:06, 21638554.75it/s]
 18%|        | 31391744/170498071 [00:01&lt;00:06, 22124984.56it/s]
 20%|        | 33816576/170498071 [00:02&lt;00:06, 22737567.36it/s]
 21%|        | 36208640/170498071 [00:02&lt;00:05, 23033669.22it/s]
 23%|       | 38600704/170498071 [00:02&lt;00:06, 21346483.57it/s]
 24%|       | 40992768/170498071 [00:02&lt;00:05, 21959022.74it/s]
 25%|       | 43220992/170498071 [00:02&lt;00:05, 22015747.93it/s]
 27%|       | 45547520/170498071 [00:02&lt;00:05, 22338010.77it/s]
 28%|       | 47874048/170498071 [00:02&lt;00:05, 22530785.42it/s]
 30%|       | 50298880/170498071 [00:02&lt;00:05, 22967759.98it/s]
 31%|       | 52625408/170498071 [00:02&lt;00:05, 22852431.17it/s]
 32%|      | 54919168/170498071 [00:03&lt;00:05, 21375343.92it/s]
 34%|      | 57442304/170498071 [00:03&lt;00:05, 21953001.98it/s]
 35%|      | 59736064/170498071 [00:03&lt;00:05, 22043982.59it/s]
 36%|      | 62193664/170498071 [00:03&lt;00:04, 22719640.22it/s]
 38%|      | 64520192/170498071 [00:03&lt;00:04, 22837065.33it/s]
 39%|      | 66912256/170498071 [00:03&lt;00:04, 23090827.25it/s]
 41%|      | 69238784/170498071 [00:03&lt;00:04, 21434941.30it/s]
 42%|     | 71598080/170498071 [00:03&lt;00:04, 21528987.90it/s]
 43%|     | 73957376/170498071 [00:03&lt;00:04, 22101318.84it/s]
 45%|     | 76382208/170498071 [00:03&lt;00:04, 22713612.13it/s]
 46%|     | 78774272/170498071 [00:04&lt;00:03, 23049555.63it/s]
 48%|     | 81100800/170498071 [00:04&lt;00:03, 23046054.13it/s]
 49%|     | 83427328/170498071 [00:04&lt;00:03, 23002801.25it/s]
 50%|     | 85753856/170498071 [00:04&lt;00:03, 21761537.60it/s]
 52%|    | 87982080/170498071 [00:04&lt;00:03, 21454970.80it/s]
 53%|    | 90406912/170498071 [00:04&lt;00:03, 22146708.92it/s]
 54%|    | 92897280/170498071 [00:04&lt;00:03, 22928036.79it/s]
 56%|    | 95322112/170498071 [00:04&lt;00:03, 23196248.01it/s]
 57%|    | 97681408/170498071 [00:04&lt;00:03, 23203079.75it/s]
 59%|    | 100007936/170498071 [00:05&lt;00:03, 21703958.06it/s]
 60%|    | 102268928/170498071 [00:05&lt;00:03, 21474369.40it/s]
 61%|   | 104693760/170498071 [00:05&lt;00:02, 22042067.62it/s]
 63%|   | 107216896/170498071 [00:05&lt;00:02, 22817615.92it/s]
 64%|   | 109838336/170498071 [00:05&lt;00:02, 23578482.77it/s]
 66%|   | 112230400/170498071 [00:05&lt;00:02, 23636152.47it/s]
 67%|   | 114622464/170498071 [00:05&lt;00:02, 21407779.89it/s]
 69%|   | 116916224/170498071 [00:05&lt;00:02, 21745182.05it/s]
 70%|   | 119308288/170498071 [00:05&lt;00:02, 22347013.98it/s]
 71%|  | 121831424/170498071 [00:05&lt;00:02, 23131587.05it/s]
 73%|  | 124321792/170498071 [00:06&lt;00:01, 23519837.97it/s]
 74%|  | 126713856/170498071 [00:06&lt;00:01, 22001305.88it/s]
 76%|  | 129007616/170498071 [00:06&lt;00:01, 21674334.72it/s]
 77%|  | 131203072/170498071 [00:06&lt;00:01, 21650529.79it/s]
 78%|  | 133660672/170498071 [00:06&lt;00:01, 22444809.69it/s]
 80%|  | 136118272/170498071 [00:06&lt;00:01, 22830470.54it/s]
 81%| | 138674176/170498071 [00:06&lt;00:01, 23545263.81it/s]
 83%| | 141066240/170498071 [00:06&lt;00:01, 22174805.43it/s]
 84%| | 143425536/170498071 [00:06&lt;00:01, 22091060.77it/s]
 85%| | 145653760/170498071 [00:07&lt;00:01, 22042161.68it/s]
 87%| | 148144128/170498071 [00:07&lt;00:00, 22749145.89it/s]
 88%| | 150437888/170498071 [00:07&lt;00:00, 22703274.68it/s]
 90%| | 152928256/170498071 [00:07&lt;00:00, 23342719.37it/s]
 91%| | 155287552/170498071 [00:07&lt;00:00, 22224249.66it/s]
 92%|| 157614080/170498071 [00:07&lt;00:00, 22502247.83it/s]
 94%|| 159907840/170498071 [00:07&lt;00:00, 22347711.68it/s]
 95%|| 162168832/170498071 [00:07&lt;00:00, 22128943.30it/s]
 97%|| 164593664/170498071 [00:07&lt;00:00, 22694314.74it/s]
 98%|| 166952960/170498071 [00:07&lt;00:00, 22888572.63it/s]
 99%|| 169246720/170498071 [00:08&lt;00:00, 22820177.59it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20912709.36it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:51:20. Total running time: 56min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:51:50. Total running time: 57min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6728399991989136, time = 38.56611657142639
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:52:20. Total running time: 57min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:52:50. Total running time: 58min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7697199583053589, time = 38.52308917045593
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:53:20. Total running time: 58min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:53:50. Total running time: 59min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8376999497413635, time = 38.70884370803833
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:54:20. Total running time: 59min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:54:50. Total running time: 1hr 0min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8943399786949158, time = 38.13762617111206
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:55:20. Total running time: 1hr 0min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:55:50. Total running time: 1hr 1min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9262599945068359, time = 38.1430881023407
(train_cifar pid=3909) train time = 294.1284604072571

Trial train_cifar_ce6f6_00005 completed after 1 iterations at 2023-10-31 16:55:56. Total running time: 1hr 1min 13s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00005 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                   306.802 |
| time_total_s                                       306.802 |
| training_iteration                                       1 |
| accuracy                                           0.92626 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00006 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00006 config         |
+----------------------------------------------+
| batch_size                                16 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:11, 346711.28it/s]
  0%|          | 229376/170498071 [00:00&lt;04:16, 664266.42it/s]
  0%|          | 524288/170498071 [00:00&lt;02:03, 1373333.00it/s]
  1%|          | 1179648/170498071 [00:00&lt;00:57, 2955091.49it/s]
  2%|         | 2588672/170498071 [00:00&lt;00:26, 6336068.42it/s]
  2%|         | 4194304/170498071 [00:00&lt;00:17, 9243817.70it/s]
  4%|         | 6094848/170498071 [00:00&lt;00:13, 12147564.06it/s]
  5%|         | 8126464/170498071 [00:00&lt;00:11, 14518639.14it/s]
  6%|         | 10289152/170498071 [00:01&lt;00:09, 16613194.38it/s]
  7%|         | 12517376/170498071 [00:01&lt;00:08, 18298009.88it/s]
  9%|         | 14876672/170498071 [00:01&lt;00:07, 19848420.90it/s]
 10%|         | 17301504/170498071 [00:01&lt;00:07, 21034951.98it/s]
 12%|        | 19857408/170498071 [00:01&lt;00:06, 22378407.94it/s]
 13%|        | 22347776/170498071 [00:01&lt;00:06, 23099905.42it/s]
 15%|        | 24969216/170498071 [00:01&lt;00:06, 23987200.96it/s]
 16%|        | 27852800/170498071 [00:01&lt;00:05, 25409362.42it/s]
 18%|        | 30703616/170498071 [00:01&lt;00:05, 26291190.57it/s]
 20%|        | 33816576/170498071 [00:01&lt;00:05, 27113097.52it/s]
 22%|       | 36765696/170498071 [00:02&lt;00:04, 27787793.68it/s]
 23%|       | 39682048/170498071 [00:02&lt;00:04, 28170878.06it/s]
 25%|       | 42663936/170498071 [00:02&lt;00:04, 28558349.58it/s]
 27%|       | 45711360/170498071 [00:02&lt;00:04, 29098606.90it/s]
 29%|       | 48627712/170498071 [00:02&lt;00:04, 28847065.76it/s]
 30%|       | 51707904/170498071 [00:02&lt;00:04, 29373595.04it/s]
 32%|      | 54657024/170498071 [00:02&lt;00:03, 29012066.31it/s]
 34%|      | 57638912/170498071 [00:02&lt;00:03, 29235303.01it/s]
 36%|      | 60588032/170498071 [00:02&lt;00:03, 28979583.25it/s]
 37%|      | 63635456/170498071 [00:03&lt;00:03, 29404352.04it/s]
 39%|      | 66682880/170498071 [00:03&lt;00:03, 29581090.29it/s]
 41%|      | 69664768/170498071 [00:03&lt;00:03, 29231924.85it/s]
 43%|     | 72810496/170498071 [00:03&lt;00:03, 29871574.01it/s]
 44%|     | 75825152/170498071 [00:03&lt;00:03, 29116610.42it/s]
 46%|     | 78872576/170498071 [00:03&lt;00:03, 29504892.50it/s]
 48%|     | 81854464/170498071 [00:03&lt;00:03, 29182797.22it/s]
 50%|     | 84869120/170498071 [00:03&lt;00:02, 29413801.85it/s]
 52%|    | 87818240/170498071 [00:03&lt;00:02, 29201838.98it/s]
 53%|    | 90963968/170498071 [00:03&lt;00:02, 29815137.83it/s]
 55%|    | 93978624/170498071 [00:04&lt;00:02, 29035858.79it/s]
 57%|    | 97255424/170498071 [00:04&lt;00:02, 29315981.54it/s]
 59%|    | 100401152/170498071 [00:04&lt;00:02, 29817251.46it/s]
 61%|    | 104071168/170498071 [00:04&lt;00:02, 31808054.49it/s]
 63%|   | 107577344/170498071 [00:04&lt;00:01, 32731909.73it/s]
 65%|   | 111181824/170498071 [00:04&lt;00:01, 33707190.04it/s]
 67%|   | 114589696/170498071 [00:04&lt;00:01, 31136599.38it/s]
 69%|   | 118226944/170498071 [00:04&lt;00:01, 32581356.88it/s]
 71%|  | 121831424/170498071 [00:04&lt;00:01, 33552054.14it/s]
 74%|  | 125468672/170498071 [00:04&lt;00:01, 34367773.27it/s]
 76%|  | 128942080/170498071 [00:05&lt;00:01, 30644919.11it/s]
 77%|  | 132120576/170498071 [00:05&lt;00:01, 30289500.90it/s]
 79%|  | 135233536/170498071 [00:05&lt;00:01, 30167712.94it/s]
 81%|  | 138313728/170498071 [00:05&lt;00:01, 29979602.45it/s]
 83%| | 141361152/170498071 [00:05&lt;00:01, 27098135.46it/s]
 85%| | 144146432/170498071 [00:05&lt;00:01, 25444842.72it/s]
 86%| | 147030016/170498071 [00:05&lt;00:00, 26251754.18it/s]
 88%| | 149913600/170498071 [00:05&lt;00:00, 26911313.79it/s]
 90%| | 152829952/170498071 [00:06&lt;00:00, 27518604.86it/s]
 91%|| 155779072/170498071 [00:06&lt;00:00, 27991563.19it/s]
 93%|| 158629888/170498071 [00:06&lt;00:00, 27659531.01it/s]
 95%|| 161546240/170498071 [00:06&lt;00:00, 28070539.61it/s]
 96%|| 164528128/170498071 [00:06&lt;00:00, 28433514.75it/s]
 98%|| 167542784/170498071 [00:06&lt;00:00, 28592581.48it/s]
100%|| 170498071/170498071 [00:06&lt;00:00, 25736246.04it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:56:20. Total running time: 1hr 1min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:56:50. Total running time: 1hr 2min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6760199666023254, time = 38.34031796455383
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:57:21. Total running time: 1hr 2min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:57:51. Total running time: 1hr 3min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.795740008354187, time = 38.32409405708313
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:58:21. Total running time: 1hr 3min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:58:51. Total running time: 1hr 4min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8547999858856201, time = 38.247161626815796
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:59:21. Total running time: 1hr 4min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:59:51. Total running time: 1hr 5min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.89301997423172, time = 38.3633337020874
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 17:00:21. Total running time: 1hr 5min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 17:00:51. Total running time: 1hr 6min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9261999726295471, time = 38.08350658416748
(train_cifar pid=3909) train time = 293.47102522850037

Trial train_cifar_ce6f6_00006 completed after 1 iterations at 2023-10-31 17:01:02. Total running time: 1hr 6min 19s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00006 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 306.49549 |
| time_total_s                                     306.49549 |
| training_iteration                                       1 |
| accuracy                                            0.9262 |
+------------------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>
Trial train_cifar_ce6f6_00007 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00007 config         |
+----------------------------------------------+
| batch_size                                32 |
| learning_rate                            0.1 |
+----------------------------------------------+
(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:18, 341950.89it/s]
  0%|          | 163840/170498071 [00:00&lt;04:41, 605128.12it/s]
  0%|          | 327680/170498071 [00:00&lt;02:51, 991297.23it/s]
  0%|          | 688128/170498071 [00:00&lt;01:30, 1877488.85it/s]
  1%|          | 1507328/170498071 [00:00&lt;00:42, 3950781.62it/s]
  1%|         | 2326528/170498071 [00:00&lt;00:31, 5277338.20it/s]
  2%|         | 3375104/170498071 [00:00&lt;00:24, 6864874.22it/s]
  3%|         | 4456448/170498071 [00:00&lt;00:20, 8054150.65it/s]
  3%|         | 5603328/170498071 [00:01&lt;00:18, 9074364.07it/s]
  4%|         | 6782976/170498071 [00:01&lt;00:16, 9832425.73it/s]
  5%|         | 8028160/170498071 [00:01&lt;00:15, 10503844.85it/s]
  5%|         | 9338880/170498071 [00:01&lt;00:14, 11278126.79it/s]
  7%|         | 12222464/170498071 [00:01&lt;00:12, 12818100.18it/s]
  8%|         | 13828096/170498071 [00:01&lt;00:11, 13706684.84it/s]
  9%|         | 15499264/170498071 [00:01&lt;00:10, 14526489.04it/s]
 10%|         | 17268736/170498071 [00:01&lt;00:09, 15433871.04it/s]
 11%|         | 19103744/170498071 [00:01&lt;00:09, 16284655.57it/s]
 12%|        | 21102592/170498071 [00:02&lt;00:08, 17293217.71it/s]
 14%|        | 23265280/170498071 [00:02&lt;00:07, 18580041.62it/s]
 15%|        | 25427968/170498071 [00:02&lt;00:07, 19454433.99it/s]
 16%|        | 27656192/170498071 [00:02&lt;00:07, 20265518.01it/s]
 18%|        | 29851648/170498071 [00:02&lt;00:06, 20672994.70it/s]
 19%|        | 32243712/170498071 [00:02&lt;00:06, 21498794.32it/s]
 20%|        | 34471936/170498071 [00:02&lt;00:06, 21560172.30it/s]
 22%|       | 36798464/170498071 [00:02&lt;00:06, 22003604.22it/s]
 23%|       | 39026688/170498071 [00:02&lt;00:06, 21908635.34it/s]
 24%|       | 41353216/170498071 [00:02&lt;00:05, 22289581.83it/s]
 26%|       | 43614208/170498071 [00:03&lt;00:05, 22282043.46it/s]
 27%|       | 46006272/170498071 [00:03&lt;00:05, 22656842.90it/s]
 28%|       | 48300032/170498071 [00:03&lt;00:05, 22464528.28it/s]
 30%|       | 50561024/170498071 [00:03&lt;00:05, 22262691.14it/s]
 31%|       | 52920320/170498071 [00:03&lt;00:05, 22604518.98it/s]
 32%|      | 55214080/170498071 [00:03&lt;00:05, 22494815.87it/s]
 34%|      | 57507840/170498071 [00:03&lt;00:04, 22599574.37it/s]
 35%|      | 59768832/170498071 [00:03&lt;00:04, 22359610.01it/s]
 36%|      | 62029824/170498071 [00:03&lt;00:04, 22255573.10it/s]
 38%|      | 64454656/170498071 [00:03&lt;00:04, 22721876.93it/s]
 39%|      | 66813952/170498071 [00:04&lt;00:04, 22708960.92it/s]
 41%|      | 69107712/170498071 [00:04&lt;00:04, 22319060.65it/s]
 42%|     | 71401472/170498071 [00:04&lt;00:04, 22498574.83it/s]
 43%|     | 73728000/170498071 [00:04&lt;00:04, 22499820.38it/s]
 45%|     | 76054528/170498071 [00:04&lt;00:04, 22683785.47it/s]
 46%|     | 78348288/170498071 [00:04&lt;00:04, 22317410.47it/s]
 47%|     | 80609280/170498071 [00:04&lt;00:04, 22399209.52it/s]
 49%|     | 82968576/170498071 [00:04&lt;00:03, 22709612.93it/s]
 50%|     | 85295104/170498071 [00:04&lt;00:03, 22863086.57it/s]
 51%|    | 87588864/170498071 [00:04&lt;00:03, 22673926.36it/s]
 53%|    | 89882624/170498071 [00:05&lt;00:03, 22273675.59it/s]
 54%|    | 92241920/170498071 [00:05&lt;00:03, 22608579.10it/s]
 55%|    | 94535680/170498071 [00:05&lt;00:03, 22589400.61it/s]
 57%|    | 96796672/170498071 [00:05&lt;00:03, 22408893.44it/s]
 58%|    | 99057664/170498071 [00:05&lt;00:03, 22211364.76it/s]
 59%|    | 101384192/170498071 [00:05&lt;00:03, 22450359.96it/s]
 61%|    | 103710720/170498071 [00:05&lt;00:02, 22631720.93it/s]
 62%|   | 106135552/170498071 [00:05&lt;00:02, 22689527.00it/s]
 64%|   | 108429312/170498071 [00:05&lt;00:02, 22452584.13it/s]
 65%|   | 110723072/170498071 [00:06&lt;00:02, 22534450.72it/s]
 66%|   | 112984064/170498071 [00:06&lt;00:02, 22303210.59it/s]
 68%|   | 115343360/170498071 [00:06&lt;00:02, 22517136.40it/s]
 69%|   | 117604352/170498071 [00:06&lt;00:02, 22369658.34it/s]
 70%|   | 119930880/170498071 [00:06&lt;00:02, 22554342.26it/s]
 72%|  | 122257408/170498071 [00:06&lt;00:02, 22484156.17it/s]
 73%|  | 124747776/170498071 [00:06&lt;00:02, 22870653.59it/s]
 75%|  | 127041536/170498071 [00:06&lt;00:01, 22714295.03it/s]
 76%|  | 129335296/170498071 [00:06&lt;00:01, 22499903.36it/s]
 77%|  | 131661824/170498071 [00:06&lt;00:01, 22315995.39it/s]
 79%|  | 133955584/170498071 [00:07&lt;00:01, 22472700.00it/s]
 80%|  | 136216576/170498071 [00:07&lt;00:01, 22469442.70it/s]
 81%| | 138543104/170498071 [00:07&lt;00:01, 22698894.20it/s]
 83%| | 140836864/170498071 [00:07&lt;00:01, 22596046.47it/s]
 84%| | 143163392/170498071 [00:07&lt;00:01, 22742213.61it/s]
 85%| | 145457152/170498071 [00:07&lt;00:01, 22666320.95it/s]
 87%| | 147750912/170498071 [00:07&lt;00:01, 22149240.53it/s]
 88%| | 150208512/170498071 [00:07&lt;00:00, 22469857.25it/s]
 89%| | 152469504/170498071 [00:07&lt;00:00, 22471052.47it/s]
 91%| | 154730496/170498071 [00:07&lt;00:00, 22308527.49it/s]
 92%|| 157089792/170498071 [00:08&lt;00:00, 22664646.46it/s]
 93%|| 159383552/170498071 [00:08&lt;00:00, 22561223.67it/s]
 95%|| 161710080/170498071 [00:08&lt;00:00, 22765948.27it/s]
 96%|| 164003840/170498071 [00:08&lt;00:00, 22612324.60it/s]
 98%|| 166363136/170498071 [00:08&lt;00:00, 22346221.23it/s]
 99%|| 168722432/170498071 [00:08&lt;00:00, 22670104.62it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 19698302.19it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:01:21. Total running time: 1hr 6min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:01:51. Total running time: 1hr 7min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6528399586677551, time = 25.4039409160614
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:02:21. Total running time: 1hr 7min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7628999948501587, time = 25.41545343399048
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:02:51. Total running time: 1hr 8min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8079800009727478, time = 24.244602918624878
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:03:21. Total running time: 1hr 8min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:03:51. Total running time: 1hr 9min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8719599843025208, time = 25.373770713806152
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:04:21. Total running time: 1hr 9min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9140599966049194, time = 25.575221300125122
(train_cifar pid=3909) train time = 196.36088514328003

Trial train_cifar_ce6f6_00007 completed after 1 iterations at 2023-10-31 17:04:31. Total running time: 1hr 9min 48s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00007 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 209.19587 |
| time_total_s                                     209.19587 |
| training_iteration                                       1 |
| accuracy                                           0.91406 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00008 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00008 config           |
+------------------------------------------------+
| batch_size                                   8 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:54, 358835.79it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 675900.94it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2088807.00it/s]
  2%|         | 2850816/170498071 [00:00&lt;00:25, 6624503.17it/s]
  3%|         | 5570560/170498071 [00:00&lt;00:13, 12244628.32it/s]
  5%|         | 7831552/170498071 [00:00&lt;00:12, 12761101.87it/s]
  6%|         | 10715136/170498071 [00:01&lt;00:09, 16807657.55it/s]
  8%|         | 13631488/170498071 [00:01&lt;00:07, 20071712.02it/s]
  9%|         | 16121856/170498071 [00:01&lt;00:08, 18095516.64it/s]
 11%|         | 18874368/170498071 [00:01&lt;00:07, 20434859.21it/s]
 13%|        | 21790720/170498071 [00:01&lt;00:06, 22707220.59it/s]
 14%|        | 24444928/170498071 [00:01&lt;00:07, 20119115.37it/s]
 16%|        | 26968064/170498071 [00:01&lt;00:06, 21383503.43it/s]
 18%|        | 29851648/170498071 [00:01&lt;00:06, 23327973.43it/s]
 19%|        | 32636928/170498071 [00:02&lt;00:06, 20885335.42it/s]
 20%|        | 34930688/170498071 [00:02&lt;00:06, 21386289.56it/s]
 22%|       | 37879808/170498071 [00:02&lt;00:05, 23493592.50it/s]
 24%|       | 40632320/170498071 [00:02&lt;00:05, 24570431.96it/s]
 25%|       | 43188224/170498071 [00:02&lt;00:06, 20943747.47it/s]
 27%|       | 45907968/170498071 [00:02&lt;00:05, 22451815.41it/s]
 29%|       | 48758784/170498071 [00:02&lt;00:05, 24015259.85it/s]
 30%|       | 51347456/170498071 [00:02&lt;00:05, 21363518.97it/s]
 31%|      | 53641216/170498071 [00:02&lt;00:05, 21447517.27it/s]
 33%|      | 56393728/170498071 [00:03&lt;00:04, 23036104.67it/s]
 35%|      | 59146240/170498071 [00:03&lt;00:04, 24254726.55it/s]
 36%|      | 61734912/170498071 [00:03&lt;00:05, 21408053.34it/s]
 38%|      | 63995904/170498071 [00:03&lt;00:04, 21511048.73it/s]
 39%|      | 66617344/170498071 [00:03&lt;00:04, 22752792.01it/s]
 41%|      | 69238784/170498071 [00:03&lt;00:04, 23700266.29it/s]
 42%|     | 72122368/170498071 [00:03&lt;00:04, 21742157.34it/s]
 44%|     | 74383360/170498071 [00:03&lt;00:04, 21694347.84it/s]
 45%|     | 77037568/170498071 [00:03&lt;00:04, 22987188.65it/s]
 47%|     | 79691776/170498071 [00:04&lt;00:03, 23968397.53it/s]
 48%|     | 82444288/170498071 [00:04&lt;00:03, 24969491.04it/s]
 50%|     | 85000192/170498071 [00:04&lt;00:04, 20972072.41it/s]
 52%|    | 87916544/170498071 [00:04&lt;00:03, 23040881.72it/s]
 53%|    | 90669056/170498071 [00:04&lt;00:03, 21779773.74it/s]
 55%|    | 92962816/170498071 [00:04&lt;00:03, 21298237.55it/s]
 56%|    | 95223808/170498071 [00:04&lt;00:03, 21591417.34it/s]
 58%|    | 98041856/170498071 [00:04&lt;00:03, 23360106.37it/s]
 59%|    | 100761600/170498071 [00:04&lt;00:02, 24425038.78it/s]
 61%|    | 103251968/170498071 [00:05&lt;00:03, 21515878.24it/s]
 62%|   | 105512960/170498071 [00:05&lt;00:03, 21386879.35it/s]
 63%|   | 107970560/170498071 [00:05&lt;00:02, 22192961.66it/s]
 65%|   | 110788608/170498071 [00:05&lt;00:02, 23840756.84it/s]
 66%|   | 113311744/170498071 [00:05&lt;00:02, 22128381.07it/s]
 68%|   | 115605504/170498071 [00:05&lt;00:02, 22158915.21it/s]
 69%|   | 117866496/170498071 [00:05&lt;00:02, 21646303.13it/s]
 70%|   | 120061952/170498071 [00:05&lt;00:02, 21667477.74it/s]
 72%|  | 122847232/170498071 [00:05&lt;00:02, 23307109.95it/s]
 74%|  | 125566976/170498071 [00:06&lt;00:02, 22362548.75it/s]
 75%|  | 127926272/170498071 [00:06&lt;00:01, 22620793.50it/s]
 76%|  | 130220032/170498071 [00:06&lt;00:01, 22220263.14it/s]
 78%|  | 132481024/170498071 [00:06&lt;00:01, 21699726.85it/s]
 79%|  | 135168000/170498071 [00:06&lt;00:01, 23150806.60it/s]
 81%|  | 137854976/170498071 [00:06&lt;00:01, 22256185.19it/s]
 82%| | 140279808/170498071 [00:06&lt;00:01, 22721405.41it/s]
 84%| | 142573568/170498071 [00:06&lt;00:01, 22310664.99it/s]
 85%| | 144834560/170498071 [00:06&lt;00:01, 22033462.21it/s]
 86%| | 147062784/170498071 [00:07&lt;00:01, 21981825.19it/s]
 88%| | 149815296/170498071 [00:07&lt;00:00, 23427596.63it/s]
 89%| | 152174592/170498071 [00:07&lt;00:00, 21992940.27it/s]
 91%| | 154664960/170498071 [00:07&lt;00:00, 22609216.75it/s]
 92%|| 156958720/170498071 [00:07&lt;00:00, 22272476.34it/s]
 93%|| 159219712/170498071 [00:07&lt;00:00, 22028772.30it/s]
 95%|| 161480704/170498071 [00:07&lt;00:00, 22164199.77it/s]
 96%|| 164036608/170498071 [00:07&lt;00:00, 23110420.51it/s]
 98%|| 166363136/170498071 [00:07&lt;00:00, 21616590.25it/s]
 99%|| 168853504/170498071 [00:08&lt;00:00, 22523841.75it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20928341.58it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:04:51. Total running time: 1hr 10min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:05:21. Total running time: 1hr 10min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:05:51. Total running time: 1hr 11min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:06:21. Total running time: 1hr 11min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.42377999424934387, time = 68.82069039344788
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:06:52. Total running time: 1hr 12min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:07:22. Total running time: 1hr 12min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:07:52. Total running time: 1hr 13min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4830999970436096, time = 66.91194462776184
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:08:22. Total running time: 1hr 13min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:08:52. Total running time: 1hr 14min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:09:22. Total running time: 1hr 14min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5298599600791931, time = 67.32850790023804
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:09:52. Total running time: 1hr 15min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:10:22. Total running time: 1hr 15min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:10:52. Total running time: 1hr 16min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:11:22. Total running time: 1hr 16min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5626800060272217, time = 67.79275965690613
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:11:52. Total running time: 1hr 17min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:12:23. Total running time: 1hr 17min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:12:53. Total running time: 1hr 18min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.5894399881362915, time = 68.66705393791199
(train_cifar pid=3909) train time = 506.94449043273926

Trial train_cifar_ce6f6_00008 completed after 1 iterations at 2023-10-31 17:13:13. Total running time: 1hr 18min 30s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00008 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 521.16806 |
| time_total_s                                     521.16806 |
| training_iteration                                       1 |
| accuracy                                           0.58944 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00009 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00009 config           |
+------------------------------------------------+
| batch_size                                  32 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:01, 353619.85it/s]
  0%|          | 229376/170498071 [00:00&lt;04:15, 666647.34it/s]
  1%|          | 917504/170498071 [00:00&lt;01:22, 2059248.40it/s]
  2%|         | 2686976/170498071 [00:00&lt;00:27, 6130577.27it/s]
  3%|         | 5373952/170498071 [00:00&lt;00:14, 11748726.54it/s]
  5%|         | 7864320/170498071 [00:00&lt;00:12, 12850168.67it/s]
  6%|         | 10682368/170498071 [00:01&lt;00:09, 16672401.06it/s]
  8%|         | 13434880/170498071 [00:01&lt;00:08, 19504725.39it/s]
  9%|         | 16089088/170498071 [00:01&lt;00:08, 17869413.49it/s]
 11%|         | 18644992/170498071 [00:01&lt;00:07, 19750394.28it/s]
 12%|        | 20905984/170498071 [00:01&lt;00:07, 20485877.74it/s]
 14%|        | 23396352/170498071 [00:01&lt;00:06, 21631590.02it/s]
 15%|        | 25722880/170498071 [00:01&lt;00:06, 22021057.60it/s]
 16%|        | 28016640/170498071 [00:01&lt;00:06, 22262444.81it/s]
 18%|        | 30310400/170498071 [00:01&lt;00:06, 22420356.19it/s]
 19%|        | 32604160/170498071 [00:02&lt;00:06, 19775429.76it/s]
 20%|        | 34865152/170498071 [00:02&lt;00:06, 20505874.30it/s]
 22%|       | 37191680/170498071 [00:02&lt;00:06, 21236598.43it/s]
 23%|       | 39419904/170498071 [00:02&lt;00:06, 21493101.45it/s]
 25%|       | 41844736/170498071 [00:02&lt;00:05, 22240237.14it/s]
 26%|       | 44105728/170498071 [00:02&lt;00:05, 22086064.34it/s]
 27%|       | 46366720/170498071 [00:02&lt;00:05, 22205633.13it/s]
 29%|       | 48627712/170498071 [00:02&lt;00:05, 22111374.09it/s]
 30%|       | 50855936/170498071 [00:02&lt;00:05, 20395518.94it/s]
 31%|       | 53084160/170498071 [00:02&lt;00:05, 20871937.53it/s]
 32%|      | 55214080/170498071 [00:03&lt;00:05, 20964288.40it/s]
 34%|      | 57507840/170498071 [00:03&lt;00:05, 21485354.47it/s]
 35%|      | 59834368/170498071 [00:03&lt;00:05, 21898294.70it/s]
 36%|      | 62226432/170498071 [00:03&lt;00:04, 22435279.58it/s]
 38%|      | 64520192/170498071 [00:03&lt;00:04, 22397453.81it/s]
 39%|      | 66781184/170498071 [00:03&lt;00:04, 22445840.78it/s]
 40%|      | 69042176/170498071 [00:03&lt;00:04, 22171862.25it/s]
 42%|     | 71335936/170498071 [00:03&lt;00:04, 22336013.54it/s]
 43%|     | 73596928/170498071 [00:03&lt;00:04, 20596893.57it/s]
 44%|     | 75825152/170498071 [00:04&lt;00:04, 21010612.21it/s]
 46%|     | 77987840/170498071 [00:04&lt;00:04, 21176061.94it/s]
 47%|     | 80248832/170498071 [00:04&lt;00:04, 21444456.96it/s]
 48%|     | 82509824/170498071 [00:04&lt;00:04, 21669137.31it/s]
 50%|     | 84803584/170498071 [00:04&lt;00:03, 21998277.38it/s]
 51%|     | 87031808/170498071 [00:04&lt;00:03, 21994612.23it/s]
 52%|    | 89358336/170498071 [00:04&lt;00:03, 22346146.02it/s]
 54%|    | 91619328/170498071 [00:04&lt;00:03, 22344373.82it/s]
 55%|    | 93880320/170498071 [00:04&lt;00:03, 21668811.17it/s]
 56%|    | 96206848/170498071 [00:04&lt;00:03, 22022511.24it/s]
 58%|    | 98435072/170498071 [00:05&lt;00:03, 21397504.90it/s]
 59%|    | 100663296/170498071 [00:05&lt;00:03, 21582123.29it/s]
 60%|    | 102924288/170498071 [00:05&lt;00:03, 21844894.53it/s]
 62%|   | 105316352/170498071 [00:05&lt;00:02, 22318129.01it/s]
 63%|   | 107577344/170498071 [00:05&lt;00:02, 22318644.75it/s]
 64%|   | 109838336/170498071 [00:05&lt;00:02, 22340196.41it/s]
 66%|   | 112099328/170498071 [00:05&lt;00:02, 22184402.16it/s]
 67%|   | 114327552/170498071 [00:05&lt;00:02, 21496748.24it/s]
 68%|   | 116523008/170498071 [00:05&lt;00:02, 21562131.63it/s]
 70%|   | 118718464/170498071 [00:05&lt;00:02, 21628121.36it/s]
 71%|   | 120946688/170498071 [00:06&lt;00:02, 21817917.60it/s]
 72%|  | 123142144/170498071 [00:06&lt;00:02, 21750978.62it/s]
 74%|  | 125337600/170498071 [00:06&lt;00:02, 21722475.83it/s]
 75%|  | 127533056/170498071 [00:06&lt;00:01, 21767365.90it/s]
 76%|  | 129761280/170498071 [00:06&lt;00:01, 21683953.86it/s]
 77%|  | 131956736/170498071 [00:06&lt;00:01, 21667175.20it/s]
 79%|  | 134184960/170498071 [00:06&lt;00:01, 21827517.20it/s]
 80%|  | 136445952/170498071 [00:06&lt;00:01, 21689376.14it/s]
 81%| | 138641408/170498071 [00:06&lt;00:01, 21554304.45it/s]
 83%| | 140869632/170498071 [00:07&lt;00:01, 21641762.75it/s]
 84%| | 143097856/170498071 [00:07&lt;00:01, 21762844.68it/s]
 85%| | 145358848/170498071 [00:07&lt;00:01, 21983352.82it/s]
 87%| | 147587072/170498071 [00:07&lt;00:01, 21896074.64it/s]
 88%| | 149782528/170498071 [00:07&lt;00:00, 21825676.58it/s]
 89%| | 151977984/170498071 [00:07&lt;00:00, 21690523.84it/s]
 90%| | 154173440/170498071 [00:07&lt;00:00, 20864522.98it/s]
 92%|| 156368896/170498071 [00:07&lt;00:00, 21178034.14it/s]
 93%|| 158629888/170498071 [00:07&lt;00:00, 21518891.27it/s]
 94%|| 160890880/170498071 [00:07&lt;00:00, 21658020.88it/s]
 96%|| 163151872/170498071 [00:08&lt;00:00, 21915841.79it/s]
 97%|| 165380096/170498071 [00:08&lt;00:00, 21969873.99it/s]
 98%|| 167641088/170498071 [00:08&lt;00:00, 21980070.34it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20365777.70it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:13:23. Total running time: 1hr 18min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Files already downloaded and verified
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:13:53. Total running time: 1hr 19min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.350380003452301, time = 25.939082860946655
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:14:23. Total running time: 1hr 19min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.41301998496055603, time = 25.695386171340942
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:14:53. Total running time: 1hr 20min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:15:23. Total running time: 1hr 20min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.45009997487068176, time = 25.19807529449463
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:15:53. Total running time: 1hr 21min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.47495999932289124, time = 25.620651483535767
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:16:23. Total running time: 1hr 21min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.4984799921512604, time = 25.44916582107544
(train_cifar pid=3909) train time = 197.71213483810425

Trial train_cifar_ce6f6_00009 completed after 1 iterations at 2023-10-31 17:16:43. Total running time: 1hr 22min 0s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00009 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 210.39063 |
| time_total_s                                     210.39063 |
| training_iteration                                       1 |
| accuracy                                           0.49848 |
+------------------------------------------------------------+

Trial status: 10 TERMINATED
Current time: 2023-10-31 17:16:43. Total running time: 1hr 22min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00009 with accuracy=0.4984799921512604 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 32}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
| train_cifar_ce6f6_00009   TERMINATED             0.001             32        1            210.391      0.49848 |
+----------------------------------------------------------------------------------------------------------------+

Best trial config: {&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 32}
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/checkpoint_000000)
</code></pre>
</div>
<div class="output error" data-ename="AttributeError"
data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-6c7a25d68bfa&gt; in &lt;cell line: 33&gt;()
     32 print(&quot;Best trial config: {}&quot;.format(best_result .config))
     33 print(&quot;Best trial final validation accuracy: {}&quot;.format(
---&gt; 34     best_result .last_result[&quot;accuracy&quot;]))
     35 print(&quot;Best trial final validation accuracy: {}&quot;.format(
     36     best_result .last_result[&quot;accuracy&quot;]))

AttributeError: &#39;Result&#39; object has no attribute &#39;last_result&#39;
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="iHjugoqJF37p" data-outputId="9146a9dd-927b-47f5-ae95-bc46649b0436">
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 170498071/170498071 [00:01&lt;00:00, 101547643.04it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting /content/cifar-10-python.tar.gz to /content
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="rq2CeJz01Ym6" data-outputId="bfd82061-758f-4272-abf0-430429141ae4">
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="18">
<pre><code>MySENet(
  (features): Sequential(
    (init_block): SEInitBlock(
      (conv1): ConvBlock(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv2): ConvBlock(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv3): ConvBlock(
        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (stage1): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage2): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage3): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (final_pool): AdaptiveAvgPool2d(output_size=1)
  )
  (output): Sequential(
    (dropout): Dropout(p=0.2, inplace=False)
    (fc): Linear(in_features=1024, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="LkQlqZeF1Pq4" data-outputId="b38b7d33-ce76-445c-a26d-e1dc18a09e8c">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6185199618339539, time = 30.925504207611084
Epoch[1]: accuracy = 0.7169399857521057, time = 22.91165590286255
Epoch[2]: accuracy = 0.8411200046539307, time = 23.687257289886475
Epoch[3]: accuracy = 0.8774999976158142, time = 23.178725004196167
Epoch[4]: accuracy = 0.9222999811172485, time = 23.38175058364868
Epoch[5]: accuracy = 0.9480599761009216, time = 23.5803005695343
Epoch[6]: accuracy = 0.9622799754142761, time = 23.58870840072632
Epoch[7]: accuracy = 0.9548400044441223, time = 23.31749963760376
Epoch[8]: accuracy = 0.971780002117157, time = 23.381746768951416
Epoch[9]: accuracy = 0.9805600047111511, time = 23.53693127632141
Epoch[10]: accuracy = 0.9869599938392639, time = 23.716327905654907
Epoch[11]: accuracy = 0.9875999689102173, time = 23.601375341415405
Epoch[12]: accuracy = 0.9893800020217896, time = 23.627362489700317
Epoch[13]: accuracy = 0.9886599779129028, time = 23.62673258781433
Epoch[14]: accuracy = 0.9909399747848511, time = 23.595299005508423
Epoch[15]: accuracy = 0.99263995885849, time = 24.11177349090576
Epoch[16]: accuracy = 0.9919999837875366, time = 23.65166664123535
Epoch[17]: accuracy = 0.9910399913787842, time = 23.771889686584473
Epoch[18]: accuracy = 0.9924999475479126, time = 23.617604970932007
Epoch[19]: accuracy = 0.9962999820709229, time = 23.714627504348755
train time = 750.3111162185669
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="DKvNV2IC1w6L" data-outputId="48b1b3a9-e74d-404c-8be1-642c49e57665">
<div class="sourceCode" id="cb88"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7657999992370605
</code></pre>
</div>
</div>
<div class="cell markdown" id="H0G35h49ZQD1">
<p>,       ().
  optimizer:</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:391}"
id="VYyGB5yEbYu0" data-outputId="f955571e-05b3-4ff8-ceb3-562cf78379ad">
<div class="sourceCode" id="cb90"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr<span class="op">=</span>learning_rate, momentum<span class="op">=</span><span class="fl">0.9</span>, weight_decay<span class="op">=</span><span class="fl">5e-4</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output error" data-ename="KeyboardInterrupt"
data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-19-6858f02af3fd&gt; in &lt;cell line: 8&gt;()
     12         labels = labels.to(device)
     13 
---&gt; 14         outputs = cnn_model(images)
     15         loss = loss_function(outputs, labels)
     16 

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-&gt; 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

&lt;ipython-input-13-4783aae559ba&gt; in forward(self, x)
     49 
     50     def forward(self, x):
---&gt; 51         x = self.features(x)
     52         x = x.view(x.size(0), -1)
     53         x = self.output(x)

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-&gt; 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py in forward(self, input)
    213     def forward(self, input):
    214         for module in self:
--&gt; 215             input = module(input)
    216         return input
    217 

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-&gt; 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

&lt;ipython-input-8-5fa41af5b1db&gt; in forward(self, x)
     26     def forward(self, x):
     27         # print(&quot;SEInitBlock&quot;)
---&gt; 28         x = self.conv1(x)
     29         x = self.conv2(x)
     30         x = self.conv3(x)

&lt;ipython-input-5-0f9ddd96e222&gt; in __call__(self, x)
     31 
     32     def __call__(self, x):
---&gt; 33         x = self.conv(x)
     34         if self.use_bn:
     35             x = self.bn(x)

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1516             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517         else:
-&gt; 1518             return self._call_impl(*args, **kwargs)
   1519 
   1520     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1525                 or _global_backward_pre_hooks or _global_backward_hooks
   1526                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527             return forward_call(*args, **kwargs)
   1528 
   1529         try:

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    458 
    459     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 460         return self._conv_forward(input, self.weight, self.bias)
    461 
    462 class Conv3d(_ConvNd):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)
    454                             weight, bias, self.stride,
    455                             _pair(0), self.dilation, self.groups)
--&gt; 456         return F.conv2d(input, weight, bias, self.stride,
    457                         self.padding, self.dilation, self.groups)
    458 

KeyboardInterrupt: 
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="II58EgxZd7BF" data-outputId="6dd28ea2-3771-48b2-dd85-65dcba3b21df">
<div class="sourceCode" id="cb92"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7361999750137329
</code></pre>
</div>
</div>
<div class="cell markdown" id="ZXt73XeZbcqa">
<p> :</p>
</div>
<div class="cell code" id="u8tBl0LzZPTW">
<div class="sourceCode" id="cb94"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicBlock(torch.nn.Module):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_planes, out_planes, stride, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BasicBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm2d(in_planes)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span>stride,</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm2d(out_planes)</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv2d(out_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.droprate <span class="op">=</span> dropRate</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.equalInOut <span class="op">=</span> (in_planes <span class="op">==</span> out_planes)</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.convShortcut <span class="op">=</span> (<span class="kw">not</span> <span class="va">self</span>.equalInOut) <span class="kw">and</span> torch.nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span>stride,</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>) <span class="kw">or</span> <span class="va">None</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.equalInOut:</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.relu1(<span class="va">self</span>.bn1(x))</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.relu1(<span class="va">self</span>.bn1(x))</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu2(<span class="va">self</span>.bn2(<span class="va">self</span>.conv1(out <span class="cf">if</span> <span class="va">self</span>.equalInOut <span class="cf">else</span> x)))</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.droprate <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span><span class="va">self</span>.droprate)(out)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.add(x <span class="cf">if</span> <span class="va">self</span>.equalInOut <span class="cf">else</span> <span class="va">self</span>.convShortcut(x), out)</span></code></pre></div>
</div>
<div class="cell code" id="3H2QKuNHZ7z_">
<div class="sourceCode" id="cb95"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyCNN(torch.nn.Module):</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, depth<span class="op">=</span><span class="dv">28</span>, widen_factor<span class="op">=</span><span class="dv">10</span>, num_classes<span class="op">=</span><span class="dv">10</span>, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MyCNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>        nChannels <span class="op">=</span> [<span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span>widen_factor, <span class="dv">32</span><span class="op">*</span>widen_factor, <span class="dv">64</span><span class="op">*</span>widen_factor]</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>((depth <span class="op">-</span> <span class="dv">4</span>) <span class="op">%</span> <span class="dv">6</span> <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> (depth <span class="op">-</span> <span class="dv">4</span>) <span class="op">//</span> <span class="dv">6</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>        block <span class="op">=</span> BasicBlock</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st conv before any network block</span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">3</span>, nChannels[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st block</span></span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">0</span>], nChannels[<span class="dv">1</span>], n, stride<span class="op">=</span><span class="dv">1</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd block</span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">1</span>], nChannels[<span class="dv">2</span>], n, stride<span class="op">=</span><span class="dv">2</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3rd block</span></span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">2</span>], nChannels[<span class="dv">3</span>], n, stride<span class="op">=</span><span class="dv">2</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># global average pooling and classifier</span></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm2d(nChannels[<span class="dv">3</span>])</span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> torch.nn.Linear(nChannels[<span class="dv">3</span>], num_classes)</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nChannels <span class="op">=</span> nChannels[<span class="dv">3</span>]</span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, block, in_planes, out_planes, nb_layers, stride, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nb_layers):</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>            layers.append(block(i <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> in_planes <span class="kw">or</span> out_planes, out_planes, i <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> stride <span class="kw">or</span> <span class="dv">1</span>, dropRate))</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block1(out)</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block2(out)</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block3(out)</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.bn1(out))</span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.nn.AvgPool2d(<span class="dv">8</span>)(out)</span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.nChannels)</span>
<span id="cb95-43"><a href="#cb95-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc(out)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Fra-pQZFZ3vp" data-outputId="3b16a9be-f2ab-49a5-ebea-372368ea2b77">
<div class="sourceCode" id="cb96"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyCNN()</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="27">
<pre><code>MyCNN(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (block2): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (block3): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=640, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="JdGBwdbZbIZt" data-outputId="aafb91f7-61bc-47f6-d0dc-65a2aa88fb1e">
<div class="sourceCode" id="cb98"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6646599769592285, time = 216.0471315383911
Epoch[1]: accuracy = 0.780019998550415, time = 224.1840682029724
Epoch[2]: accuracy = 0.8097800016403198, time = 224.2865493297577
Epoch[3]: accuracy = 0.8788599967956543, time = 224.24250078201294
Epoch[4]: accuracy = 0.8708999752998352, time = 224.06565022468567
Epoch[5]: accuracy = 0.9170599579811096, time = 223.96775150299072
Epoch[6]: accuracy = 0.943340003490448, time = 224.04663920402527
Epoch[7]: accuracy = 0.9487800002098083, time = 223.8983211517334
Epoch[8]: accuracy = 0.9468599557876587, time = 223.89434027671814
Epoch[9]: accuracy = 0.9655199646949768, time = 223.94078588485718
Epoch[10]: accuracy = 0.9796199798583984, time = 223.8154489994049
Epoch[11]: accuracy = 0.9723199605941772, time = 224.03929209709167
Epoch[12]: accuracy = 0.9829399585723877, time = 223.99121856689453
Epoch[13]: accuracy = 0.9848600029945374, time = 224.0068883895874
Epoch[14]: accuracy = 0.9912599921226501, time = 224.071857213974
Epoch[15]: accuracy = 0.9803199768066406, time = 224.23546528816223
Epoch[16]: accuracy = 0.9924799799919128, time = 224.06210255622864
Epoch[17]: accuracy = 0.9927999973297119, time = 224.21832489967346
Epoch[18]: accuracy = 0.9984399676322937, time = 224.24001669883728
Epoch[19]: accuracy = 0.9944799542427063, time = 224.643981218338
train time = 5946.710683584213
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="KGWbK-21HA7H" data-outputId="d0040274-5246-4b2e-f1b7-466c9d856f26">
<div class="sourceCode" id="cb100"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8708999752998352
</code></pre>
</div>
</div>
<div class="cell markdown" id="3I5eUPFI5tM6">
<p>  - :</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ZAsANYm95wDw" data-outputId="361e8086-91a2-4dec-b0d7-60fcc419a4a8">
<div class="sourceCode" id="cb102"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="20">
<pre><code>MySENet(
  (features): Sequential(
    (init_block): SEInitBlock(
      (conv1): ConvBlock(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv2): ConvBlock(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv3): ConvBlock(
        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (stage1): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage2): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage3): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (final_pool): AdaptiveAvgPool2d(output_size=1)
  )
  (output): Sequential(
    (dropout): Dropout(p=0.2, inplace=False)
    (fc): Linear(in_features=1024, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="hZr2PDc25Vtk" data-outputId="cdfd8f22-ca31-40f7-f99e-2977f357f4a6">
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb104-18"><a href="#cb104-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb104-19"><a href="#cb104-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb104-20"><a href="#cb104-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb104-21"><a href="#cb104-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb104-22"><a href="#cb104-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb104-23"><a href="#cb104-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6634399890899658, time = 24.122328758239746
Epoch[1]: accuracy = 0.716759979724884, time = 23.211890935897827
Epoch[2]: accuracy = 0.835319995880127, time = 23.26145625114441
Epoch[3]: accuracy = 0.8821199536323547, time = 23.305988550186157
Epoch[4]: accuracy = 0.9200800061225891, time = 23.478625774383545
Epoch[5]: accuracy = 0.9249799847602844, time = 23.50758123397827
Epoch[6]: accuracy = 0.9584999680519104, time = 23.55452609062195
Epoch[7]: accuracy = 0.9628399610519409, time = 23.591036558151245
Epoch[8]: accuracy = 0.9675799608230591, time = 24.133979320526123
Epoch[9]: accuracy = 0.9758399724960327, time = 23.713650226593018
Epoch[10]: accuracy = 0.9860199689865112, time = 23.911246299743652
Epoch[11]: accuracy = 0.986799955368042, time = 23.437397480010986
Epoch[12]: accuracy = 0.9896199703216553, time = 23.587135314941406
Epoch[13]: accuracy = 0.9909999966621399, time = 23.57857060432434
Epoch[14]: accuracy = 0.9911999702453613, time = 23.62588858604431
Epoch[15]: accuracy = 0.9935999512672424, time = 24.28003740310669
Epoch[16]: accuracy = 0.9948399662971497, time = 23.970224380493164
Epoch[17]: accuracy = 0.9947999715805054, time = 23.831673860549927
Epoch[18]: accuracy = 0.9938399791717529, time = 23.811015844345093
Epoch[19]: accuracy = 0.9972999691963196, time = 23.820574283599854
Epoch[20]: accuracy = 0.9964399933815002, time = 23.670047283172607
Epoch[21]: accuracy = 0.9965399503707886, time = 23.702030420303345
Epoch[22]: accuracy = 0.9954999685287476, time = 24.313854694366455
Epoch[23]: accuracy = 0.9973399639129639, time = 23.70396852493286
Epoch[24]: accuracy = 0.9972399473190308, time = 23.64582371711731
Epoch[25]: accuracy = 0.997439980506897, time = 23.557504892349243
Epoch[26]: accuracy = 0.9980599880218506, time = 23.773165464401245
Epoch[27]: accuracy = 0.9987399578094482, time = 23.718693494796753
Epoch[28]: accuracy = 0.9987399578094482, time = 23.669323921203613
Epoch[29]: accuracy = 0.9979400038719177, time = 23.676069498062134
Epoch[30]: accuracy = 0.9988799691200256, time = 23.632630109786987
Epoch[31]: accuracy = 0.9987999796867371, time = 23.714840412139893
Epoch[32]: accuracy = 0.9991599917411804, time = 23.733750581741333
Epoch[33]: accuracy = 0.9989199638366699, time = 23.858803510665894
Epoch[34]: accuracy = 0.996679961681366, time = 23.602992296218872
Epoch[35]: accuracy = 0.9986400008201599, time = 23.731011629104614
Epoch[36]: accuracy = 0.9978799819946289, time = 23.797691583633423
Epoch[37]: accuracy = 0.998479962348938, time = 24.513044834136963
Epoch[38]: accuracy = 0.9985599517822266, time = 23.696763277053833
Epoch[39]: accuracy = 0.9965999722480774, time = 23.853230714797974
Epoch[40]: accuracy = 0.9960799813270569, time = 23.75005340576172
Epoch[41]: accuracy = 0.9975599646568298, time = 23.72229766845703
Epoch[42]: accuracy = 0.998199999332428, time = 23.64405083656311
Epoch[43]: accuracy = 0.9968400001525879, time = 23.686433792114258
Epoch[44]: accuracy = 0.998420000076294, time = 24.53098154067993
Epoch[45]: accuracy = 0.9977799654006958, time = 23.812706232070923
Epoch[46]: accuracy = 0.9981399774551392, time = 23.68435311317444
Epoch[47]: accuracy = 0.999019980430603, time = 23.733465671539307
Epoch[48]: accuracy = 0.9985999464988708, time = 23.779900550842285
Epoch[49]: accuracy = 0.9992599487304688, time = 23.777764320373535
Epoch[50]: accuracy = 0.9988199472427368, time = 23.73011088371277
Epoch[51]: accuracy = 0.9993399977684021, time = 24.7533175945282
Epoch[52]: accuracy = 0.9995799660682678, time = 23.65035390853882
Epoch[53]: accuracy = 0.999459981918335, time = 23.6651930809021
Epoch[54]: accuracy = 0.999239981174469, time = 23.688783645629883
Epoch[55]: accuracy = 0.9995599985122681, time = 23.725172758102417
Epoch[56]: accuracy = 0.9997199773788452, time = 23.775808095932007
Epoch[57]: accuracy = 0.9996399879455566, time = 23.651299238204956
Epoch[58]: accuracy = 0.9997199773788452, time = 24.380287170410156
Epoch[59]: accuracy = 0.9997400045394897, time = 23.650272607803345
Epoch[60]: accuracy = 0.9997999668121338, time = 23.739842891693115
Epoch[61]: accuracy = 0.9996599555015564, time = 23.708515882492065
Epoch[62]: accuracy = 0.9997999668121338, time = 23.651775598526
Epoch[63]: accuracy = 0.9998799562454224, time = 23.815360069274902
Epoch[64]: accuracy = 0.9998599886894226, time = 23.760265111923218
Epoch[65]: accuracy = 0.9998799562454224, time = 23.791862726211548
Epoch[66]: accuracy = 0.9995599985122681, time = 23.677488803863525
Epoch[67]: accuracy = 0.9997999668121338, time = 23.632988691329956
Epoch[68]: accuracy = 0.9999199509620667, time = 23.752504587173462
Epoch[69]: accuracy = 0.9999799728393555, time = 23.92630434036255
Epoch[70]: accuracy = 0.9999799728393555, time = 23.69932532310486
Epoch[71]: accuracy = 0.9999399781227112, time = 23.838200092315674
Epoch[72]: accuracy = 0.9999399781227112, time = 23.87880563735962
Epoch[73]: accuracy = 0.9999199509620667, time = 24.022541999816895
Epoch[74]: accuracy = 0.9999399781227112, time = 23.984588146209717
Epoch[75]: accuracy = 0.9998799562454224, time = 23.923177242279053
Epoch[76]: accuracy = 0.9998999834060669, time = 23.885048151016235
Epoch[77]: accuracy = 0.9999199509620667, time = 24.002543210983276
Epoch[78]: accuracy = 0.9999199509620667, time = 23.897764444351196
Epoch[79]: accuracy = 0.9999399781227112, time = 23.96094799041748
Epoch[80]: accuracy = 0.9999599456787109, time = 23.848762035369873
Epoch[81]: accuracy = 0.9999599456787109, time = 24.080783367156982
Epoch[82]: accuracy = 0.9999599456787109, time = 23.935168743133545
Epoch[83]: accuracy = 1.0, time = 24.15556764602661
Epoch[84]: accuracy = 0.9999599456787109, time = 23.805927753448486
Epoch[85]: accuracy = 1.0, time = 23.79943871498108
Epoch[86]: accuracy = 0.9999799728393555, time = 23.89954948425293
Epoch[87]: accuracy = 0.9999599456787109, time = 24.162338733673096
Epoch[88]: accuracy = 0.9999599456787109, time = 23.946527004241943
Epoch[89]: accuracy = 0.9999799728393555, time = 23.869439840316772
Epoch[90]: accuracy = 0.9999599456787109, time = 23.871333122253418
Epoch[91]: accuracy = 1.0, time = 23.944881200790405
Epoch[92]: accuracy = 1.0, time = 23.837337493896484
Epoch[93]: accuracy = 0.9999599456787109, time = 23.85340929031372
Epoch[94]: accuracy = 0.9999799728393555, time = 24.394443035125732
Epoch[95]: accuracy = 1.0, time = 23.856178998947144
Epoch[96]: accuracy = 0.9999599456787109, time = 23.876816034317017
Epoch[97]: accuracy = 0.9999599456787109, time = 23.915628671646118
Epoch[98]: accuracy = 0.9999599456787109, time = 23.885361909866333
Epoch[99]: accuracy = 0.9999799728393555, time = 23.863641500473022
Epoch[100]: accuracy = 0.9999799728393555, time = 23.77765679359436
Epoch[101]: accuracy = 0.9999799728393555, time = 24.312410354614258
Epoch[102]: accuracy = 0.9999799728393555, time = 23.728402853012085
Epoch[103]: accuracy = 0.9999799728393555, time = 23.84916043281555
Epoch[104]: accuracy = 1.0, time = 23.74649429321289
Epoch[105]: accuracy = 0.9999799728393555, time = 23.743155479431152
Epoch[106]: accuracy = 1.0, time = 23.785982608795166
Epoch[107]: accuracy = 0.9999599456787109, time = 23.75729751586914
Epoch[108]: accuracy = 0.9999799728393555, time = 24.192339181900024
Epoch[109]: accuracy = 0.9999599456787109, time = 23.95153021812439
Epoch[110]: accuracy = 0.9999799728393555, time = 23.805859327316284
Epoch[111]: accuracy = 0.9999799728393555, time = 23.815690755844116
Epoch[112]: accuracy = 0.9999399781227112, time = 23.8340802192688
Epoch[113]: accuracy = 0.9999799728393555, time = 23.7896671295166
Epoch[114]: accuracy = 1.0, time = 23.780721426010132
Epoch[115]: accuracy = 1.0, time = 24.165925979614258
Epoch[116]: accuracy = 1.0, time = 23.708655834197998
Epoch[117]: accuracy = 1.0, time = 23.7803852558136
Epoch[118]: accuracy = 0.9999799728393555, time = 23.838976621627808
Epoch[119]: accuracy = 0.9999799728393555, time = 23.75201725959778
Epoch[120]: accuracy = 0.9999799728393555, time = 23.72487211227417
Epoch[121]: accuracy = 1.0, time = 23.79038143157959
Epoch[122]: accuracy = 0.9999799728393555, time = 24.6807804107666
Epoch[123]: accuracy = 1.0, time = 23.804891347885132
Epoch[124]: accuracy = 1.0, time = 24.026015758514404
Epoch[125]: accuracy = 1.0, time = 23.80795979499817
Epoch[126]: accuracy = 1.0, time = 23.772376775741577
Epoch[127]: accuracy = 1.0, time = 23.639506578445435
Epoch[128]: accuracy = 0.9999599456787109, time = 23.686718463897705
Epoch[129]: accuracy = 1.0, time = 24.183906078338623
Epoch[130]: accuracy = 0.9999799728393555, time = 23.867984771728516
Epoch[131]: accuracy = 0.9999799728393555, time = 23.686965942382812
Epoch[132]: accuracy = 1.0, time = 23.70675754547119
Epoch[133]: accuracy = 1.0, time = 23.68428659439087
Epoch[134]: accuracy = 0.9999799728393555, time = 23.65080976486206
Epoch[135]: accuracy = 1.0, time = 23.62020206451416
Epoch[136]: accuracy = 1.0, time = 24.557939767837524
Epoch[137]: accuracy = 0.9999799728393555, time = 23.679954528808594
Epoch[138]: accuracy = 1.0, time = 23.693705320358276
Epoch[139]: accuracy = 0.9999799728393555, time = 23.585308074951172
Epoch[140]: accuracy = 1.0, time = 23.679938554763794
Epoch[141]: accuracy = 1.0, time = 23.760734796524048
Epoch[142]: accuracy = 1.0, time = 23.57346510887146
Epoch[143]: accuracy = 0.9999799728393555, time = 24.13339138031006
Epoch[144]: accuracy = 1.0, time = 23.623027324676514
Epoch[145]: accuracy = 1.0, time = 23.66124939918518
Epoch[146]: accuracy = 1.0, time = 23.70865821838379
Epoch[147]: accuracy = 1.0, time = 23.684845447540283
Epoch[148]: accuracy = 1.0, time = 23.86601734161377
Epoch[149]: accuracy = 1.0, time = 24.01506280899048
Epoch[150]: accuracy = 1.0, time = 24.540356397628784
Epoch[151]: accuracy = 1.0, time = 23.791738748550415
Epoch[152]: accuracy = 1.0, time = 23.597903966903687
Epoch[153]: accuracy = 0.9999799728393555, time = 23.71092700958252
Epoch[154]: accuracy = 1.0, time = 23.744665384292603
Epoch[155]: accuracy = 1.0, time = 23.75165557861328
Epoch[156]: accuracy = 1.0, time = 23.558255910873413
Epoch[157]: accuracy = 0.9999799728393555, time = 24.178043842315674
Epoch[158]: accuracy = 0.9999399781227112, time = 23.670772075653076
Epoch[159]: accuracy = 1.0, time = 23.719915628433228
Epoch[160]: accuracy = 1.0, time = 23.688920497894287
Epoch[161]: accuracy = 1.0, time = 23.771072149276733
Epoch[162]: accuracy = 1.0, time = 23.773135662078857
Epoch[163]: accuracy = 1.0, time = 23.733024835586548
Epoch[164]: accuracy = 1.0, time = 24.17136812210083
Epoch[165]: accuracy = 1.0, time = 23.719517707824707
Epoch[166]: accuracy = 1.0, time = 23.659939289093018
Epoch[167]: accuracy = 1.0, time = 23.639638662338257
Epoch[168]: accuracy = 1.0, time = 23.606274843215942
Epoch[169]: accuracy = 1.0, time = 23.53998112678528
Epoch[170]: accuracy = 1.0, time = 23.66230082511902
Epoch[171]: accuracy = 1.0, time = 24.487303018569946
Epoch[172]: accuracy = 0.9999799728393555, time = 23.656899452209473
Epoch[173]: accuracy = 1.0, time = 23.722230434417725
Epoch[174]: accuracy = 0.9999799728393555, time = 23.827127695083618
Epoch[175]: accuracy = 1.0, time = 23.64025330543518
Epoch[176]: accuracy = 1.0, time = 23.679829120635986
Epoch[177]: accuracy = 1.0, time = 23.752427101135254
Epoch[178]: accuracy = 1.0, time = 23.887234449386597
Epoch[179]: accuracy = 1.0, time = 23.731553316116333
Epoch[180]: accuracy = 0.9999799728393555, time = 23.61935830116272
Epoch[181]: accuracy = 1.0, time = 23.737061023712158
Epoch[182]: accuracy = 0.9999799728393555, time = 23.61503553390503
Epoch[183]: accuracy = 0.9999799728393555, time = 23.537933111190796
Epoch[184]: accuracy = 1.0, time = 23.48256206512451
Epoch[185]: accuracy = 0.9999399781227112, time = 23.673277854919434
Epoch[186]: accuracy = 1.0, time = 23.65608501434326
Epoch[187]: accuracy = 1.0, time = 23.728358507156372
Epoch[188]: accuracy = 1.0, time = 23.66857671737671
Epoch[189]: accuracy = 1.0, time = 23.656216382980347
Epoch[190]: accuracy = 1.0, time = 23.81125831604004
Epoch[191]: accuracy = 1.0, time = 23.77385950088501
Epoch[192]: accuracy = 1.0, time = 23.78359293937683
Epoch[193]: accuracy = 0.9999799728393555, time = 23.763309955596924
Epoch[194]: accuracy = 1.0, time = 23.89836311340332
Epoch[195]: accuracy = 1.0, time = 23.63127589225769
Epoch[196]: accuracy = 1.0, time = 23.536782264709473
Epoch[197]: accuracy = 0.9999799728393555, time = 23.585378885269165
Epoch[198]: accuracy = 1.0, time = 23.681058406829834
Epoch[199]: accuracy = 1.0, time = 23.675348043441772
train time = 7438.533365249634
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="DKMUE6YJ55S6" data-outputId="46f213ec-6bae-41d0-cc58-bb18c64f44c2">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7849000096321106
</code></pre>
</div>
</div>
<div class="cell markdown" id="_hrxs_6qjckQ">
<p> Resnet:</p>
</div>
<div class="cell code" id="gTTWXviRkOHd">
<div class="sourceCode" id="cb108"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyResNet(torch.nn.Module):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MyResNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv3x3_block(</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_maxpool_1 <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>            torch.nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residue1 <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)))</span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_maxpool_2 <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>            torch.nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb108-30"><a href="#cb108-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_maxpool_3 <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb108-31"><a href="#cb108-31" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-32"><a href="#cb108-32" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb108-33"><a href="#cb108-33" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb108-34"><a href="#cb108-34" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb108-35"><a href="#cb108-35" aria-hidden="true" tabindex="-1"></a>             torch.nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb108-36"><a href="#cb108-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residue2 <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb108-37"><a href="#cb108-37" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-38"><a href="#cb108-38" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb108-39"><a href="#cb108-39" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb108-40"><a href="#cb108-40" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb108-41"><a href="#cb108-41" aria-hidden="true" tabindex="-1"></a>            conv3x3_block(</span>
<span id="cb108-42"><a href="#cb108-42" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb108-43"><a href="#cb108-43" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb108-44"><a href="#cb108-44" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)))</span>
<span id="cb108-45"><a href="#cb108-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-46"><a href="#cb108-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_pooling <span class="op">=</span> torch.nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb108-47"><a href="#cb108-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Dropout(<span class="fl">0.2</span>), torch.nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>))</span>
<span id="cb108-48"><a href="#cb108-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-49"><a href="#cb108-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb108-50"><a href="#cb108-50" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb108-51"><a href="#cb108-51" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_maxpool_1(out)</span>
<span id="cb108-52"><a href="#cb108-52" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.residue1(out) <span class="op">+</span> out</span>
<span id="cb108-53"><a href="#cb108-53" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_maxpool_2(out)</span>
<span id="cb108-54"><a href="#cb108-54" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_maxpool_3(out)</span>
<span id="cb108-55"><a href="#cb108-55" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.residue2(out) <span class="op">+</span> out</span>
<span id="cb108-56"><a href="#cb108-56" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.final_pooling(out)</span>
<span id="cb108-57"><a href="#cb108-57" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.dense(out)</span>
<span id="cb108-58"><a href="#cb108-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="89MUBEcEq-1Y" data-outputId="f9a3126f-1856-40ff-8049-a1a3ad5ee166">
<div class="sourceCode" id="cb109"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyResNet()</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>cnn_model</span></code></pre></div>
<div class="output execute_result" data-execution_count="25">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ijkXAS7GrLXI" data-outputId="d8c2bf12-f0ed-4142-ce25-b7f99898f103">
<div class="sourceCode" id="cb111"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb111-17"><a href="#cb111-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-18"><a href="#cb111-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb111-19"><a href="#cb111-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb111-20"><a href="#cb111-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb111-21"><a href="#cb111-21" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb111-22"><a href="#cb111-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb111-23"><a href="#cb111-23" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb111-24"><a href="#cb111-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.5675199627876282, time = 32.50957226753235
Epoch[1]: accuracy = 0.6653800010681152, time = 31.323213577270508
Epoch[2]: accuracy = 0.7318999767303467, time = 31.73861861228943
Epoch[3]: accuracy = 0.7467199563980103, time = 31.48417615890503
Epoch[4]: accuracy = 0.7937600016593933, time = 31.488081216812134
Epoch[5]: accuracy = 0.8130599856376648, time = 32.00517988204956
Epoch[6]: accuracy = 0.8452199697494507, time = 31.57983160018921
Epoch[7]: accuracy = 0.8733800053596497, time = 31.436741590499878
Epoch[8]: accuracy = 0.8791799545288086, time = 31.51221466064453
Epoch[9]: accuracy = 0.903659999370575, time = 31.449609518051147
Epoch[10]: accuracy = 0.9253999590873718, time = 31.449159383773804
Epoch[11]: accuracy = 0.9301999807357788, time = 31.49715518951416
Epoch[12]: accuracy = 0.9357399940490723, time = 31.393566846847534
Epoch[13]: accuracy = 0.9510799646377563, time = 31.525501012802124
Epoch[14]: accuracy = 0.9553799629211426, time = 31.386075019836426
Epoch[15]: accuracy = 0.9644799828529358, time = 31.513094186782837
Epoch[16]: accuracy = 0.9699999690055847, time = 31.413445711135864
Epoch[17]: accuracy = 0.9698799848556519, time = 31.445122480392456
Epoch[18]: accuracy = 0.9772799611091614, time = 31.4307644367218
Epoch[19]: accuracy = 0.9836399555206299, time = 31.410396337509155
train time = 916.0078914165497
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="hIHc-kM2re0r" data-outputId="c8400128-c47f-4a36-8486-e216f9d515ec">
<div class="sourceCode" id="cb113"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8039000034332275
</code></pre>
</div>
</div>
<div class="cell markdown" id="baO5sHw4QK8O">
<p> batch_size</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="t9QoXpvyQ5Aj" data-outputId="72a931f9-87f4-4675-db09-6a1f1d60f38f">
<div class="sourceCode" id="cb115"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="-mSGUMY9QKBi" data-outputId="561a0a70-dfbc-4589-b565-10356cd3f3b8">
<div class="sourceCode" id="cb117"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyResNet()</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>cnn_model</span></code></pre></div>
<div class="output execute_result" data-execution_count="30">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="z1VtO27mQGlX" data-outputId="61b224cf-e3e2-4b34-f9b5-ae6e2b5e41db">
<div class="sourceCode" id="cb119"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.429639995098114, time = 22.260792016983032
Epoch[1]: accuracy = 0.5027399659156799, time = 21.34193515777588
Epoch[2]: accuracy = 0.5474199652671814, time = 21.33625864982605
Epoch[3]: accuracy = 0.5890799760818481, time = 21.85615873336792
Epoch[4]: accuracy = 0.6189199686050415, time = 21.332317113876343
Epoch[5]: accuracy = 0.6416199803352356, time = 21.354668140411377
Epoch[6]: accuracy = 0.6661199927330017, time = 21.386837244033813
Epoch[7]: accuracy = 0.687279999256134, time = 21.37298822402954
Epoch[8]: accuracy = 0.6996200084686279, time = 21.35922908782959
Epoch[9]: accuracy = 0.7189199924468994, time = 21.353496313095093
Epoch[10]: accuracy = 0.7381599545478821, time = 21.366549730300903
Epoch[11]: accuracy = 0.7442399859428406, time = 21.352461338043213
Epoch[12]: accuracy = 0.7637400031089783, time = 21.990505695343018
Epoch[13]: accuracy = 0.7694199681282043, time = 23.401999711990356
Epoch[14]: accuracy = 0.7848199605941772, time = 21.35201072692871
Epoch[15]: accuracy = 0.7927799820899963, time = 21.3564932346344
Epoch[16]: accuracy = 0.8042599558830261, time = 21.33446216583252
Epoch[17]: accuracy = 0.8220199942588806, time = 21.294798851013184
Epoch[18]: accuracy = 0.8251199722290039, time = 21.332383632659912
Epoch[19]: accuracy = 0.8353399634361267, time = 21.356523752212524
train time = 609.9428925514221
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="q37rSNZMQ7Gr" data-outputId="0c4cc95c-4049-4b30-facd-80c040368c9f">
<div class="sourceCode" id="cb121"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.6920999884605408
</code></pre>
</div>
</div>
<div class="cell markdown" id="Cvg74JOuSaUT">
<p> optimizer:</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="oS2_OUP1ShXB" data-outputId="096c7e8c-9779-45b9-d803-d3bdf4be838b">
<div class="sourceCode" id="cb123"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyResNet()</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>cnn_model</span></code></pre></div>
<div class="output execute_result" data-execution_count="33">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="22-KRLeESca7" data-outputId="bca9e65e-7723-4041-f846-80966433b06d">
<div class="sourceCode" id="cb125"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-15"><a href="#cb125-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb125-16"><a href="#cb125-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb125-17"><a href="#cb125-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-18"><a href="#cb125-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb125-19"><a href="#cb125-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb125-20"><a href="#cb125-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb125-21"><a href="#cb125-21" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb125-22"><a href="#cb125-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb125-23"><a href="#cb125-23" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb125-24"><a href="#cb125-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.7186399698257446, time = 22.137978076934814
Epoch[1]: accuracy = 0.8066999912261963, time = 21.911046028137207
Epoch[2]: accuracy = 0.8587599992752075, time = 21.856054306030273
Epoch[3]: accuracy = 0.8898800015449524, time = 22.018725633621216
Epoch[4]: accuracy = 0.9099400043487549, time = 21.88341498374939
Epoch[5]: accuracy = 0.9293999671936035, time = 21.950273513793945
Epoch[6]: accuracy = 0.9459399580955505, time = 21.97384476661682
Epoch[7]: accuracy = 0.9527599811553955, time = 21.966202974319458
Epoch[8]: accuracy = 0.9638399481773376, time = 21.933661699295044
Epoch[9]: accuracy = 0.9716999530792236, time = 21.937330961227417
Epoch[10]: accuracy = 0.9784999489784241, time = 21.89295983314514
Epoch[11]: accuracy = 0.9782599806785583, time = 21.89150834083557
Epoch[12]: accuracy = 0.9814399480819702, time = 21.941291332244873
Epoch[13]: accuracy = 0.9850599765777588, time = 21.905558586120605
Epoch[14]: accuracy = 0.9827399849891663, time = 21.911592960357666
Epoch[15]: accuracy = 0.9845199584960938, time = 21.89947772026062
Epoch[16]: accuracy = 0.9869799613952637, time = 21.924667835235596
Epoch[17]: accuracy = 0.9861399531364441, time = 21.947152137756348
Epoch[18]: accuracy = 0.9859199523925781, time = 21.937124490737915
Epoch[19]: accuracy = 0.9920399785041809, time = 21.97798442840576
train time = 613.5147731304169
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Or6u-6pkSg11" data-outputId="4fa7d1d1-8e9c-4552-8ab7-56f75c3da204">
<div class="sourceCode" id="cb127"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8744999766349792
</code></pre>
</div>
</div>
<div class="cell markdown" id="iHAwKr5JWryJ">
<p> batch_size +  optimizer</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="XxZuDJsfWfd9" data-outputId="9b0f9635-2b1e-4c0b-ce30-3390f241126a">
<div class="sourceCode" id="cb129"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb129-15"><a href="#cb129-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-16"><a href="#cb129-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb129-17"><a href="#cb129-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb129-18"><a href="#cb129-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-19"><a href="#cb129-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb129-20"><a href="#cb129-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb129-21"><a href="#cb129-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-22"><a href="#cb129-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb129-23"><a href="#cb129-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb129-24"><a href="#cb129-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="dIjyS5ANWxX1" data-outputId="7f6c270c-c2d9-4dcc-96fa-6e58cf06ddf3">
<div class="sourceCode" id="cb131"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyResNet()</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>cnn_model</span></code></pre></div>
<div class="output execute_result" data-execution_count="45">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="X6ybIkyBW0O7" data-outputId="b8abb496-8a3a-462d-a8d9-979fef77ab20">
<div class="sourceCode" id="cb133"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb133-22"><a href="#cb133-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb133-23"><a href="#cb133-23" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb133-24"><a href="#cb133-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6878600120544434, time = 23.394200086593628
Epoch[1]: accuracy = 0.7754999995231628, time = 24.55526351928711
Epoch[2]: accuracy = 0.8452199697494507, time = 28.300565004348755
Epoch[3]: accuracy = 0.876479983329773, time = 24.6095232963562
Epoch[4]: accuracy = 0.899899959564209, time = 23.760430574417114
Epoch[5]: accuracy = 0.9155199527740479, time = 24.869466066360474
Epoch[6]: accuracy = 0.9413399696350098, time = 23.811559200286865
Epoch[7]: accuracy = 0.960159957408905, time = 23.814013242721558
Epoch[8]: accuracy = 0.965399980545044, time = 23.8287513256073
Epoch[9]: accuracy = 0.9716999530792236, time = 23.780418872833252
Epoch[10]: accuracy = 0.9840999841690063, time = 23.769834518432617
Epoch[11]: accuracy = 0.9748599529266357, time = 24.31764531135559
Epoch[12]: accuracy = 0.9854799509048462, time = 23.758888483047485
Epoch[13]: accuracy = 0.9827999472618103, time = 24.390244960784912
Epoch[14]: accuracy = 0.9860599637031555, time = 23.772758722305298
Epoch[15]: accuracy = 0.986579954624176, time = 23.770816802978516
Epoch[16]: accuracy = 0.9910199642181396, time = 23.736677408218384
Epoch[17]: accuracy = 0.9869199991226196, time = 23.741962671279907
Epoch[18]: accuracy = 0.9889799952507019, time = 23.77177095413208
Epoch[19]: accuracy = 0.9918399453163147, time = 24.168721437454224
train time = 672.210871219635
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="dJe5wdhgW6RT" data-outputId="10e02380-6a0c-40f0-d2c5-0835877095cb">
<div class="sourceCode" id="cb135"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8721999526023865
</code></pre>
</div>
</div>
<div class="cell markdown" id="xQoZN3vEfepK">
<p>  :</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="YO-kQFOgg1h8" data-outputId="80c46a2b-a38f-4df1-9d74-9cd9b9b92bea">
<div class="sourceCode" id="cb137"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-13"><a href="#cb137-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb137-14"><a href="#cb137-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb137-15"><a href="#cb137-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-16"><a href="#cb137-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb137-17"><a href="#cb137-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb137-18"><a href="#cb137-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-19"><a href="#cb137-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb137-20"><a href="#cb137-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb137-21"><a href="#cb137-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb137-22"><a href="#cb137-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb137-23"><a href="#cb137-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb137-24"><a href="#cb137-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_57956fd07bd348e8b83755a30a5758f3/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="VpKF_YFPfeSH" data-outputId="6685f2fe-f4f8-4923-f409-82f876ba5f2b">
<div class="sourceCode" id="cb139"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyResNet()</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>cnn_model</span></code></pre></div>
<div class="output execute_result" data-execution_count="59">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code" id="uDAoZoSAZvKu">
<div class="sourceCode" id="cb141"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(model):</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(model)</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">type</span>(model) <span class="op">==</span> torch.nn.Conv2d:</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>    torch.nn.init.xavier_normal_(model.weight)</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model.weight)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="EBp_KxsQfWAN" data-outputId="75af24b9-1619-4295-d748-ce0acb3615e9">
<div class="sourceCode" id="cb142"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>cnn_model.<span class="bu">apply</span>(init_weights)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[-0.0649, -0.0079,  0.0560],
          [ 0.0007, -0.0176,  0.0338],
          [ 0.0124, -0.0595, -0.0217]],

         [[ 0.0569,  0.0364, -0.0991],
          [-0.0815, -0.0583, -0.0002],
          [ 0.0252, -0.0506,  0.0198]],

         [[-0.0987,  0.0398,  0.0194],
          [ 0.0444, -0.0615, -0.0879],
          [-0.0142,  0.0558, -0.0333]]],


        [[[ 0.0516,  0.0607,  0.0494],
          [ 0.0375,  0.0206, -0.0837],
          [-0.0253,  0.0159,  0.0285]],

         [[-0.0043,  0.1110, -0.0016],
          [ 0.0895,  0.0441,  0.0175],
          [-0.0731, -0.0646,  0.0138]],

         [[ 0.0221,  0.0133, -0.0097],
          [ 0.0534, -0.0567,  0.1549],
          [ 0.0011,  0.0018,  0.0671]]],


        [[[-0.0674,  0.0720,  0.0161],
          [ 0.0294, -0.0017,  0.0334],
          [-0.0257,  0.0005,  0.0242]],

         [[ 0.0279,  0.0364, -0.0339],
          [ 0.0152,  0.0354,  0.0377],
          [ 0.0326,  0.0739, -0.0433]],

         [[-0.0020,  0.0261, -0.1023],
          [ 0.0708, -0.0080,  0.0368],
          [ 0.1057,  0.0301,  0.0118]]],


        ...,


        [[[-0.0055,  0.0994,  0.0887],
          [ 0.0026, -0.1001,  0.1115],
          [ 0.1226, -0.0039, -0.1057]],

         [[-0.0395, -0.0395,  0.0339],
          [-0.0131,  0.0916, -0.0773],
          [ 0.0142, -0.0697,  0.0202]],

         [[-0.0840, -0.0185, -0.0193],
          [-0.0007, -0.0975,  0.0991],
          [ 0.1133,  0.0016,  0.0457]]],


        [[[ 0.0510,  0.0645,  0.1279],
          [-0.0488, -0.0462,  0.0625],
          [ 0.0276, -0.0124, -0.1350]],

         [[-0.0753, -0.0881,  0.0123],
          [ 0.0764,  0.0419,  0.0878],
          [ 0.0223,  0.0654, -0.0245]],

         [[-0.0024,  0.0206, -0.0570],
          [-0.0439,  0.0280,  0.0483],
          [-0.0036,  0.0342,  0.0502]]],


        [[[ 0.0537, -0.0533, -0.0262],
          [-0.0286,  0.0351, -0.0077],
          [ 0.0142, -0.0447, -0.0707]],

         [[ 0.0295, -0.0475, -0.0062],
          [ 0.0032,  0.0616,  0.0179],
          [-0.0171,  0.0544,  0.0041]],

         [[ 0.0743, -0.0602,  0.0035],
          [-0.0740,  0.0232,  0.1502],
          [-0.0688,  0.0928,  0.0319]]]], requires_grad=True)
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[ 0.0017, -0.0151, -0.0537],
          [ 0.0649, -0.0455,  0.0211],
          [-0.0007, -0.0241, -0.0217]],

         [[ 0.0499, -0.0276, -0.0488],
          [-0.0144, -0.0137,  0.0349],
          [-0.0470,  0.0242,  0.0538]],

         [[-0.0086, -0.0088,  0.0320],
          [ 0.0140, -0.0372,  0.0137],
          [ 0.0749, -0.0331, -0.0107]],

         ...,

         [[ 0.0179,  0.0706, -0.0028],
          [ 0.0236, -0.0127, -0.0130],
          [-0.0434, -0.0036,  0.0023]],

         [[ 0.0192, -0.0238, -0.0413],
          [-0.0710, -0.0144, -0.0120],
          [ 0.0010, -0.0229,  0.0249]],

         [[ 0.0180, -0.0332, -0.0573],
          [ 0.0172, -0.0199, -0.0067],
          [ 0.0972,  0.0323,  0.0072]]],


        [[[ 0.0407,  0.0413, -0.0509],
          [ 0.0260,  0.0638,  0.0424],
          [ 0.0091,  0.0145, -0.0024]],

         [[ 0.0104,  0.0162, -0.0354],
          [ 0.0360, -0.0281,  0.0036],
          [ 0.0246,  0.0649, -0.0228]],

         [[ 0.0149,  0.0051, -0.0249],
          [ 0.0086, -0.0338,  0.0385],
          [ 0.0512,  0.0452,  0.0654]],

         ...,

         [[-0.0064,  0.0294, -0.0623],
          [ 0.0494,  0.0078,  0.0183],
          [ 0.0448, -0.0151, -0.0396]],

         [[ 0.0093, -0.0460, -0.0467],
          [-0.0123, -0.0065,  0.0427],
          [ 0.0253, -0.0131,  0.0275]],

         [[ 0.0182, -0.0003,  0.0193],
          [ 0.0121,  0.0077,  0.0345],
          [ 0.0204,  0.0569,  0.0262]]],


        [[[ 0.0694, -0.0792, -0.0737],
          [ 0.0313, -0.0112, -0.0162],
          [-0.0284,  0.0295,  0.0560]],

         [[ 0.0671, -0.0237, -0.0179],
          [ 0.0419, -0.0759,  0.0295],
          [ 0.0454,  0.0141, -0.0384]],

         [[ 0.0034, -0.0272,  0.0394],
          [-0.0107, -0.0528,  0.0347],
          [-0.0097,  0.0073,  0.0069]],

         ...,

         [[ 0.0271, -0.0216, -0.0098],
          [-0.0597, -0.0067,  0.0182],
          [ 0.0027, -0.0268,  0.0223]],

         [[-0.0003,  0.0524, -0.0555],
          [-0.0130,  0.0087,  0.0360],
          [ 0.0043, -0.0006, -0.0033]],

         [[ 0.0189,  0.0393, -0.0043],
          [-0.0301,  0.0058,  0.0106],
          [ 0.0085,  0.0477, -0.0729]]],


        ...,


        [[[ 0.0079, -0.0214,  0.0368],
          [ 0.0685,  0.0482, -0.0218],
          [-0.0019,  0.0031,  0.0006]],

         [[ 0.0400,  0.0215,  0.0147],
          [-0.0445, -0.0259,  0.0127],
          [ 0.0300, -0.0437,  0.0077]],

         [[-0.0278, -0.0157,  0.0002],
          [-0.0844,  0.0325, -0.0022],
          [-0.0399, -0.0145,  0.0134]],

         ...,

         [[ 0.0248, -0.0107, -0.0051],
          [-0.0548, -0.0519, -0.0110],
          [-0.0285, -0.0257, -0.0471]],

         [[ 0.0147,  0.0660, -0.0263],
          [ 0.0297, -0.0526,  0.0407],
          [-0.0282, -0.0115, -0.0435]],

         [[ 0.0274, -0.0177, -0.0028],
          [ 0.0337, -0.0413, -0.0394],
          [ 0.0386,  0.0613, -0.0207]]],


        [[[-0.0287, -0.0548, -0.0325],
          [ 0.0371, -0.0221,  0.0327],
          [-0.0029,  0.0283,  0.0568]],

         [[-0.0252, -0.0082,  0.0469],
          [-0.0129,  0.0346,  0.0175],
          [-0.0130, -0.0145, -0.0418]],

         [[ 0.0475,  0.0526,  0.0303],
          [-0.0086,  0.0687, -0.0142],
          [-0.0397, -0.0087,  0.0060]],

         ...,

         [[-0.0299, -0.0265,  0.0158],
          [-0.0150, -0.0002,  0.0584],
          [ 0.0118, -0.0038, -0.0170]],

         [[-0.0403, -0.0224,  0.0163],
          [ 0.0290, -0.0243,  0.0236],
          [ 0.0371,  0.0551, -0.0497]],

         [[-0.0084, -0.0315, -0.0733],
          [ 0.0023, -0.0104,  0.0572],
          [-0.0738,  0.0467, -0.0220]]],


        [[[-0.0236, -0.0586, -0.0026],
          [-0.1097,  0.0059,  0.0228],
          [ 0.0564,  0.0237, -0.0089]],

         [[ 0.0336, -0.0419, -0.0422],
          [-0.0207,  0.0193,  0.0302],
          [ 0.0220,  0.0131, -0.0146]],

         [[ 0.0238,  0.0037,  0.0438],
          [-0.0228,  0.0130,  0.0294],
          [-0.0512, -0.0251,  0.0323]],

         ...,

         [[-0.0562,  0.0446,  0.0050],
          [-0.0055, -0.0238,  0.0363],
          [ 0.0122, -0.0043, -0.0105]],

         [[ 0.0410, -0.0174, -0.0020],
          [ 0.0383, -0.0351, -0.0525],
          [ 0.0443,  0.0413, -0.0040]],

         [[-0.0337,  0.0223,  0.0342],
          [ 0.0300,  0.0011, -0.0220],
          [-0.0537, -0.0694, -0.0013]]]], requires_grad=True)
BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Sequential(
  (0): ConvBlock(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[-9.9121e-03, -1.0123e-02,  2.2333e-02],
          [-2.9352e-02, -4.9814e-02, -4.7136e-02],
          [ 7.8113e-03,  5.0567e-02,  5.9028e-03]],

         [[ 3.7098e-02,  2.9688e-02, -2.4821e-02],
          [-2.0308e-03, -6.0239e-02, -2.5410e-02],
          [-2.2843e-02, -7.7042e-03, -4.9238e-03]],

         [[ 1.4678e-02, -1.8910e-03, -1.0896e-02],
          [-2.1218e-02,  1.6972e-02, -1.1504e-02],
          [ 3.0194e-03, -1.9683e-02,  1.6968e-02]],

         ...,

         [[ 1.2528e-02,  1.5524e-03, -3.3847e-03],
          [-2.6309e-05,  2.5509e-02, -7.8015e-02],
          [-3.9994e-02,  5.0778e-03, -1.2592e-02]],

         [[ 6.1516e-02, -1.4670e-02,  4.8698e-04],
          [ 7.5006e-04, -6.2429e-03, -2.2662e-02],
          [-2.0268e-02,  2.6127e-02, -1.1347e-02]],

         [[-2.3778e-02,  4.6517e-03,  2.1306e-02],
          [ 2.0574e-02, -9.4807e-04,  4.9196e-02],
          [-5.4917e-03,  2.5379e-03, -1.1036e-03]]],


        [[[ 1.6918e-02, -1.0319e-02,  1.5064e-02],
          [ 1.2052e-02, -9.3650e-03,  6.1596e-02],
          [-2.2849e-02,  3.8972e-02,  4.9902e-02]],

         [[ 3.8035e-02,  2.8692e-02,  2.2684e-02],
          [ 2.2957e-02,  2.8440e-02,  4.1978e-02],
          [-2.4492e-02,  1.7685e-02, -2.2395e-02]],

         [[-1.9501e-02,  4.3804e-02,  8.9692e-04],
          [-5.6095e-04, -2.6151e-03,  4.7352e-02],
          [ 7.1352e-03,  6.6845e-02,  1.8343e-02]],

         ...,

         [[-3.8586e-02,  4.9486e-02, -4.7427e-02],
          [-1.7523e-02, -1.8607e-02, -2.3462e-02],
          [-1.1652e-02, -6.7544e-03,  1.8110e-02]],

         [[-1.5105e-02, -2.3820e-02, -5.0971e-02],
          [ 6.5391e-02, -1.0391e-02,  1.0877e-03],
          [-1.1483e-02,  1.5578e-02,  8.9673e-03]],

         [[-9.8404e-03,  3.3367e-03,  5.2559e-03],
          [-6.6762e-03,  4.8977e-02, -8.7508e-03],
          [ 5.2239e-03, -7.7330e-03,  4.7629e-02]]],


        [[[-1.2726e-02,  4.3778e-03,  5.4798e-04],
          [ 1.7577e-02,  3.2218e-02, -1.4095e-02],
          [ 5.4335e-02, -3.6566e-02, -4.3806e-02]],

         [[ 1.0339e-02, -5.2808e-02, -1.1224e-03],
          [ 3.2759e-02,  1.0503e-02, -1.2759e-02],
          [-4.6919e-03,  1.8532e-02,  3.2762e-03]],

         [[ 2.0207e-02, -2.3424e-02,  5.9670e-03],
          [-1.5941e-02, -1.9688e-02, -3.0321e-02],
          [ 1.0419e-02,  5.0194e-02, -1.2968e-02]],

         ...,

         [[ 4.3672e-04, -4.7921e-02,  4.5266e-02],
          [ 4.0097e-02,  5.8866e-02, -1.3495e-02],
          [-2.9733e-02,  7.6835e-03,  2.0600e-04]],

         [[ 5.5614e-02,  1.6703e-04, -3.8038e-03],
          [-4.0943e-02,  2.2528e-02,  2.9078e-02],
          [-1.0509e-02,  9.6280e-04,  5.6172e-02]],

         [[ 9.8794e-03, -1.2497e-02,  1.3209e-02],
          [-3.1222e-02,  9.6996e-03,  1.3832e-02],
          [-5.4471e-02, -3.0821e-02,  6.9888e-03]]],


        ...,


        [[[ 1.4277e-02, -9.3300e-03,  1.4343e-02],
          [ 3.9653e-02, -5.8820e-02,  9.1802e-03],
          [-1.2939e-02,  2.4838e-03,  2.8019e-02]],

         [[-2.1654e-02,  4.5790e-02,  2.5189e-02],
          [-1.8606e-04,  5.0631e-02,  1.8056e-02],
          [-2.2039e-02, -6.7958e-02,  1.0171e-02]],

         [[-3.0488e-02,  4.3546e-02,  1.2638e-02],
          [-1.8338e-02,  4.6922e-02,  1.4246e-02],
          [ 2.9663e-02,  6.0357e-02, -1.9041e-02]],

         ...,

         [[ 2.5482e-02,  3.8065e-02, -1.1196e-02],
          [ 5.6828e-03, -3.1314e-02, -2.2063e-02],
          [ 3.7692e-04, -3.0068e-02, -2.0627e-02]],

         [[-1.7725e-03, -1.7880e-02,  2.7009e-02],
          [ 8.2328e-03, -2.3473e-02, -3.8422e-02],
          [ 5.8984e-02,  1.1251e-02, -3.4761e-02]],

         [[-2.9256e-02, -1.7618e-02,  2.7163e-02],
          [ 3.4746e-03,  4.4787e-02, -2.2252e-03],
          [ 1.4898e-02,  4.4437e-02,  4.3106e-02]]],


        [[[-9.2160e-03, -4.2137e-02, -1.4242e-02],
          [ 2.6086e-02,  2.2007e-02, -1.6421e-02],
          [ 2.5151e-02, -3.4215e-02, -9.5534e-03]],

         [[ 3.4207e-03,  1.4414e-03,  5.7809e-02],
          [ 8.6341e-03, -4.3922e-03, -2.1097e-02],
          [-3.6381e-03,  3.3249e-02,  4.2701e-02]],

         [[ 4.9087e-02, -5.8919e-02,  3.8189e-02],
          [-1.0983e-02, -6.9502e-03,  1.9241e-02],
          [-1.5597e-02, -3.9244e-02, -1.0090e-02]],

         ...,

         [[ 1.8592e-02,  2.2494e-03, -1.2371e-03],
          [-3.9529e-02, -1.0514e-02,  1.9537e-02],
          [ 2.9762e-02,  8.0175e-04, -5.1833e-02]],

         [[ 4.5937e-02, -5.4486e-04, -4.5836e-03],
          [ 7.2436e-03, -2.0665e-02, -1.3631e-02],
          [-6.5974e-03, -1.9701e-02, -3.4644e-02]],

         [[ 5.3112e-02, -8.0847e-03, -1.1124e-02],
          [ 1.8521e-03,  3.2791e-02, -2.6909e-02],
          [ 3.5575e-02, -2.0248e-02,  2.8322e-02]]],


        [[[-1.8124e-02, -1.8786e-02,  2.0782e-02],
          [ 4.9092e-02, -7.7136e-04, -1.8944e-02],
          [ 3.2795e-02,  2.9276e-02, -1.6763e-02]],

         [[-2.7176e-02,  3.9677e-02, -1.2484e-03],
          [-1.3277e-02,  3.6931e-02,  3.7107e-02],
          [ 1.9621e-02,  2.6875e-02,  2.0475e-02]],

         [[ 3.6596e-02, -3.6757e-02, -3.2811e-02],
          [ 6.2793e-03, -5.8534e-02,  9.1871e-03],
          [-1.6020e-03, -2.2864e-02, -2.2335e-02]],

         ...,

         [[ 5.0126e-03, -6.3836e-02,  1.9209e-02],
          [ 3.7965e-02,  1.3083e-02, -2.1302e-03],
          [-1.7406e-02,  7.1368e-03, -7.8921e-03]],

         [[-4.7961e-02,  2.3835e-02, -1.0354e-02],
          [ 3.8323e-04,  4.0095e-02,  2.4689e-02],
          [-7.0787e-03, -3.9147e-02,  3.8259e-02]],

         [[-3.0931e-03, -1.1574e-04, -5.8496e-02],
          [ 7.3343e-02, -2.2690e-02,  3.3479e-02],
          [ 9.1951e-03,  4.6084e-02, -1.2741e-03]]]], requires_grad=True)
BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[ 8.3269e-03, -3.5134e-02, -3.7431e-02],
          [-1.9291e-02, -1.0800e-02,  1.3189e-02],
          [-1.4368e-02,  5.3346e-02, -1.1685e-02]],

         [[ 2.0350e-04, -2.9089e-02,  2.6251e-02],
          [-1.6093e-02,  3.3946e-02, -4.1572e-02],
          [-8.9990e-03,  1.8773e-03, -2.2305e-02]],

         [[-4.2136e-03,  2.5281e-02, -1.2113e-02],
          [ 2.7211e-02,  9.3893e-04, -1.0278e-02],
          [-2.0774e-02,  2.5848e-02, -2.1693e-02]],

         ...,

         [[-2.1863e-02,  1.5476e-02, -5.5452e-02],
          [ 6.2040e-02, -3.9947e-02, -2.7489e-02],
          [ 4.4242e-02,  7.9122e-03,  2.7461e-02]],

         [[-4.2436e-02, -3.1717e-03, -5.3884e-02],
          [ 6.5039e-03,  1.4800e-02, -1.0735e-02],
          [ 1.0509e-02, -4.0901e-02,  1.2041e-03]],

         [[ 2.9845e-02,  1.7658e-02,  2.4837e-02],
          [ 6.9184e-02, -5.6260e-02, -4.0385e-02],
          [-4.8668e-02, -9.0358e-03,  2.6716e-02]]],


        [[[-4.4902e-04, -2.3155e-02, -4.0537e-02],
          [ 1.5609e-02, -1.0671e-02,  1.7177e-03],
          [-5.7033e-02,  9.7015e-03,  4.2397e-02]],

         [[ 3.9755e-02,  2.4124e-03, -2.3623e-02],
          [ 3.5924e-03, -2.2870e-02,  1.2192e-02],
          [ 1.5780e-02,  5.5205e-02,  2.5547e-02]],

         [[ 1.1826e-02, -4.4262e-03, -2.7821e-02],
          [-1.3898e-02, -2.0797e-02, -7.5916e-03],
          [-7.1790e-04,  9.7981e-03,  4.3273e-02]],

         ...,

         [[-6.2943e-02,  5.3863e-02,  7.5070e-03],
          [-2.9317e-03, -4.2199e-03, -4.1778e-02],
          [ 2.6880e-02, -1.4458e-02,  1.7739e-02]],

         [[-1.2544e-02, -6.8630e-03,  2.2418e-03],
          [-2.1214e-02,  1.9533e-02, -1.0829e-02],
          [-1.9002e-02, -2.5052e-02,  3.1811e-02]],

         [[-4.6831e-02, -2.8485e-02,  2.5411e-02],
          [ 6.1596e-02,  6.0390e-03,  2.2158e-02],
          [ 2.7091e-02,  2.1907e-02,  1.0816e-03]]],


        [[[-5.2126e-03,  2.5695e-02, -1.7361e-02],
          [-1.8220e-02,  4.7744e-03, -3.6488e-02],
          [ 7.1056e-02,  1.9620e-03, -1.1092e-02]],

         [[ 2.3867e-02,  5.6326e-03, -2.7709e-02],
          [ 4.3830e-03, -1.6393e-02,  1.4155e-02],
          [-3.6426e-02, -5.9953e-02,  8.1823e-02]],

         [[-2.2751e-02, -4.5557e-02, -5.0958e-02],
          [ 1.6041e-02, -2.4445e-04,  6.8521e-02],
          [-2.1568e-03, -5.1734e-03, -1.1095e-04]],

         ...,

         [[-9.7984e-04,  1.7355e-02,  4.4719e-03],
          [ 5.5690e-04, -7.0830e-03, -4.7628e-03],
          [-1.6623e-02,  3.8575e-02, -1.4157e-02]],

         [[-1.2412e-02,  1.1819e-02, -2.8495e-02],
          [ 2.4914e-02, -1.4742e-02, -9.6996e-03],
          [-1.5340e-02, -2.0823e-02, -3.7569e-02]],

         [[ 9.2585e-04,  4.7357e-03,  2.1563e-05],
          [ 4.7240e-02, -9.7799e-03, -1.1545e-02],
          [ 2.5235e-02, -3.6965e-02, -6.9785e-03]]],


        ...,


        [[[ 2.6424e-02, -5.0573e-02,  2.0633e-02],
          [-7.5980e-02, -6.6664e-03, -6.2103e-02],
          [ 2.6601e-02,  2.1429e-02, -2.3837e-02]],

         [[-1.7143e-02,  1.9364e-02, -1.3287e-02],
          [-1.2578e-02, -2.0315e-02, -5.2931e-02],
          [-2.9333e-02, -2.5455e-02,  3.9552e-02]],

         [[ 4.8195e-02, -1.9011e-02,  9.5458e-03],
          [-5.4744e-02,  2.0205e-02, -3.1475e-02],
          [ 2.0008e-02,  4.8962e-02, -3.4680e-02]],

         ...,

         [[-2.1596e-02, -8.1613e-03, -7.5599e-03],
          [ 2.5980e-02,  5.3365e-03,  3.9890e-02],
          [-4.0362e-02, -1.6284e-02, -6.6825e-03]],

         [[ 1.3202e-02,  1.2090e-02,  2.6268e-02],
          [ 6.4679e-02, -2.4807e-02,  5.1860e-02],
          [ 5.1778e-02,  2.3230e-02, -4.2808e-02]],

         [[ 1.8713e-02,  1.8343e-02, -7.6076e-02],
          [-6.7190e-03, -6.8959e-03,  6.5672e-02],
          [-4.1159e-02, -3.7828e-02, -2.5631e-02]]],


        [[[ 2.3462e-02,  3.8074e-03, -2.7622e-03],
          [-1.9605e-02,  2.4338e-02,  4.9705e-02],
          [ 4.2361e-02,  2.0832e-02, -1.7791e-02]],

         [[ 1.9600e-02, -2.2643e-02, -3.2654e-03],
          [ 1.6689e-02,  2.0883e-02, -1.5683e-02],
          [ 4.5449e-02, -6.7117e-03, -5.2389e-02]],

         [[ 3.1751e-02,  1.0429e-03,  3.6379e-02],
          [ 6.5176e-03, -2.6485e-02, -1.2818e-02],
          [-2.6537e-02, -8.9042e-03, -1.1701e-02]],

         ...,

         [[ 2.2137e-02,  7.4602e-03, -1.8221e-02],
          [-3.6412e-02,  3.6506e-02, -5.6468e-02],
          [-2.7687e-02, -1.2641e-02,  1.0870e-02]],

         [[-4.0169e-02, -3.2022e-02,  2.2778e-02],
          [-1.3549e-02,  1.7572e-02,  1.1616e-03],
          [-3.6917e-02,  1.2861e-02,  2.0891e-02]],

         [[ 2.7742e-02, -9.1432e-03, -2.6956e-02],
          [ 6.3085e-03, -2.7139e-02, -1.5449e-02],
          [ 4.8141e-02,  3.2462e-02, -1.7525e-02]]],


        [[[ 4.8090e-02,  5.9466e-02,  6.7652e-02],
          [ 1.0873e-02, -1.9764e-02, -2.3716e-02],
          [-9.2178e-03, -1.0182e-02,  1.6743e-02]],

         [[-3.9040e-02, -1.5198e-02,  3.0853e-02],
          [ 1.3629e-02, -3.3065e-02,  9.6480e-04],
          [ 1.1494e-02, -8.9411e-03,  2.8738e-02]],

         [[-2.0850e-02,  1.0569e-02, -3.9543e-03],
          [-4.2467e-02,  8.1879e-04, -8.9524e-03],
          [ 2.6538e-02, -2.9316e-02, -5.5309e-02]],

         ...,

         [[-9.4939e-04, -3.7367e-03,  2.1171e-02],
          [ 3.7869e-02,  1.3172e-02,  1.3432e-02],
          [-3.7903e-02,  5.9218e-03,  4.9522e-02]],

         [[-1.0400e-02, -2.6745e-02,  4.6472e-02],
          [-1.0347e-02,  1.8295e-03, -7.3114e-02],
          [-3.0344e-02, -1.8484e-02,  1.3294e-02]],

         [[ 2.6583e-02, -1.5166e-02, -2.1355e-02],
          [-2.9414e-02,  1.3314e-02,  1.8319e-02],
          [ 4.8216e-02, -5.0880e-03,  2.1567e-02]]]], requires_grad=True)
BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
Sequential(
  (0): ConvBlock(
    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (1): ConvBlock(
    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
)
Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[-0.0052,  0.0178,  0.0032],
          [ 0.0236, -0.0359,  0.0167],
          [ 0.0049,  0.0028,  0.0062]],

         [[-0.0248, -0.0081, -0.0270],
          [-0.0443,  0.0073,  0.0019],
          [ 0.0108,  0.0187, -0.0020]],

         [[-0.0185, -0.0152,  0.0031],
          [ 0.0237,  0.0188,  0.0222],
          [-0.0089, -0.0137, -0.0088]],

         ...,

         [[-0.0034,  0.0051,  0.0230],
          [ 0.0552, -0.0334,  0.0148],
          [-0.0116, -0.0118,  0.0033]],

         [[ 0.0366,  0.0094,  0.0042],
          [-0.0305, -0.0083, -0.0193],
          [-0.0395, -0.0579, -0.0189]],

         [[-0.0195, -0.0366, -0.0187],
          [-0.0296, -0.0090,  0.0120],
          [ 0.0142, -0.0259,  0.0084]]],


        [[[ 0.0170,  0.0202,  0.0029],
          [-0.0407, -0.0017, -0.0384],
          [ 0.0155, -0.0132,  0.0011]],

         [[-0.0361, -0.0061, -0.0568],
          [-0.0184,  0.0117,  0.0073],
          [-0.0230,  0.0211,  0.0116]],

         [[-0.0183,  0.0187,  0.0104],
          [-0.0357,  0.0048,  0.0117],
          [ 0.0159, -0.0052,  0.0215]],

         ...,

         [[-0.0266, -0.0533, -0.0204],
          [-0.0150, -0.0210, -0.0134],
          [-0.0143, -0.0308, -0.0206]],

         [[ 0.0155,  0.0040,  0.0024],
          [-0.0066, -0.0086,  0.0163],
          [ 0.0060, -0.0253, -0.0025]],

         [[ 0.0218, -0.0065,  0.0367],
          [ 0.0481,  0.0095, -0.0360],
          [-0.0198, -0.0360, -0.0412]]],


        [[[-0.0054,  0.0471,  0.0051],
          [-0.0391,  0.0331, -0.0046],
          [ 0.0100, -0.0463, -0.0411]],

         [[-0.0257, -0.0301, -0.0358],
          [-0.0133, -0.0074, -0.0027],
          [ 0.0294,  0.0037, -0.0177]],

         [[ 0.0183,  0.0008, -0.0200],
          [-0.0199, -0.0109,  0.0020],
          [ 0.0176,  0.0137, -0.0255]],

         ...,

         [[-0.0190, -0.0441, -0.0059],
          [ 0.0158, -0.0161,  0.0336],
          [ 0.0535, -0.0222, -0.0161]],

         [[-0.0176,  0.0105,  0.0221],
          [ 0.0371, -0.0063,  0.0006],
          [-0.0301, -0.0190, -0.0276]],

         [[ 0.0242, -0.0206, -0.0139],
          [ 0.0186, -0.0072,  0.0230],
          [-0.0119, -0.0389,  0.0227]]],


        ...,


        [[[ 0.0014, -0.0047, -0.0157],
          [-0.0264, -0.0091, -0.0281],
          [ 0.0200,  0.0044,  0.0119]],

         [[-0.0165,  0.0359,  0.0414],
          [-0.0306,  0.0145,  0.0318],
          [ 0.0228, -0.0288,  0.0057]],

         [[-0.0226,  0.0033, -0.0018],
          [ 0.0219,  0.0090,  0.0131],
          [ 0.0321,  0.0064, -0.0282]],

         ...,

         [[ 0.0346, -0.0610,  0.0011],
          [ 0.0231,  0.0238,  0.0401],
          [-0.0101, -0.0137,  0.0137]],

         [[-0.0090,  0.0567,  0.0091],
          [-0.0077,  0.0027,  0.0077],
          [-0.0113, -0.0736, -0.0040]],

         [[ 0.0501,  0.0113,  0.0230],
          [ 0.0307, -0.0045,  0.0277],
          [-0.0372,  0.0236,  0.0097]]],


        [[[ 0.0257,  0.0204,  0.0240],
          [-0.0445, -0.0297,  0.0171],
          [-0.0097, -0.0201, -0.0468]],

         [[-0.0495,  0.0389,  0.0585],
          [ 0.0070,  0.0309, -0.0189],
          [-0.0352, -0.0724, -0.0129]],

         [[ 0.0325,  0.0024, -0.0050],
          [-0.0196, -0.0392,  0.0152],
          [-0.0038, -0.0013,  0.0174]],

         ...,

         [[ 0.0217,  0.0087, -0.0101],
          [ 0.0096,  0.0400, -0.0018],
          [ 0.0494, -0.0236, -0.0261]],

         [[-0.0026,  0.0009, -0.0576],
          [ 0.0047,  0.0258,  0.0043],
          [ 0.0293, -0.0039, -0.0046]],

         [[ 0.0435,  0.0147,  0.0060],
          [-0.0192, -0.0076, -0.0376],
          [ 0.0316,  0.0220,  0.0255]]],


        [[[ 0.0244, -0.0268,  0.0388],
          [ 0.0270, -0.0079, -0.0185],
          [ 0.0003,  0.0199, -0.0304]],

         [[ 0.0035, -0.0332, -0.0166],
          [-0.0093,  0.0122,  0.0292],
          [ 0.0121,  0.0099,  0.0368]],

         [[-0.0126, -0.0048,  0.0003],
          [-0.0240,  0.0044, -0.0217],
          [ 0.0032, -0.0302, -0.0445]],

         ...,

         [[ 0.0094, -0.0044,  0.0165],
          [-0.0044, -0.0233, -0.0019],
          [ 0.0066, -0.0263, -0.0438]],

         [[-0.0239,  0.0363, -0.0126],
          [ 0.0175,  0.0013, -0.0271],
          [-0.0043,  0.0319, -0.0235]],

         [[-0.0009,  0.0010, -0.0208],
          [-0.0256, -0.0127, -0.0255],
          [ 0.0028, -0.0001, -0.0351]]]], requires_grad=True)
BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Sequential(
  (0): ConvBlock(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[ 2.3877e-03, -2.2504e-02, -9.0408e-03],
          [-8.8291e-03,  2.7592e-03,  1.1705e-02],
          [ 1.3818e-02, -1.4557e-02, -1.1757e-02]],

         [[-9.7309e-03, -4.2020e-05,  1.2505e-02],
          [-2.0401e-02, -7.5455e-03, -1.3592e-02],
          [ 6.0382e-02,  1.9493e-03,  1.2554e-02]],

         [[-5.8455e-03, -2.1588e-02, -1.4735e-02],
          [ 5.8157e-04, -1.1512e-02,  4.9751e-03],
          [ 1.5267e-02,  1.3043e-02,  1.2065e-02]],

         ...,

         [[ 1.4715e-02,  7.8907e-03,  3.2011e-03],
          [ 1.0398e-02,  1.8705e-02, -1.9132e-02],
          [-1.8085e-03, -7.0104e-03,  2.7404e-03]],

         [[-1.8098e-02,  8.1532e-03, -2.6093e-02],
          [-4.3559e-02, -8.7253e-03,  1.3431e-02],
          [-2.1395e-02,  2.2888e-03, -1.5473e-03]],

         [[-4.9987e-03,  2.3574e-02, -3.6041e-02],
          [-1.3407e-02, -2.8742e-02,  1.2440e-02],
          [ 2.6295e-02,  1.2597e-03,  1.6855e-02]]],


        [[[-2.0301e-02, -3.2705e-03, -1.5827e-04],
          [-1.3916e-02, -1.3412e-02,  1.9743e-02],
          [ 8.6589e-03, -1.1233e-02,  1.1692e-02]],

         [[-1.3579e-02, -1.6052e-03,  2.0046e-03],
          [ 1.5841e-02,  1.4476e-02, -1.4729e-02],
          [-1.4345e-02, -4.7270e-03,  8.2034e-03]],

         [[-3.6788e-03,  7.1767e-03,  4.7153e-03],
          [ 7.8085e-03,  8.7545e-03,  1.4452e-02],
          [-4.4327e-03, -6.6658e-03,  2.8402e-02]],

         ...,

         [[-2.0229e-02, -1.5967e-02, -3.3059e-04],
          [ 3.0094e-03,  2.0204e-02, -2.4975e-03],
          [-4.8666e-03, -1.7448e-02,  8.7291e-03]],

         [[ 5.4320e-03, -8.8069e-05, -3.0563e-02],
          [ 1.0895e-02,  1.0089e-02,  1.5582e-02],
          [-2.3683e-02, -2.4135e-03,  8.5034e-03]],

         [[-7.5235e-03,  1.5553e-02, -1.8300e-02],
          [ 1.6004e-02,  2.6781e-02,  2.0462e-03],
          [ 1.7052e-02, -1.0659e-02,  1.4487e-02]]],


        [[[ 1.9380e-02,  2.9596e-03,  5.2485e-03],
          [-1.2185e-02, -8.2127e-03, -2.6408e-03],
          [ 1.9302e-02,  3.5608e-02, -1.6101e-03]],

         [[ 2.3535e-02, -2.0635e-02,  1.5320e-02],
          [ 1.6612e-03,  1.2625e-02, -1.4057e-02],
          [-8.3912e-03,  1.3753e-02,  2.6805e-02]],

         [[-1.3057e-02,  1.3753e-02, -2.9528e-03],
          [-7.5392e-03,  9.2443e-03, -1.5506e-02],
          [-1.8602e-02,  7.5290e-03,  1.5761e-02]],

         ...,

         [[-3.1724e-02, -1.0082e-02, -8.4305e-04],
          [ 2.3144e-03, -1.3151e-02,  2.3913e-02],
          [ 3.3073e-03,  1.1308e-04,  1.9675e-02]],

         [[ 9.3260e-05, -2.2914e-02, -2.8034e-03],
          [ 3.4890e-02,  4.6966e-02,  2.1810e-03],
          [ 2.6084e-03, -2.6700e-03, -1.8121e-02]],

         [[ 8.8457e-03,  4.3771e-03,  1.8322e-02],
          [-1.9239e-02, -2.8678e-02, -7.0220e-03],
          [ 1.0284e-03,  8.1638e-03, -1.4599e-02]]],


        ...,


        [[[-1.8057e-02, -1.9259e-02,  2.5337e-02],
          [ 9.5990e-03, -9.8370e-03,  7.4405e-03],
          [-1.1346e-02, -8.3997e-03,  1.2328e-02]],

         [[-1.5529e-02,  3.8244e-02,  2.0420e-02],
          [-1.0959e-02, -2.2683e-02,  2.2407e-02],
          [-9.2804e-04, -1.6094e-02, -1.5514e-02]],

         [[-1.2691e-02, -7.8120e-03,  2.1520e-03],
          [ 3.2535e-04,  4.1145e-02,  1.9968e-02],
          [-1.0556e-02, -2.0731e-02,  8.2605e-03]],

         ...,

         [[-3.0402e-03, -3.7470e-03, -1.0099e-02],
          [ 4.0814e-04,  2.1029e-03, -8.0301e-03],
          [ 1.6734e-02, -5.1081e-03, -3.4944e-02]],

         [[ 2.2175e-02,  2.0512e-02,  1.2317e-02],
          [-8.9759e-03,  1.4954e-02,  2.5972e-02],
          [ 1.3397e-02, -1.9313e-02,  3.1861e-02]],

         [[-1.9682e-02, -9.3407e-03, -1.5910e-04],
          [ 2.4991e-02, -6.1607e-03, -1.5600e-02],
          [ 1.3233e-02, -1.1841e-02,  1.2100e-02]]],


        [[[ 1.0659e-02, -2.4465e-02, -3.0541e-02],
          [ 1.6480e-02,  2.5403e-02, -2.9544e-02],
          [ 1.4448e-02, -2.8261e-02, -1.9977e-02]],

         [[ 3.3927e-02,  7.4609e-03, -1.1648e-02],
          [ 1.4947e-02,  2.8147e-02,  5.2625e-03],
          [ 2.1881e-02,  3.0643e-02,  1.9711e-03]],

         [[ 1.8986e-02,  1.1552e-02,  3.0288e-02],
          [ 9.4335e-03,  8.6978e-03, -2.0283e-02],
          [ 1.0434e-04, -1.8617e-02,  1.5473e-02]],

         ...,

         [[ 2.7342e-02,  3.4053e-03, -3.8810e-03],
          [ 7.1972e-03, -2.1197e-02,  8.0238e-03],
          [-1.9354e-02, -6.1523e-03,  1.9251e-02]],

         [[-8.1954e-03,  2.3000e-02, -3.8244e-02],
          [ 2.4186e-02, -1.8230e-02, -3.9593e-03],
          [ 9.7375e-03,  3.2066e-03,  1.1074e-03]],

         [[-2.8009e-02,  1.6244e-02, -1.4720e-02],
          [ 1.9459e-02,  6.7344e-03,  2.8197e-02],
          [-1.5696e-02, -4.3430e-03, -1.8845e-02]]],


        [[[ 5.5423e-03,  1.1190e-02, -1.0857e-03],
          [ 6.3235e-03,  2.1418e-02,  5.9766e-03],
          [-1.4476e-02, -3.2146e-04, -3.5090e-03]],

         [[-1.5558e-02,  3.2757e-02,  1.1774e-02],
          [-2.4358e-02,  1.6706e-02,  3.2891e-03],
          [ 1.6802e-02,  2.0920e-02, -6.9668e-03]],

         [[-6.0398e-03,  2.5067e-02,  8.4780e-03],
          [-1.0808e-02, -2.5557e-03,  1.0824e-03],
          [-3.5219e-02,  2.0276e-02,  9.0999e-03]],

         ...,

         [[ 3.0296e-02,  2.0350e-02,  4.8437e-03],
          [-1.8662e-02,  1.5426e-03,  1.0521e-02],
          [-1.5375e-02,  3.1253e-02, -8.2512e-05]],

         [[ 5.6543e-03,  1.5936e-03,  3.9841e-02],
          [-1.6496e-02, -2.5299e-02, -2.7512e-02],
          [-1.8345e-02, -1.1314e-02,  2.7249e-02]],

         [[ 1.8063e-02, -3.2015e-02,  2.3985e-02],
          [ 2.3503e-02, -7.3668e-03, -2.0985e-02],
          [ 2.5380e-02, -1.8972e-02, -5.8892e-04]]]], requires_grad=True)
BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
Sequential(
  (0): ConvBlock(
    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[ 0.0073,  0.0075,  0.0204],
          [-0.0059, -0.0127, -0.0107],
          [-0.0109, -0.0162, -0.0192]],

         [[-0.0010, -0.0030,  0.0059],
          [-0.0041,  0.0157, -0.0029],
          [ 0.0255,  0.0021,  0.0414]],

         [[-0.0027, -0.0003, -0.0169],
          [ 0.0115, -0.0009, -0.0091],
          [ 0.0027,  0.0219,  0.0091]],

         ...,

         [[ 0.0320,  0.0123, -0.0214],
          [ 0.0115, -0.0075,  0.0043],
          [-0.0104, -0.0317,  0.0046]],

         [[-0.0029,  0.0036,  0.0093],
          [-0.0068,  0.0164,  0.0191],
          [-0.0199, -0.0145,  0.0112]],

         [[ 0.0126,  0.0047, -0.0003],
          [ 0.0132, -0.0054, -0.0073],
          [-0.0003, -0.0244,  0.0273]]],


        [[[ 0.0254,  0.0041, -0.0063],
          [ 0.0232,  0.0045,  0.0086],
          [ 0.0002, -0.0158,  0.0113]],

         [[-0.0119, -0.0185,  0.0039],
          [-0.0466, -0.0072,  0.0023],
          [ 0.0015, -0.0076, -0.0366]],

         [[-0.0025,  0.0058, -0.0058],
          [-0.0008,  0.0019,  0.0182],
          [ 0.0029, -0.0321, -0.0184]],

         ...,

         [[-0.0308,  0.0147, -0.0275],
          [ 0.0045,  0.0294, -0.0085],
          [ 0.0029,  0.0180,  0.0120]],

         [[-0.0081, -0.0113,  0.0071],
          [-0.0006, -0.0012, -0.0281],
          [-0.0210, -0.0059,  0.0077]],

         [[ 0.0182,  0.0121,  0.0147],
          [-0.0297, -0.0142, -0.0017],
          [-0.0193,  0.0047, -0.0354]]],


        [[[ 0.0088,  0.0180,  0.0086],
          [ 0.0149,  0.0080,  0.0027],
          [ 0.0095,  0.0019, -0.0286]],

         [[ 0.0009,  0.0132,  0.0001],
          [ 0.0113, -0.0019,  0.0013],
          [ 0.0005,  0.0026,  0.0032]],

         [[-0.0007,  0.0081,  0.0016],
          [ 0.0165,  0.0136, -0.0035],
          [-0.0092, -0.0117,  0.0155]],

         ...,

         [[-0.0174, -0.0221,  0.0037],
          [ 0.0360, -0.0002,  0.0046],
          [-0.0042, -0.0084,  0.0022]],

         [[-0.0027, -0.0101, -0.0084],
          [-0.0186,  0.0010, -0.0231],
          [ 0.0117, -0.0231,  0.0173]],

         [[ 0.0289,  0.0254, -0.0179],
          [-0.0304, -0.0178, -0.0320],
          [ 0.0026, -0.0085, -0.0078]]],


        ...,


        [[[-0.0083,  0.0054, -0.0094],
          [ 0.0005, -0.0153,  0.0007],
          [ 0.0160, -0.0104,  0.0142]],

         [[-0.0078, -0.0027,  0.0058],
          [ 0.0124,  0.0201,  0.0010],
          [ 0.0028,  0.0034, -0.0159]],

         [[ 0.0041, -0.0224,  0.0093],
          [-0.0226, -0.0082,  0.0227],
          [ 0.0105,  0.0266,  0.0076]],

         ...,

         [[ 0.0087,  0.0151, -0.0055],
          [-0.0224,  0.0042, -0.0042],
          [ 0.0162, -0.0153, -0.0068]],

         [[-0.0282, -0.0024, -0.0089],
          [-0.0067, -0.0049,  0.0190],
          [ 0.0053, -0.0217,  0.0014]],

         [[ 0.0114,  0.0138,  0.0254],
          [-0.0056, -0.0127,  0.0085],
          [-0.0152, -0.0001,  0.0053]]],


        [[[ 0.0085,  0.0047, -0.0055],
          [-0.0103, -0.0149, -0.0042],
          [ 0.0228, -0.0179,  0.0042]],

         [[-0.0022,  0.0202,  0.0356],
          [-0.0113, -0.0024,  0.0282],
          [-0.0060,  0.0332,  0.0171]],

         [[-0.0095,  0.0009,  0.0126],
          [ 0.0110,  0.0074, -0.0131],
          [-0.0138,  0.0063,  0.0114]],

         ...,

         [[-0.0024,  0.0036, -0.0060],
          [-0.0052, -0.0045, -0.0016],
          [ 0.0310,  0.0049,  0.0250]],

         [[-0.0145,  0.0047, -0.0024],
          [ 0.0188,  0.0236, -0.0136],
          [-0.0075, -0.0061,  0.0078]],

         [[-0.0138, -0.0057,  0.0369],
          [ 0.0018, -0.0345,  0.0128],
          [ 0.0062, -0.0299, -0.0246]]],


        [[[-0.0031, -0.0245, -0.0204],
          [-0.0036,  0.0142,  0.0025],
          [-0.0167, -0.0057, -0.0081]],

         [[ 0.0100, -0.0079, -0.0309],
          [ 0.0074, -0.0018,  0.0026],
          [ 0.0241, -0.0067, -0.0018]],

         [[ 0.0127,  0.0106, -0.0048],
          [ 0.0069,  0.0083,  0.0007],
          [-0.0116,  0.0184, -0.0038]],

         ...,

         [[ 0.0006,  0.0109,  0.0173],
          [-0.0118, -0.0158,  0.0019],
          [ 0.0173, -0.0049,  0.0239]],

         [[-0.0124,  0.0052,  0.0156],
          [-0.0038,  0.0219,  0.0022],
          [ 0.0178,  0.0176,  0.0059]],

         [[ 0.0069, -0.0008,  0.0138],
          [-0.0378,  0.0046,  0.0024],
          [-0.0005, -0.0138,  0.0015]]]], requires_grad=True)
BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Parameter containing:
tensor([[[[ 1.9092e-02, -8.1798e-03, -1.2306e-02],
          [ 1.2832e-02,  5.1709e-04,  9.8070e-03],
          [-2.4207e-02, -4.1758e-03,  5.1461e-03]],

         [[-4.7302e-03,  6.2960e-03, -2.6779e-02],
          [ 8.4948e-04,  6.7131e-03, -3.2378e-03],
          [-2.5624e-02,  7.8542e-03, -1.9024e-02]],

         [[ 4.3330e-03, -3.9571e-03,  3.9324e-03],
          [-3.2257e-02,  7.3814e-03,  2.9000e-03],
          [-2.7549e-02, -1.0503e-02, -3.8788e-02]],

         ...,

         [[ 7.9243e-03,  1.8858e-02,  1.8622e-02],
          [-1.0575e-02, -1.3974e-02, -2.2089e-02],
          [-3.5634e-03, -1.4564e-02,  1.9595e-02]],

         [[ 4.3999e-03,  2.7145e-03, -7.9861e-03],
          [ 2.0088e-02,  3.9074e-03, -2.3871e-03],
          [ 9.9338e-03, -4.7839e-03, -1.3605e-02]],

         [[-1.4011e-03,  1.1645e-02,  1.3448e-02],
          [ 1.7055e-02,  7.8569e-04,  3.8224e-03],
          [-4.0883e-03,  8.7291e-03,  2.9765e-03]]],


        [[[-5.0869e-03,  3.6545e-03, -1.4380e-02],
          [-9.0741e-03, -1.1822e-03,  4.3006e-03],
          [-1.7304e-02,  6.9266e-03, -9.6276e-03]],

         [[ 2.3928e-02, -1.2443e-03,  1.3376e-02],
          [-1.0837e-04,  8.1362e-04,  3.6090e-02],
          [ 3.2838e-03, -1.8225e-02,  4.0303e-03]],

         [[ 2.2247e-02,  2.1335e-02,  1.5181e-02],
          [ 9.3160e-04,  1.2866e-02,  2.1252e-02],
          [-6.9047e-03, -1.3289e-02, -2.1693e-03]],

         ...,

         [[-2.6992e-03, -9.6855e-03, -7.7679e-03],
          [ 3.1431e-02, -1.7025e-02,  9.3197e-03],
          [ 1.2502e-02, -1.0897e-02,  1.4419e-02]],

         [[ 4.8696e-03,  1.3526e-02, -9.3105e-03],
          [ 8.4421e-03,  4.2753e-04,  2.3467e-02],
          [ 5.2153e-03, -6.1143e-03,  1.2476e-02]],

         [[ 1.4772e-03,  1.4760e-02,  1.4236e-02],
          [ 1.4022e-02, -2.6095e-02,  1.7789e-02],
          [-6.5993e-04,  1.2720e-02, -6.3669e-03]]],


        [[[-2.5597e-02, -1.9851e-02,  3.9983e-05],
          [ 1.6024e-02, -1.7359e-02,  9.8042e-04],
          [-3.7190e-03,  2.4982e-02, -1.3905e-02]],

         [[-4.5695e-03,  3.9396e-02,  1.3269e-02],
          [-5.2329e-03,  2.0425e-02, -1.9588e-02],
          [-5.2037e-03,  1.3064e-02,  7.5370e-03]],

         [[ 1.0947e-02,  1.5128e-02, -3.1276e-03],
          [ 3.4747e-03, -1.4069e-02, -1.3045e-02],
          [ 1.1937e-02,  4.7920e-03, -1.0485e-02]],

         ...,

         [[-1.1636e-03,  3.2605e-03, -5.1802e-03],
          [-1.7575e-02, -1.6535e-02,  1.2731e-02],
          [ 2.2771e-02, -2.0286e-02, -7.0829e-03]],

         [[ 1.5321e-02,  2.7318e-02, -5.7185e-03],
          [-1.4468e-02, -4.1056e-03, -8.2113e-03],
          [ 1.8535e-03, -1.9473e-02,  1.7226e-03]],

         [[-1.8918e-02, -1.7822e-02, -6.4719e-03],
          [ 3.9992e-03,  3.0953e-03,  1.0150e-03],
          [ 1.4452e-02,  1.2226e-02, -2.7234e-02]]],


        ...,


        [[[ 2.3123e-02, -3.1439e-02,  1.3370e-02],
          [ 4.7574e-03, -3.0119e-03,  1.7712e-02],
          [-2.1383e-03,  3.5619e-03,  1.6442e-02]],

         [[ 1.3959e-02,  5.8612e-03,  2.4390e-02],
          [ 3.1232e-02, -1.8421e-02, -6.0806e-03],
          [ 6.6670e-03, -8.4231e-03, -6.9868e-03]],

         [[-1.1141e-02,  1.3429e-02,  6.0238e-03],
          [ 1.5690e-02, -2.4771e-02,  1.3330e-02],
          [-1.2722e-02, -1.5981e-03,  1.5482e-02]],

         ...,

         [[ 6.7085e-03, -1.2144e-02, -2.5833e-02],
          [ 6.8928e-03, -1.4762e-03,  3.7950e-03],
          [-1.0286e-03,  1.0783e-02, -2.1464e-02]],

         [[-5.8625e-03,  1.3838e-02, -4.5982e-03],
          [ 9.5476e-03,  1.1068e-02,  1.3129e-02],
          [ 1.9842e-02, -2.8314e-02, -1.1635e-02]],

         [[-1.6590e-02,  1.0157e-02, -1.4222e-02],
          [-1.1503e-02, -1.0323e-02,  2.7690e-03],
          [ 2.1937e-02, -2.9248e-03,  1.0302e-02]]],


        [[[ 2.2264e-02,  2.8019e-04, -2.4296e-03],
          [-1.5505e-02,  5.4271e-03,  2.3108e-02],
          [ 1.4047e-02, -2.9157e-02,  1.7616e-02]],

         [[ 3.0275e-03,  2.0346e-02,  2.6886e-03],
          [ 2.7675e-02,  5.7657e-03,  8.2421e-03],
          [ 2.2079e-02,  1.7483e-02, -3.9870e-03]],

         [[-2.6661e-02, -2.4382e-02, -4.7090e-04],
          [-4.8808e-02, -1.2193e-03,  2.0806e-03],
          [ 2.6030e-02,  1.2518e-02, -1.8600e-02]],

         ...,

         [[-2.1578e-03, -7.4790e-03, -3.9170e-02],
          [-2.8261e-02, -1.3441e-02,  6.0494e-03],
          [ 2.1124e-02, -9.7742e-03, -3.1997e-03]],

         [[ 1.4248e-02, -3.7971e-03, -1.6122e-02],
          [-2.1368e-02,  1.6668e-02,  6.1431e-03],
          [-5.3905e-03,  1.6269e-02, -6.5061e-03]],

         [[-2.0037e-02,  3.2882e-02, -4.7926e-05],
          [ 1.9820e-02,  4.0256e-03,  3.2052e-03],
          [ 1.4578e-02, -7.9172e-03, -2.2576e-02]]],


        [[[-7.4614e-03,  1.0248e-02,  8.9034e-03],
          [-8.4330e-03, -2.3662e-02, -4.0133e-02],
          [ 3.9460e-03, -1.2918e-02,  3.6381e-02]],

         [[ 4.8070e-05,  1.5938e-02, -8.3469e-03],
          [-1.2462e-02,  1.1275e-02,  1.8223e-02],
          [ 6.3140e-03, -1.6509e-02,  3.9651e-03]],

         [[ 1.7390e-03, -8.8471e-03, -2.1150e-02],
          [-1.7153e-02, -2.1562e-02,  3.1126e-03],
          [-6.0190e-03, -5.3840e-03,  2.3907e-02]],

         ...,

         [[ 1.2297e-02,  2.0506e-02,  2.7411e-04],
          [-7.7758e-03, -3.2916e-02, -4.1983e-03],
          [-6.6927e-03,  1.1226e-02, -1.3390e-02]],

         [[-2.6079e-02, -2.3027e-02,  2.6702e-02],
          [ 9.6393e-03, -2.1605e-02, -5.3489e-03],
          [ 3.8967e-03, -1.3776e-02,  3.7228e-03]],

         [[-2.9266e-03, -6.1948e-03,  3.1683e-02],
          [ 1.9306e-02, -2.8036e-02,  1.2262e-02],
          [ 7.2765e-03,  2.1585e-03,  9.5776e-04]]]], requires_grad=True)
BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
ConvBlock(
  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activ): ReLU(inplace=True)
)
Sequential(
  (0): ConvBlock(
    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (1): ConvBlock(
    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
)
MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
Flatten(start_dim=1, end_dim=-1)
Dropout(p=0.2, inplace=False)
Linear(in_features=512, out_features=10, bias=True)
Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
  (1): Dropout(p=0.2, inplace=False)
  (2): Linear(in_features=512, out_features=10, bias=True)
)
MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="61">
<pre><code>MyResNet(
  (conv1): ConvBlock(
    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activ): ReLU(inplace=True)
  )
  (conv_maxpool_1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue1): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (conv_maxpool_2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_maxpool_3): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (residue2): Sequential(
    (0): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
    (1): ConvBlock(
      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activ): ReLU(inplace=True)
    )
  )
  (final_pooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (dense): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Dropout(p=0.2, inplace=False)
    (2): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="QisuIB2IfmpD" data-outputId="468c9b6a-b073-4660-de4c-44aa6f5e0f2d">
<div class="sourceCode" id="cb145"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-15"><a href="#cb145-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb145-16"><a href="#cb145-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb145-17"><a href="#cb145-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-18"><a href="#cb145-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb145-19"><a href="#cb145-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb145-20"><a href="#cb145-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb145-21"><a href="#cb145-21" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb145-22"><a href="#cb145-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb145-23"><a href="#cb145-23" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb145-24"><a href="#cb145-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.689300000667572, time = 23.276665210723877
Epoch[1]: accuracy = 0.802899956703186, time = 21.82638669013977
Epoch[2]: accuracy = 0.8448999524116516, time = 21.901376485824585
Epoch[3]: accuracy = 0.8857399821281433, time = 21.985389232635498
Epoch[4]: accuracy = 0.9204199910163879, time = 21.870365619659424
Epoch[5]: accuracy = 0.9374199509620667, time = 21.95445203781128
Epoch[6]: accuracy = 0.95169997215271, time = 21.931962490081787
Epoch[7]: accuracy = 0.9638399481773376, time = 21.870370864868164
Epoch[8]: accuracy = 0.958299994468689, time = 21.91478705406189
Epoch[9]: accuracy = 0.9669999480247498, time = 21.915133237838745
Epoch[10]: accuracy = 0.97461998462677, time = 21.925008296966553
Epoch[11]: accuracy = 0.9774999618530273, time = 21.926766395568848
Epoch[12]: accuracy = 0.9808599948883057, time = 21.909826278686523
Epoch[13]: accuracy = 0.9769799709320068, time = 21.90623378753662
Epoch[14]: accuracy = 0.9861799478530884, time = 21.915648698806763
Epoch[15]: accuracy = 0.9855199456214905, time = 21.920161485671997
Epoch[16]: accuracy = 0.9853399991989136, time = 21.886755228042603
Epoch[17]: accuracy = 0.9830999970436096, time = 21.90430474281311
Epoch[18]: accuracy = 0.9900799989700317, time = 21.885007858276367
Epoch[19]: accuracy = 0.9921599626541138, time = 21.884541988372803
train time = 613.7749378681183
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="mq34TuXefpEP" data-outputId="45da99e5-8208-4bfe-c750-17c81073dc94">
<div class="sourceCode" id="cb147"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8727999925613403
</code></pre>
</div>
</div>
<div class="cell code" id="7K_yLfG-oWfp">
<div class="sourceCode" id="cb149"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
