<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>d8e4170c57d64cdf94b6000e81a9fc09</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code" data-execution_count="1"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="cjmvWEK3plPp" data-outputId="29fd3de3-12e6-4517-b17b-1c0ee54f517d">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pip install torchviz</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting torchviz
  Downloading torchviz-0.0.2.tar.gz (4.9 kB)
  Preparing metadata (setup.py) ... ent already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu118)
Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.12.4)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (1.12)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1.2)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2023.6.0)
Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2.1.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-&gt;torchviz) (2.1.3)
Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch-&gt;torchviz) (1.3.0)
Building wheels for collected packages: torchviz
  Building wheel for torchviz (setup.py) ... e=torchviz-0.0.2-py3-none-any.whl size=4130 sha256=33373677fbf48fbac1d97ccdebfb4452ac62167b78beca849435950fe4b8b5ba
  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094
Successfully built torchviz
Installing collected packages: torchviz
Successfully installed torchviz-0.0.2
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="2" id="IaHduMBQoTk1">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.datasets</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plot</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchsummary <span class="im">import</span> summary</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="FdUeHhUHs0-t" data-outputId="189c706d-4853-4bf4-8315-701672b7bfa5">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div>
<div class="output execute_result" data-execution_count="3">
<pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="4" id="jcfSvHP3rL-X">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>CLASSES <span class="op">=</span> [<span class="st">&#39;plane&#39;</span>, <span class="st">&#39;car&#39;</span>, <span class="st">&#39;bird&#39;</span>, <span class="st">&#39;cat&#39;</span>, <span class="st">&#39;deer&#39;</span>, <span class="st">&#39;dog&#39;</span>, <span class="st">&#39;frog&#39;</span>, <span class="st">&#39;horse&#39;</span>, <span class="st">&#39;ship&#39;</span>, <span class="st">&#39;truck&#39;</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>N_CLASSES <span class="op">=</span> <span class="bu">len</span>(CLASSES)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_images(images, title):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    num_showed_imgs_x <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    num_showed_imgs_y <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plot.subplots(num_showed_imgs_y, num_showed_imgs_x, figsize <span class="op">=</span> figsize)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(title)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    plot.setp(plot.gcf().get_axes(), xticks <span class="op">=</span> [], yticks <span class="op">=</span> [])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    dataiter <span class="op">=</span> <span class="bu">iter</span>(images)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flat):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> images[i][<span class="dv">0</span>].numpy().transpose(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        ax.text(<span class="dv">1</span>, <span class="dv">1</span>, CLASSES[images[i][<span class="dv">1</span>]], bbox<span class="op">=</span><span class="bu">dict</span>(fill<span class="op">=</span><span class="va">False</span>, edgecolor<span class="op">=</span><span class="st">&#39;red&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>))</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="t8-bTcbHqDSm" data-outputId="b0b2b720-fb3f-40e3-9447-5fcc5da229d3">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_34886d2bd42949ffa43c5041e94c6532/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_34886d2bd42949ffa43c5041e94c6532/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="tdKE80FZqL3n" data-outputId="7efa0b26-d51b-4efc-a788-12b8edd5b09c">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_dataset[<span class="dv">6</span>][<span class="dv">0</span>].shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="6">
<pre><code>torch.Size([3, 32, 32])</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Kd72p0Tdu06A" data-outputId="9e57840c-9722-4f2d-c2c3-02c283ed30e4">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>test_dataset[<span class="dv">6</span>][<span class="dv">0</span>].shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="7">
<pre><code>torch.Size([3, 32, 32])</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="5" id="JhTIn62ICBE-">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvBlock(torch.nn.Sequential):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Standard convolution block with Batch normalization and activation.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                 ksize,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                 padding,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                 groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>                 use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>                 bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                 activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activate <span class="op">=</span> (activation <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_bn <span class="op">=</span> use_bn</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># with self.init_scope():</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>ksize,</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span>padding,</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span>groups)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_bn:</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bn <span class="op">=</span> torch.nn.BatchNorm2d(out_channels, eps<span class="op">=</span>bn_eps)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activate:</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activ <span class="op">=</span> activation</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv(x)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_bn:</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.bn(x)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activate:</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.activ(x)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="6" id="RWQWbY5dEJ79">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv1x1_block(in_channels,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                  out_channels,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                  stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>                  padding<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                  groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                  use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                  bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                  activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ConvBlock(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        ksize<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        stride<span class="op">=</span>stride,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        use_bn<span class="op">=</span>use_bn,</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        bn_eps<span class="op">=</span>bn_eps,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span>activation)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="7" id="-mC7q7obBcZp">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv3x3_block(in_channels,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>                  out_channels,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                  stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                  padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                  groups<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                  use_bn<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                  bn_eps<span class="op">=</span><span class="fl">1e-5</span>,</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                  activation<span class="op">=</span>torch.nn.ReLU()):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ConvBlock(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        ksize<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        stride<span class="op">=</span>stride,</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        use_bn<span class="op">=</span>use_bn,</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        bn_eps<span class="op">=</span>bn_eps,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span>activation)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="8" id="mvNPybk8AFuL">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEInitBlock(torch.nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    SENet specific initial block.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                 out_channels):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEInitBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        mid_channels <span class="op">=</span> out_channels <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>mid_channels)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv3x3_block(</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> torch.nn.MaxPool2d(</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SEInitBlock&quot;)</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="9" id="Ah_OYMAndkOz">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> round_channels(channels,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                   divisor<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Round weighted channel number (make divisible operation).</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    rounded_channels <span class="op">=</span> <span class="bu">max</span>(<span class="bu">int</span>(channels <span class="op">+</span> divisor <span class="op">/</span> <span class="fl">2.0</span>) <span class="op">//</span> divisor <span class="op">*</span> divisor, divisor)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">float</span>(rounded_channels) <span class="op">&lt;</span> <span class="fl">0.9</span> <span class="op">*</span> channels:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        rounded_channels <span class="op">+=</span> divisor</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rounded_channels</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="10" id="ZUhqp0ehcD-I">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SEBlock(torch.nn.Sequential):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Squeeze-and-Excitation block</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                 channels,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                 reduction<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                 mid_channels<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                 round_mid<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                 use_conv<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                 mid_activation<span class="op">=</span>torch.nn.ReLU(),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                 out_activation<span class="op">=</span>torch.nn.Sigmoid()):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SEBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_conv <span class="op">=</span> use_conv</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mid_channels <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            mid_channels <span class="op">=</span> channels <span class="op">//</span> reduction <span class="cf">if</span> <span class="kw">not</span> round_mid <span class="cf">else</span> round_channels(<span class="bu">float</span>(channels) <span class="op">/</span> reduction)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_conv:</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span>channels,</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span>mid_channels,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fc1 <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>                in_features<span class="op">=</span>channels,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>                out_features<span class="op">=</span>mid_channels)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activ <span class="op">=</span> mid_activation</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_conv:</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv2d(</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>                in_channels<span class="op">=</span>mid_channels,</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span>channels,</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fc2 <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>                in_features<span class="op">=</span>mid_channels,</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>                out_features<span class="op">=</span>channels)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> out_activation</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SEBlock&quot;)</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        avgpool <span class="op">=</span> torch.nn.AvgPool2d(kernel_size<span class="op">=</span>x.shape[<span class="dv">2</span>:])</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> avgpool(x)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.use_conv:</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> torch.reshape(w, shape<span class="op">=</span>(w.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.conv1(w) <span class="cf">if</span> <span class="va">self</span>.use_conv <span class="cf">else</span> <span class="va">self</span>.fc1(w)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.activ(w)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.conv2(w) <span class="cf">if</span> <span class="va">self</span>.use_conv <span class="cf">else</span> <span class="va">self</span>.fc2(w)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.sigmoid(w)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.use_conv:</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> torch.unsqueeze(torch.unsqueeze(w, axis<span class="op">=</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">*</span> w</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="11" id="YVTcnbE3aLta">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SENetBottleneck(torch.nn.Sequential):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    SENet bottleneck block for residual path in SENet unit.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SENetBottleneck, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        mid_channels <span class="op">=</span> out_channels <span class="op">//</span> <span class="dv">4</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> <span class="bu">int</span>(math.floor(mid_channels <span class="op">*</span> (bottleneck_width <span class="op">/</span> <span class="fl">64.0</span>)))</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        group_width <span class="op">=</span> number_of_groups <span class="op">*</span> D</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        group_width2 <span class="op">=</span> group_width <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1_block(</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>group_width2)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3_block(</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>group_width2,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>group_width,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            groups<span class="op">=</span>number_of_groups)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1_block(</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>group_width,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;SENetBottleneck&quot;)</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="12" id="BI07To1nEGjS">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SENetUnit(torch.nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                 in_channels,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                 out_channels,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                 stride,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>                 identity_conv3x3):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SENetUnit, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resize_identity <span class="op">=</span> (in_channels <span class="op">!=</span> out_channels) <span class="kw">or</span> (stride <span class="op">!=</span> <span class="dv">1</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.body <span class="op">=</span> SENetBottleneck(</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>stride,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>            bottleneck_width<span class="op">=</span>bottleneck_width)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.se <span class="op">=</span> SEBlock(channels<span class="op">=</span>out_channels)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.resize_identity:</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> identity_conv3x3:</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.identity_conv <span class="op">=</span> conv3x3_block(</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.identity_conv <span class="op">=</span> conv1x1_block(</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activ <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.resize_identity:</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.identity_conv(x)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> x</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.body(x)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.se(x)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> identity</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activ(x)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="13" id="SFGDgCXpv86s">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySENet(torch.nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                 channels,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                 init_block_channels,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                 number_of_groups,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                 bottleneck_width,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>                 in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>                 in_size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>),</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span>N_CLASSES):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MySENet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_size <span class="op">=</span> in_size</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_classes <span class="op">=</span> num_classes</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.add_module(<span class="st">&quot;init_block&quot;</span>, SEInitBlock(</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            in_channels<span class="op">=</span>in_channels,</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>init_block_channels))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        in_channels <span class="op">=</span> init_block_channels</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, channels_per_stage <span class="kw">in</span> <span class="bu">enumerate</span>(channels):</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            stage <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>            identity_conv3x3 <span class="op">=</span> (i <span class="op">!=</span> <span class="dv">0</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j, out_channels <span class="kw">in</span> <span class="bu">enumerate</span>(channels_per_stage):</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>                stride <span class="op">=</span> <span class="dv">2</span> <span class="cf">if</span> (j <span class="op">==</span> <span class="dv">0</span>) <span class="kw">and</span> (i <span class="op">!=</span> <span class="dv">0</span>) <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>                stage.add_module(<span class="st">&quot;unit</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(j <span class="op">+</span> <span class="dv">1</span>), SENetUnit(</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>                    in_channels<span class="op">=</span>in_channels,</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>                    out_channels<span class="op">=</span>out_channels,</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>                    stride<span class="op">=</span>stride,</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>                    number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>                    bottleneck_width<span class="op">=</span>bottleneck_width,</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>                    identity_conv3x3<span class="op">=</span>identity_conv3x3))</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>                in_channels <span class="op">=</span> out_channels</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.features.add_module(<span class="st">&quot;stage</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(i <span class="op">+</span> <span class="dv">1</span>), stage)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.add_module(<span class="st">&quot;final_pool&quot;</span>, torch.nn.AdaptiveAvgPool2d(<span class="dv">1</span>))</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> torch.nn.Sequential()</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output.add_module(<span class="st">&quot;dropout&quot;</span>, torch.nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>))</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output.add_module(<span class="st">&quot;fc&quot;</span>, torch.nn.Linear(</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>            in_features<span class="op">=</span>in_channels,</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>            out_features<span class="op">=</span>num_classes))</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_params()</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_params(<span class="va">self</span>):</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.named_modules():</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(module, torch.nn.Conv2d):</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.kaiming_uniform_(module.weight)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>                    torch.nn.init.constant_(module.bias, <span class="dv">0</span>)</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.features(x)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="14" id="gnr-LlF_hcxY">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_senet(blocks,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>              model_name<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>              <span class="op">**</span>kwargs):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> blocks <span class="op">==</span> <span class="dv">16</span>:</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> blocks <span class="op">==</span> <span class="dv">28</span>:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> blocks <span class="op">==</span> <span class="dv">40</span>:</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        number_of_groups <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&quot;Unsupported SENet with number of blocks: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(blocks))</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    bottleneck_width <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    init_block_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    channels_per_layers <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">1024</span>]</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    channels <span class="op">=</span> [[ci] <span class="op">*</span> li <span class="cf">for</span> (ci, li) <span class="kw">in</span> <span class="bu">zip</span>(channels_per_layers, layers)]</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    net <span class="op">=</span> MySENet(</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        channels<span class="op">=</span>channels,</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        init_block_channels<span class="op">=</span>init_block_channels,</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        number_of_groups<span class="op">=</span>number_of_groups,</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        bottleneck_width<span class="op">=</span>bottleneck_width,</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> net</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet16(<span class="op">**</span>kwargs):</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">16</span>, model_name<span class="op">=</span><span class="st">&quot;senet16&quot;</span>)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet28(<span class="op">**</span>kwargs):</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">28</span>, model_name<span class="op">=</span><span class="st">&quot;senet28&quot;</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> senet40(<span class="op">**</span>kwargs):</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_senet(blocks<span class="op">=</span><span class="dv">40</span>, model_name<span class="op">=</span><span class="st">&quot;senet40&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" id="pxVuxNbKicZo">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="15" id="f_eqcE00hcmJ">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_accuracy(data_loader, model):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    tp <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> data_loader:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.to(device)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>            n <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            tp <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tp <span class="op">/</span> n</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="n9pOQa9-pawM" data-outputId="506381d7-78ed-4d99-9ca4-d5c274cb4ab6">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">#      </span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot; </span><span class="ch">\&quot;</span><span class="st">CNN</span><span class="ch">\&quot;</span><span class="st">:&quot;</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>summary(cnn_model.to(device), input_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot; :&quot;</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>display(make_dot(cnn_model(torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>).to(device)), params<span class="op">=</span><span class="bu">dict</span>(cnn_model.named_parameters())))</span></code></pre></div>
<div class="output stream stdout">
<pre><code> &quot;CNN&quot;:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 16, 16]           1,792
       BatchNorm2d-2           [-1, 64, 16, 16]             128
              ReLU-3           [-1, 64, 16, 16]               0
              ReLU-4           [-1, 64, 16, 16]               0
              ReLU-5           [-1, 64, 16, 16]               0
              ReLU-6           [-1, 64, 16, 16]               0
              ReLU-7           [-1, 64, 16, 16]               0
              ReLU-8           [-1, 64, 16, 16]               0
            Conv2d-9           [-1, 64, 16, 16]          36,928
      BatchNorm2d-10           [-1, 64, 16, 16]             128
             ReLU-11           [-1, 64, 16, 16]               0
             ReLU-12           [-1, 64, 16, 16]               0
             ReLU-13           [-1, 64, 16, 16]               0
             ReLU-14           [-1, 64, 16, 16]               0
             ReLU-15           [-1, 64, 16, 16]               0
             ReLU-16           [-1, 64, 16, 16]               0
           Conv2d-17          [-1, 128, 16, 16]          73,856
      BatchNorm2d-18          [-1, 128, 16, 16]             256
             ReLU-19          [-1, 128, 16, 16]               0
             ReLU-20          [-1, 128, 16, 16]               0
             ReLU-21          [-1, 128, 16, 16]               0
             ReLU-22          [-1, 128, 16, 16]               0
             ReLU-23          [-1, 128, 16, 16]               0
             ReLU-24          [-1, 128, 16, 16]               0
        MaxPool2d-25            [-1, 128, 8, 8]               0
      SEInitBlock-26            [-1, 128, 8, 8]               0
           Conv2d-27            [-1, 256, 8, 8]          33,024
      BatchNorm2d-28            [-1, 256, 8, 8]             512
           Conv2d-29             [-1, 64, 8, 8]           8,256
      BatchNorm2d-30             [-1, 64, 8, 8]             128
             ReLU-31             [-1, 64, 8, 8]               0
             ReLU-32             [-1, 64, 8, 8]               0
             ReLU-33             [-1, 64, 8, 8]               0
           Conv2d-34            [-1, 128, 8, 8]          73,856
      BatchNorm2d-35            [-1, 128, 8, 8]             256
             ReLU-36            [-1, 128, 8, 8]               0
             ReLU-37            [-1, 128, 8, 8]               0
             ReLU-38            [-1, 128, 8, 8]               0
             ReLU-39            [-1, 128, 8, 8]               0
             ReLU-40            [-1, 128, 8, 8]               0
             ReLU-41            [-1, 128, 8, 8]               0
           Conv2d-42            [-1, 256, 8, 8]          33,024
      BatchNorm2d-43            [-1, 256, 8, 8]             512
           Conv2d-44             [-1, 16, 1, 1]           4,112
             ReLU-45             [-1, 16, 1, 1]               0
             ReLU-46             [-1, 16, 1, 1]               0
             ReLU-47             [-1, 16, 1, 1]               0
           Conv2d-48            [-1, 256, 1, 1]           4,352
          Sigmoid-49            [-1, 256, 1, 1]               0
          Sigmoid-50            [-1, 256, 1, 1]               0
          Sigmoid-51            [-1, 256, 1, 1]               0
             ReLU-52            [-1, 256, 8, 8]               0
           Conv2d-53            [-1, 512, 4, 4]       1,180,160
      BatchNorm2d-54            [-1, 512, 4, 4]           1,024
           Conv2d-55            [-1, 128, 8, 8]          32,896
      BatchNorm2d-56            [-1, 128, 8, 8]             256
             ReLU-57            [-1, 128, 8, 8]               0
             ReLU-58            [-1, 128, 8, 8]               0
             ReLU-59            [-1, 128, 8, 8]               0
           Conv2d-60            [-1, 256, 4, 4]         295,168
      BatchNorm2d-61            [-1, 256, 4, 4]             512
             ReLU-62            [-1, 256, 4, 4]               0
             ReLU-63            [-1, 256, 4, 4]               0
             ReLU-64            [-1, 256, 4, 4]               0
             ReLU-65            [-1, 256, 4, 4]               0
             ReLU-66            [-1, 256, 4, 4]               0
             ReLU-67            [-1, 256, 4, 4]               0
           Conv2d-68            [-1, 512, 4, 4]         131,584
      BatchNorm2d-69            [-1, 512, 4, 4]           1,024
           Conv2d-70             [-1, 32, 1, 1]          16,416
             ReLU-71             [-1, 32, 1, 1]               0
             ReLU-72             [-1, 32, 1, 1]               0
             ReLU-73             [-1, 32, 1, 1]               0
           Conv2d-74            [-1, 512, 1, 1]          16,896
          Sigmoid-75            [-1, 512, 1, 1]               0
          Sigmoid-76            [-1, 512, 1, 1]               0
          Sigmoid-77            [-1, 512, 1, 1]               0
             ReLU-78            [-1, 512, 4, 4]               0
           Conv2d-79           [-1, 1024, 2, 2]       4,719,616
      BatchNorm2d-80           [-1, 1024, 2, 2]           2,048
           Conv2d-81            [-1, 256, 4, 4]         131,328
      BatchNorm2d-82            [-1, 256, 4, 4]             512
             ReLU-83            [-1, 256, 4, 4]               0
             ReLU-84            [-1, 256, 4, 4]               0
             ReLU-85            [-1, 256, 4, 4]               0
           Conv2d-86            [-1, 512, 2, 2]       1,180,160
      BatchNorm2d-87            [-1, 512, 2, 2]           1,024
             ReLU-88            [-1, 512, 2, 2]               0
             ReLU-89            [-1, 512, 2, 2]               0
             ReLU-90            [-1, 512, 2, 2]               0
             ReLU-91            [-1, 512, 2, 2]               0
             ReLU-92            [-1, 512, 2, 2]               0
             ReLU-93            [-1, 512, 2, 2]               0
           Conv2d-94           [-1, 1024, 2, 2]         525,312
      BatchNorm2d-95           [-1, 1024, 2, 2]           2,048
           Conv2d-96             [-1, 64, 1, 1]          65,600
             ReLU-97             [-1, 64, 1, 1]               0
             ReLU-98             [-1, 64, 1, 1]               0
             ReLU-99             [-1, 64, 1, 1]               0
          Conv2d-100           [-1, 1024, 1, 1]          66,560
         Sigmoid-101           [-1, 1024, 1, 1]               0
         Sigmoid-102           [-1, 1024, 1, 1]               0
         Sigmoid-103           [-1, 1024, 1, 1]               0
            ReLU-104           [-1, 1024, 2, 2]               0
AdaptiveAvgPool2d-105           [-1, 1024, 1, 1]               0
         Dropout-106                 [-1, 1024]               0
          Linear-107                   [-1, 10]          10,250
================================================================
Total params: 8,651,514
Trainable params: 8,651,514
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 6.79
Params size (MB): 33.00
Estimated Total Size (MB): 39.81
----------------------------------------------------------------
 :
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_34886d2bd42949ffa43c5041e94c6532/63df40b3677df615e44398b38375e0c5f6c546e8.svg" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ajEsLLCyjYd2" data-outputId="d83b7414-5212-45f7-fba1-c06813a045a2">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6835799813270569, time = 38.34700584411621
Epoch[1]: accuracy = 0.7913399934768677, time = 37.3702495098114
Epoch[2]: accuracy = 0.8375200033187866, time = 37.933491468429565
Epoch[3]: accuracy = 0.8983599543571472, time = 37.171271324157715
Epoch[4]: accuracy = 0.9249799847602844, time = 37.8235969543457
Epoch[5]: accuracy = 0.9432799816131592, time = 37.87195301055908
Epoch[6]: accuracy = 0.9565399885177612, time = 37.62522339820862
Epoch[7]: accuracy = 0.9709799885749817, time = 37.49402189254761
Epoch[8]: accuracy = 0.9698799848556519, time = 37.93991184234619
Epoch[9]: accuracy = 0.9809999465942383, time = 37.350093603134155
Epoch[10]: accuracy = 0.9861199855804443, time = 37.438411235809326
Epoch[11]: accuracy = 0.9856599569320679, time = 37.6477108001709
Epoch[12]: accuracy = 0.9884600043296814, time = 37.56942677497864
Epoch[13]: accuracy = 0.9910999536514282, time = 37.77301216125488
Epoch[14]: accuracy = 0.9917399883270264, time = 37.387455463409424
Epoch[15]: accuracy = 0.9926999807357788, time = 37.198869705200195
Epoch[16]: accuracy = 0.9943599700927734, time = 36.868306398391724
Epoch[17]: accuracy = 0.9939199686050415, time = 37.069987058639526
Epoch[18]: accuracy = 0.9953999519348145, time = 36.89101576805115
Epoch[19]: accuracy = 0.9947800040245056, time = 37.30176520347595
train time = 1165.9577128887177
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="D2nJs1Y5kuuG" data-outputId="b0cba7bd-770e-4834-f3f5-d0acc8bca94b">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7863999605178833
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="8HUhxYrNCjUe" data-outputId="0cf162b6-2d31-448d-faa8-15f119669b17">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>pip install ray</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting ray
  Downloading ray-2.7.1-cp310-cp310-manylinux2014_x86_64.whl (62.4 MB)
 62.4/62.4 MB 12.5 MB/s eta 0:00:00
ent already satisfied: click&gt;=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.12.4)
Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.1)
Requirement already satisfied: msgpack&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.7)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (23.2)
Requirement already satisfied: protobuf!=3.19.5,&gt;=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)
Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)
Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.0)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)
Requirement already satisfied: numpy&gt;=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray) (1.23.5)
Requirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (23.1.0)
Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (2023.7.1)
Requirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (0.30.2)
Requirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-&gt;ray) (0.10.6)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (3.3.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (2.0.7)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;ray) (2023.7.22)
Installing collected packages: ray
Successfully installed ray-2.7.1
</code></pre>
</div>
</div>
<div class="cell code" id="AUV8n55OChmT">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ray</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.tune <span class="im">import</span> CLIReporter</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.tune.schedulers <span class="im">import</span> ASHAScheduler</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> filelock <span class="im">import</span> FileLock</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray <span class="im">import</span> train, tune</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ray.train <span class="im">import</span> Checkpoint</span></code></pre></div>
</div>
<div class="cell code" id="1ddXjr_fEiA2">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_cifar(config):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  learning_rate <span class="op">=</span> config[<span class="st">&quot;learning_rate&quot;</span>]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  batch_size <span class="op">=</span> config[<span class="st">&quot;batch_size&quot;</span>]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>      root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>      transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>  test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>      root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>      transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(&#39;Number of train samples: {}&#39;.format(len(train_dataset)))</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show_images(train_dataset, &#39;Train samples&#39;)</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(&#39;Number of test samples: {}&#39;.format(len(test_dataset)))</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show_images(test_dataset, &#39;Test samples&#39;)</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>  train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>      train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>  test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>      test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>  cnn_model<span class="op">=</span>senet16()</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>  cnn_model.to(device)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>  loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>  start_all <span class="op">=</span> time.time()</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>      start <span class="op">=</span> time.time()</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>          images <span class="op">=</span> images.to(device)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>          labels <span class="op">=</span> labels.to(device)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>          outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>          loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>          optimizer.zero_grad()</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>          loss.backward()</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>          optimizer.step()</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>      end <span class="op">=</span> time.time()</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>      accuracy <span class="op">=</span> get_accuracy(train_data_loader, cnn_model)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, accuracy, (end <span class="op">-</span> start)))</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>  end_all <span class="op">=</span> time.time()</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>  os.makedirs(<span class="st">&quot;my_model&quot;</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>  torch.save(</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>      (cnn_model.state_dict(), optimizer.state_dict()), <span class="st">&quot;my_model/checkpoint.pt&quot;</span>)</span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>  checkpoint <span class="op">=</span> Checkpoint.from_directory(<span class="st">&quot;my_model&quot;</span>)</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>  train.report({<span class="st">&quot;accuracy&quot;</span>: accuracy.item()}, checkpoint<span class="op">=</span>checkpoint)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="kwUePJ_zN4Tz" data-outputId="268ac626-3e83-4efd-9736-0e9fa3c8e6c1">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;learning_rate&quot;</span>: tune.choice([<span class="fl">1e-3</span>, <span class="fl">1e-1</span>]),</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;batch_size&quot;</span>: tune.choice([<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>])</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>max_num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ASHAScheduler(</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># metric=&quot;loss&quot;,</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mode=&quot;min&quot;,</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        max_t<span class="op">=</span>max_num_epochs,</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        grace_period<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>tuner <span class="op">=</span> tune.Tuner(</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    tune.with_resources(</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        tune.with_parameters(train_cifar),</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># resources={&quot;cpu&quot;: 2, &quot;gpu&quot;: 0}</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        resources<span class="op">=</span>{<span class="st">&quot;cpu&quot;</span>: <span class="dv">2</span>, <span class="st">&quot;gpu&quot;</span>: <span class="dv">1</span>}</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    tune_config<span class="op">=</span>tune.TuneConfig(</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        metric<span class="op">=</span><span class="st">&quot;accuracy&quot;</span>,</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">&quot;min&quot;</span>,</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        scheduler<span class="op">=</span>scheduler,</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>        num_samples<span class="op">=</span>num_samples,</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    param_space<span class="op">=</span>config,</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> tuner.fit()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>best_result <span class="op">=</span> results.get_best_result(<span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;min&quot;</span>, <span class="st">&quot;last&quot;</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial config: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(best_result .config))</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial final validation accuracy: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    best_result .last_result[<span class="st">&quot;accuracy&quot;</span>]))</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best trial final validation accuracy: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    best_result .last_result[<span class="st">&quot;accuracy&quot;</span>]))</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a><span class="co"># best_trained_model = cnn_model(best_trial.config[&quot;l1&quot;], best_trial.config[&quot;l2&quot;])</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="co"># best_trained_model.to(device)</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;Test accuracy: {}&#39;.format(get_accuracy(test_data_loader, best_trained_model)))</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>2023-10-31 15:54:42,829	INFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2023-10-31 15:54:42,864	WARNING callback.py:137 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it&#39;s dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>+--------------------------------------------------------------------+
| Configuration for experiment     train_cifar_2023-10-31_15-54-42   |
+--------------------------------------------------------------------+
| Search algorithm                 BasicVariantGenerator             |
| Scheduler                        AsyncHyperBandScheduler           |
| Number of trials                 10                                |
+--------------------------------------------------------------------+

View detailed results here: /root/ray_results/train_cifar_2023-10-31_15-54-42

Trial status: 10 PENDING
Current time: 2023-10-31 15:54:43. Total running time: 0s
Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   PENDING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+

Trial train_cifar_ce6f6_00000 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00000 config         |
+----------------------------------------------+
| batch_size                                 4 |
| learning_rate                            0.1 |
+----------------------------------------------+
(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:49, 362849.74it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 675969.40it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2084945.92it/s]
  2%|         | 2719744/170498071 [00:00&lt;00:26, 6272409.30it/s]
  3%|         | 5636096/170498071 [00:00&lt;00:13, 12513233.51it/s]
  5%|         | 9273344/170498071 [00:00&lt;00:08, 19193316.41it/s]
  8%|         | 13008896/170498071 [00:00&lt;00:06, 24368777.35it/s]
 10%|         | 16744448/170498071 [00:01&lt;00:05, 28105997.70it/s]
 12%|        | 20480000/170498071 [00:01&lt;00:04, 30789564.19it/s]
 14%|        | 24084480/170498071 [00:01&lt;00:05, 27042541.53it/s]
 16%|        | 27820032/170498071 [00:01&lt;00:04, 29707288.73it/s]
 19%|        | 31555584/170498071 [00:01&lt;00:04, 31768512.66it/s]
 21%|        | 35291136/170498071 [00:01&lt;00:04, 33156681.78it/s]
 23%|       | 38993920/170498071 [00:01&lt;00:03, 34252775.22it/s]
 25%|       | 42663936/170498071 [00:01&lt;00:03, 34940719.76it/s]
 27%|       | 46432256/170498071 [00:01&lt;00:03, 35708780.86it/s]
 29%|       | 50069504/170498071 [00:02&lt;00:04, 30098587.32it/s]
 32%|      | 53772288/170498071 [00:02&lt;00:03, 31886902.60it/s]
 34%|      | 57475072/170498071 [00:02&lt;00:03, 33263436.57it/s]
 36%|      | 61014016/170498071 [00:02&lt;00:03, 33837884.73it/s]
 38%|      | 64749568/170498071 [00:02&lt;00:03, 34813460.61it/s]
 40%|      | 68452352/170498071 [00:02&lt;00:02, 35432752.67it/s]
 42%|     | 72056832/170498071 [00:02&lt;00:02, 35608333.68it/s]
 44%|     | 75661312/170498071 [00:02&lt;00:03, 30041866.34it/s]
 47%|     | 79331328/170498071 [00:02&lt;00:02, 31738001.93it/s]
 49%|     | 83165184/170498071 [00:03&lt;00:02, 33504554.62it/s]
 51%|     | 86638592/170498071 [00:03&lt;00:02, 33670223.58it/s]
 53%|    | 90112000/170498071 [00:03&lt;00:02, 33918612.01it/s]
 55%|    | 93585408/170498071 [00:03&lt;00:02, 33433299.85it/s]
 57%|    | 96993280/170498071 [00:03&lt;00:02, 33563191.04it/s]
 59%|    | 100401152/170498071 [00:03&lt;00:02, 33259433.39it/s]
 61%|    | 103776256/170498071 [00:03&lt;00:02, 29573400.60it/s]
 63%|   | 107184128/170498071 [00:03&lt;00:02, 30688039.70it/s]
 65%|   | 110559232/170498071 [00:03&lt;00:01, 31527394.86it/s]
 67%|   | 114393088/170498071 [00:04&lt;00:01, 32223817.20it/s]
 69%|   | 117866496/170498071 [00:04&lt;00:01, 32906643.40it/s]
 71%|   | 121208832/170498071 [00:04&lt;00:01, 32837610.33it/s]
 73%|  | 124911616/170498071 [00:04&lt;00:01, 34023762.52it/s]
 75%|  | 128352256/170498071 [00:04&lt;00:01, 21855036.71it/s]
 77%|  | 132055040/170498071 [00:04&lt;00:01, 25044305.51it/s]
 80%|  | 135823360/170498071 [00:04&lt;00:01, 27546352.43it/s]
 82%| | 139526144/170498071 [00:04&lt;00:01, 29872205.91it/s]
 84%| | 142966784/170498071 [00:05&lt;00:00, 31016002.71it/s]
 86%| | 146341888/170498071 [00:05&lt;00:00, 31309366.15it/s]
 88%| | 149716992/170498071 [00:05&lt;00:00, 28266610.61it/s]
 90%| | 152731648/170498071 [00:05&lt;00:00, 21611515.63it/s]
 92%|| 156794880/170498071 [00:05&lt;00:00, 25740279.80it/s]
 94%|| 160235520/170498071 [00:05&lt;00:00, 27786371.57it/s]
 96%|| 163545088/170498071 [00:05&lt;00:00, 29118796.98it/s]
 98%|| 167247872/170498071 [00:05&lt;00:00, 31191099.14it/s]
100%|| 170498071/170498071 [00:06&lt;00:00, 28310477.68it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:55:13. Total running time: 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:55:43. Total running time: 1min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:56:13. Total running time: 1min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:56:43. Total running time: 2min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:57:13. Total running time: 2min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:57:43. Total running time: 3min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:58:13. Total running time: 3min 30s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.670479953289032, time = 132.9324929714203
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:58:43. Total running time: 4min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:59:13. Total running time: 4min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 15:59:43. Total running time: 5min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:00:14. Total running time: 5min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:00:44. Total running time: 6min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:01:14. Total running time: 6min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7766000032424927, time = 128.77364206314087
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:01:44. Total running time: 7min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:02:14. Total running time: 7min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:02:44. Total running time: 8min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:03:14. Total running time: 8min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:03:44. Total running time: 9min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:04:14. Total running time: 9min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.830839991569519, time = 127.71064853668213
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:04:44. Total running time: 10min 1s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:05:14. Total running time: 10min 31s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:05:44. Total running time: 11min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:06:14. Total running time: 11min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:06:45. Total running time: 12min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:07:15. Total running time: 12min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.874019980430603, time = 129.5284743309021
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:07:45. Total running time: 13min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:08:15. Total running time: 13min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:08:45. Total running time: 14min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:09:15. Total running time: 14min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:09:45. Total running time: 15min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:10:15. Total running time: 15min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
Trial status: 1 RUNNING | 9 PENDING
Current time: 2023-10-31 16:10:45. Total running time: 16min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
+---------------------------------------------------------------------+
| Trial name                status       learning_rate     batch_size |
+---------------------------------------------------------------------+
| train_cifar_ce6f6_00000   RUNNING              0.1                4 |
| train_cifar_ce6f6_00001   PENDING              0.001             16 |
| train_cifar_ce6f6_00002   PENDING              0.1               32 |
| train_cifar_ce6f6_00003   PENDING              0.001              4 |
| train_cifar_ce6f6_00004   PENDING              0.001              4 |
| train_cifar_ce6f6_00005   PENDING              0.1               16 |
| train_cifar_ce6f6_00006   PENDING              0.1               16 |
| train_cifar_ce6f6_00007   PENDING              0.1               32 |
| train_cifar_ce6f6_00008   PENDING              0.001              8 |
| train_cifar_ce6f6_00009   PENDING              0.001             32 |
+---------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9064599871635437, time = 127.27779459953308
(train_cifar pid=3909) train time = 946.7944233417511

Trial train_cifar_ce6f6_00000 completed after 1 iterations at 2023-10-31 16:10:48. Total running time: 16min 5s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00000 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 959.95732 |
| time_total_s                                     959.95732 |
| training_iteration                                       1 |
| accuracy                                           0.90646 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00001 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00001 config           |
+------------------------------------------------+
| batch_size                                  16 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00000_0_batch_size=4,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:53, 359828.34it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 676505.04it/s]
  0%|          | 720896/170498071 [00:00&lt;01:24, 2011566.63it/s]
  1%|          | 1835008/170498071 [00:00&lt;00:42, 3947702.43it/s]
  3%|         | 4685824/170498071 [00:00&lt;00:15, 10458543.03it/s]
  4%|         | 7667712/170498071 [00:00&lt;00:10, 15715886.24it/s]
  6%|         | 9863168/170498071 [00:01&lt;00:10, 14822876.50it/s]
  7%|         | 12419072/170498071 [00:01&lt;00:09, 17531852.90it/s]
  9%|         | 15368192/170498071 [00:01&lt;00:07, 20713427.55it/s]
 11%|         | 18087936/170498071 [00:01&lt;00:07, 20401821.21it/s]
 12%|        | 20283392/170498071 [00:01&lt;00:07, 19566230.98it/s]
 13%|        | 22577152/170498071 [00:01&lt;00:07, 20439571.47it/s]
 15%|        | 25100288/170498071 [00:01&lt;00:06, 21734468.47it/s]
 16%|        | 27983872/170498071 [00:01&lt;00:06, 23705055.56it/s]
 18%|        | 30441472/170498071 [00:01&lt;00:06, 20879331.12it/s]
 19%|        | 32702464/170498071 [00:02&lt;00:06, 21147766.08it/s]
 21%|        | 35225600/170498071 [00:02&lt;00:06, 22160088.27it/s]
 22%|       | 38141952/170498071 [00:02&lt;00:05, 24103625.96it/s]
 24%|       | 40632320/170498071 [00:02&lt;00:06, 21232908.74it/s]
 25%|       | 42893312/170498071 [00:02&lt;00:05, 21578514.89it/s]
 27%|       | 45187072/170498071 [00:02&lt;00:05, 21874585.36it/s]
 28%|       | 47710208/170498071 [00:02&lt;00:05, 22799359.82it/s]
 30%|       | 50462720/170498071 [00:02&lt;00:05, 22523861.34it/s]
 31%|       | 52789248/170498071 [00:02&lt;00:05, 21557655.18it/s]
 32%|      | 55181312/170498071 [00:03&lt;00:05, 22004122.31it/s]
 34%|      | 57540608/170498071 [00:03&lt;00:05, 22314926.47it/s]
 35%|      | 60129280/170498071 [00:03&lt;00:04, 23281981.42it/s]
 37%|      | 62652416/170498071 [00:03&lt;00:04, 22387352.17it/s]
 38%|      | 65011712/170498071 [00:03&lt;00:04, 22640718.31it/s]
 39%|      | 67305472/170498071 [00:03&lt;00:04, 21920632.33it/s]
 41%|      | 69566464/170498071 [00:03&lt;00:04, 22098699.18it/s]
 42%|     | 71860224/170498071 [00:03&lt;00:04, 22319652.14it/s]
 44%|     | 74219520/170498071 [00:03&lt;00:04, 22646936.91it/s]
 45%|     | 76906496/170498071 [00:03&lt;00:03, 23451667.06it/s]
 46%|     | 79265792/170498071 [00:04&lt;00:04, 21735099.17it/s]
 48%|     | 81494016/170498071 [00:04&lt;00:04, 21598772.91it/s]
 49%|     | 83722240/170498071 [00:04&lt;00:03, 21743865.57it/s]
 50%|     | 85917696/170498071 [00:04&lt;00:03, 21778149.28it/s]
 52%|    | 88408064/170498071 [00:04&lt;00:03, 22678125.01it/s]
 53%|    | 91062272/170498071 [00:04&lt;00:03, 23763478.14it/s]
 55%|    | 93454336/170498071 [00:04&lt;00:03, 22010013.05it/s]
 56%|    | 95748096/170498071 [00:04&lt;00:03, 21894977.20it/s]
 57%|    | 97976320/170498071 [00:04&lt;00:03, 21572439.63it/s]
 59%|    | 100171776/170498071 [00:05&lt;00:03, 21573294.08it/s]
 60%|    | 102367232/170498071 [00:05&lt;00:03, 21647755.36it/s]
 61%|   | 104660992/170498071 [00:05&lt;00:03, 21901388.84it/s]
 63%|   | 106987520/170498071 [00:05&lt;00:02, 22186418.64it/s]
 64%|   | 109346816/170498071 [00:05&lt;00:02, 22586902.80it/s]
 65%|   | 111640576/170498071 [00:05&lt;00:02, 22612567.39it/s]
 67%|   | 114196480/170498071 [00:05&lt;00:02, 23236842.09it/s]
 68%|   | 116523008/170498071 [00:05&lt;00:02, 22446526.62it/s]
 70%|   | 118784000/170498071 [00:05&lt;00:02, 22475329.31it/s]
 71%|   | 121044992/170498071 [00:05&lt;00:02, 21624456.61it/s]
 72%|  | 123240448/170498071 [00:06&lt;00:02, 21138880.01it/s]
 74%|  | 125403136/170498071 [00:06&lt;00:02, 21247439.04it/s]
 75%|  | 127631360/170498071 [00:06&lt;00:01, 21512765.03it/s]
 76%|  | 130056192/170498071 [00:06&lt;00:01, 22269551.62it/s]
 78%|  | 132579328/170498071 [00:06&lt;00:01, 23033972.62it/s]
 79%|  | 134905856/170498071 [00:06&lt;00:01, 22743234.72it/s]
 81%|  | 137330688/170498071 [00:06&lt;00:01, 23126840.93it/s]
 82%| | 139657216/170498071 [00:06&lt;00:01, 22329021.22it/s]
 83%| | 141918208/170498071 [00:06&lt;00:01, 22036667.70it/s]
 85%| | 144146432/170498071 [00:07&lt;00:01, 21022787.80it/s]
 86%| | 146341888/170498071 [00:07&lt;00:01, 21141541.59it/s]
 87%| | 148570112/170498071 [00:07&lt;00:01, 21431320.80it/s]
 89%| | 151027712/170498071 [00:07&lt;00:00, 22300502.10it/s]
 90%| | 153485312/170498071 [00:07&lt;00:00, 22946330.28it/s]
 91%|| 155844608/170498071 [00:07&lt;00:00, 22882840.56it/s]
 93%|| 158171136/170498071 [00:07&lt;00:00, 22839263.48it/s]
 94%|| 160464896/170498071 [00:07&lt;00:00, 22757572.10it/s]
 95%|| 162758656/170498071 [00:07&lt;00:00, 22497712.21it/s]
 97%|| 165019648/170498071 [00:07&lt;00:00, 21335096.76it/s]
 98%|| 167280640/170498071 [00:08&lt;00:00, 21557412.36it/s]
 99%|| 169541632/170498071 [00:08&lt;00:00, 21849552.71it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20776756.24it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:11:15. Total running time: 16min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:11:45. Total running time: 17min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.3935999870300293, time = 39.21790838241577
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:12:15. Total running time: 17min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:12:45. Total running time: 18min 2s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4590199887752533, time = 38.98495864868164
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:13:15. Total running time: 18min 32s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:13:45. Total running time: 19min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.4958999752998352, time = 38.62439441680908
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:14:15. Total running time: 19min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:14:46. Total running time: 20min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5320999622344971, time = 38.550172567367554
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:15:16. Total running time: 20min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING
Current time: 2023-10-31 16:15:46. Total running time: 21min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00000 with accuracy=0.9064599871635437 and params={&#39;learning_rate&#39;: 0.1, &#39;batch_size&#39;: 4}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00001   RUNNING                0.001             16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00002   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.5552600026130676, time = 39.06446933746338
(train_cifar pid=3909) train time = 294.28418612480164

Trial train_cifar_ce6f6_00001 completed after 1 iterations at 2023-10-31 16:15:55. Total running time: 21min 12s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00001 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 306.70894 |
| time_total_s                                     306.70894 |
| training_iteration                                       1 |
| accuracy                                           0.55526 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00002 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00002 config         |
+----------------------------------------------+
| batch_size                                32 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00001_1_batch_size=16,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:55, 358722.47it/s]
  0%|          | 229376/170498071 [00:00&lt;04:12, 674576.34it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2084334.08it/s]
  2%|         | 2850816/170498071 [00:00&lt;00:25, 6617514.85it/s]
  3%|         | 5472256/170498071 [00:00&lt;00:13, 11959497.44it/s]
  5%|         | 7864320/170498071 [00:00&lt;00:12, 12910891.47it/s]
  6%|         | 10715136/170498071 [00:01&lt;00:09, 16846600.07it/s]
  8%|         | 13500416/170498071 [00:01&lt;00:07, 19748671.61it/s]
  9%|         | 16121856/170498071 [00:01&lt;00:08, 18148842.99it/s]
 11%|         | 19038208/170498071 [00:01&lt;00:07, 20871137.16it/s]
 13%|        | 21790720/170498071 [00:01&lt;00:06, 22610627.23it/s]
 14%|        | 24346624/170498071 [00:01&lt;00:07, 20044921.90it/s]
 16%|        | 26640384/170498071 [00:01&lt;00:06, 20752543.40it/s]
 17%|        | 29065216/170498071 [00:01&lt;00:06, 21664663.84it/s]
 19%|        | 31883264/170498071 [00:01&lt;00:05, 23437302.49it/s]
 20%|        | 34701312/170498071 [00:02&lt;00:06, 21181602.73it/s]
 22%|       | 36962304/170498071 [00:02&lt;00:06, 21359227.26it/s]
 23%|       | 39813120/170498071 [00:02&lt;00:05, 23256855.87it/s]
 25%|       | 42532864/170498071 [00:02&lt;00:05, 24328187.35it/s]
 26%|       | 45056000/170498071 [00:02&lt;00:05, 21011677.48it/s]
 28%|       | 47415296/170498071 [00:02&lt;00:05, 21671655.72it/s]
 29%|       | 50233344/170498071 [00:02&lt;00:05, 23361423.42it/s]
 31%|       | 52854784/170498071 [00:02&lt;00:04, 24099727.98it/s]
 32%|      | 55345152/170498071 [00:03&lt;00:05, 21315674.30it/s]
 34%|      | 57606144/170498071 [00:03&lt;00:05, 21299287.28it/s]
 35%|      | 60325888/170498071 [00:03&lt;00:04, 22877108.05it/s]
 37%|      | 63012864/170498071 [00:03&lt;00:04, 23979188.17it/s]
 38%|      | 65470464/170498071 [00:03&lt;00:04, 22190742.33it/s]
 40%|      | 67764224/170498071 [00:03&lt;00:04, 21383502.26it/s]
 41%|      | 70025216/170498071 [00:03&lt;00:04, 21381796.03it/s]
 43%|     | 72646656/170498071 [00:03&lt;00:04, 22689615.22it/s]
 44%|     | 75235328/170498071 [00:03&lt;00:04, 23524570.65it/s]
 46%|     | 77627392/170498071 [00:04&lt;00:04, 22191936.07it/s]
 47%|     | 79888384/170498071 [00:04&lt;00:04, 21796060.39it/s]
 48%|     | 82149376/170498071 [00:04&lt;00:04, 21955721.55it/s]
 49%|     | 84377600/170498071 [00:04&lt;00:03, 21950975.79it/s]
 51%|     | 86638592/170498071 [00:04&lt;00:03, 22016900.15it/s]
 52%|    | 89227264/170498071 [00:04&lt;00:03, 23107200.43it/s]
 54%|    | 91684864/170498071 [00:04&lt;00:03, 21775014.88it/s]
 55%|    | 94175232/170498071 [00:04&lt;00:03, 22285023.32it/s]
 57%|    | 96501760/170498071 [00:04&lt;00:03, 22522415.40it/s]
 58%|    | 98795520/170498071 [00:04&lt;00:03, 22414092.81it/s]
 59%|    | 101089280/170498071 [00:05&lt;00:03, 22481292.82it/s]
 61%|    | 103415808/170498071 [00:05&lt;00:02, 22709278.20it/s]
 62%|   | 105906176/170498071 [00:05&lt;00:02, 23324798.23it/s]
 63%|   | 108265472/170498071 [00:05&lt;00:02, 21819538.92it/s]
 65%|   | 110493696/170498071 [00:05&lt;00:02, 21810226.61it/s]
 66%|   | 112951296/170498071 [00:05&lt;00:02, 22592391.32it/s]
 68%|   | 115245056/170498071 [00:05&lt;00:02, 22524215.35it/s]
 69%|   | 117538816/170498071 [00:05&lt;00:02, 22532470.77it/s]
 70%|   | 119799808/170498071 [00:05&lt;00:02, 22546581.03it/s]
 72%|  | 122257408/170498071 [00:05&lt;00:02, 23140263.78it/s]
 73%|  | 124583936/170498071 [00:06&lt;00:02, 21568046.67it/s]
 74%|  | 126779392/170498071 [00:06&lt;00:02, 21654737.02it/s]
 76%|  | 129007616/170498071 [00:06&lt;00:01, 21268982.99it/s]
 77%|  | 131760128/170498071 [00:06&lt;00:01, 23013426.59it/s]
 79%|  | 134086656/170498071 [00:06&lt;00:01, 22332112.98it/s]
 80%|  | 136577024/170498071 [00:06&lt;00:01, 21219015.47it/s]
 82%| | 139165696/170498071 [00:06&lt;00:01, 22492405.03it/s]
 83%| | 141459456/170498071 [00:06&lt;00:01, 21931948.56it/s]
 84%| | 143785984/170498071 [00:06&lt;00:01, 22291698.25it/s]
 86%| | 146341888/170498071 [00:07&lt;00:01, 22793209.03it/s]
 87%| | 148865024/170498071 [00:07&lt;00:01, 21600806.60it/s]
 89%| | 151289856/170498071 [00:07&lt;00:00, 22306690.28it/s]
 90%| | 153550848/170498071 [00:07&lt;00:00, 21306230.26it/s]
 92%|| 156008448/170498071 [00:07&lt;00:00, 22169300.47it/s]
 93%|| 158433280/170498071 [00:07&lt;00:00, 22724949.81it/s]
 94%|| 160825344/170498071 [00:07&lt;00:00, 22851248.95it/s]
 96%|| 163151872/170498071 [00:07&lt;00:00, 21218293.51it/s]
 97%|| 165511168/170498071 [00:07&lt;00:00, 21840124.16it/s]
 98%|| 167739392/170498071 [00:08&lt;00:00, 21507444.49it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20816163.25it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:16:16. Total running time: 21min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:16:46. Total running time: 22min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6808599829673767, time = 25.51612639427185
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:17:16. Total running time: 22min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.765779972076416, time = 24.523948669433594
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:17:46. Total running time: 23min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8193999528884888, time = 25.170745372772217
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:18:16. Total running time: 23min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8781999945640564, time = 25.780768156051636
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:18:46. Total running time: 24min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING
Current time: 2023-10-31 16:19:16. Total running time: 24min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00002   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00003   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9199399948120117, time = 24.40403652191162
(train_cifar pid=3909) train time = 195.2490997314453

Trial train_cifar_ce6f6_00002 completed after 1 iterations at 2023-10-31 16:19:22. Total running time: 24min 39s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00002 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 207.59839 |
| time_total_s                                     207.59839 |
| training_iteration                                       1 |
| accuracy                                           0.91994 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00003 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00003 config           |
+------------------------------------------------+
| batch_size                                   4 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00002_2_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:07, 349900.85it/s]
  0%|          | 229376/170498071 [00:00&lt;04:12, 673769.59it/s]
  0%|          | 524288/170498071 [00:00&lt;02:02, 1384727.51it/s]
  1%|          | 1212416/170498071 [00:00&lt;00:55, 3065007.00it/s]
  2%|         | 2621440/170498071 [00:00&lt;00:26, 6351286.76it/s]
  3%|         | 4489216/170498071 [00:00&lt;00:16, 10040763.61it/s]
  4%|         | 6520832/170498071 [00:00&lt;00:12, 12985970.09it/s]
  5%|         | 9142272/170498071 [00:00&lt;00:09, 16898840.53it/s]
  7%|         | 11501568/170498071 [00:01&lt;00:08, 18887535.82it/s]
  8%|         | 13664256/170498071 [00:01&lt;00:08, 18798718.29it/s]
  9%|         | 15892480/170498071 [00:01&lt;00:07, 19767548.93it/s]
 11%|         | 18186240/170498071 [00:01&lt;00:07, 20583997.31it/s]
 12%|        | 20512768/170498071 [00:01&lt;00:07, 21324141.42it/s]
 13%|        | 22806528/170498071 [00:01&lt;00:06, 21788371.08it/s]
 15%|        | 25034752/170498071 [00:01&lt;00:06, 21924534.52it/s]
 18%|        | 29917184/170498071 [00:01&lt;00:06, 23259614.00it/s]
 19%|        | 32276480/170498071 [00:02&lt;00:06, 21935304.94it/s]
 20%|        | 34504704/170498071 [00:02&lt;00:06, 21968040.04it/s]
 22%|       | 36765696/170498071 [00:02&lt;00:06, 22060329.62it/s]
 23%|       | 39092224/170498071 [00:02&lt;00:05, 22384294.64it/s]
 24%|       | 41385984/170498071 [00:02&lt;00:05, 22484887.81it/s]
 26%|       | 43679744/170498071 [00:02&lt;00:05, 22456293.72it/s]
 27%|       | 46039040/170498071 [00:02&lt;00:05, 22782035.14it/s]
 29%|       | 48594944/170498071 [00:02&lt;00:05, 23575115.80it/s]
 30%|       | 50987008/170498071 [00:02&lt;00:05, 22030625.38it/s]
 31%|       | 53215232/170498071 [00:02&lt;00:05, 21985317.33it/s]
 33%|      | 55574528/170498071 [00:03&lt;00:05, 22439349.53it/s]
 34%|      | 57835520/170498071 [00:03&lt;00:05, 22487720.36it/s]
 35%|      | 60096512/170498071 [00:03&lt;00:04, 22421147.91it/s]
 37%|      | 62390272/170498071 [00:03&lt;00:04, 22551290.79it/s]
 38%|      | 64782336/170498071 [00:03&lt;00:04, 22910257.30it/s]
 39%|      | 67305472/170498071 [00:03&lt;00:04, 22360752.81it/s]
 41%|      | 69632000/170498071 [00:03&lt;00:04, 22481643.05it/s]
 42%|     | 71892992/170498071 [00:03&lt;00:04, 22189900.99it/s]
 44%|     | 74252288/170498071 [00:03&lt;00:04, 22579042.88it/s]
 45%|     | 76546048/170498071 [00:03&lt;00:04, 22440558.14it/s]
 46%|     | 78807040/170498071 [00:04&lt;00:04, 22439136.54it/s]
 48%|     | 81264640/170498071 [00:04&lt;00:03, 22780009.36it/s]
 49%|     | 83820544/170498071 [00:04&lt;00:03, 22476312.49it/s]
 50%|     | 86081536/170498071 [00:04&lt;00:03, 22502094.20it/s]
 52%|    | 88375296/170498071 [00:04&lt;00:03, 22533867.94it/s]
 53%|    | 90636288/170498071 [00:04&lt;00:03, 22478873.53it/s]
 55%|    | 92930048/170498071 [00:04&lt;00:03, 22517306.35it/s]
 56%|    | 95191040/170498071 [00:04&lt;00:03, 22480162.00it/s]
 57%|    | 97484800/170498071 [00:04&lt;00:03, 22606771.99it/s]
 59%|    | 99844096/170498071 [00:05&lt;00:03, 22744515.64it/s]
 60%|    | 102400000/170498071 [00:05&lt;00:03, 22269632.13it/s]
 61%|   | 104792064/170498071 [00:05&lt;00:02, 22617050.10it/s]
 63%|   | 107085824/170498071 [00:05&lt;00:02, 22363402.95it/s]
 64%|   | 109412352/170498071 [00:05&lt;00:02, 22341358.47it/s]
 66%|   | 111706112/170498071 [00:05&lt;00:02, 22513110.77it/s]
 67%|   | 114032640/170498071 [00:05&lt;00:02, 22712269.54it/s]
 68%|   | 116359168/170498071 [00:05&lt;00:02, 22812319.65it/s]
 70%|   | 118784000/170498071 [00:05&lt;00:02, 23233460.06it/s]
 71%|   | 121110528/170498071 [00:05&lt;00:02, 22216851.30it/s]
 72%|  | 123371520/170498071 [00:06&lt;00:02, 22206323.54it/s]
 74%|  | 125632512/170498071 [00:06&lt;00:02, 22314473.46it/s]
 75%|  | 127991808/170498071 [00:06&lt;00:01, 22300443.47it/s]
 76%|  | 130285568/170498071 [00:06&lt;00:01, 22465845.52it/s]
 78%|  | 132644864/170498071 [00:06&lt;00:01, 22703298.21it/s]
 79%|  | 134971392/170498071 [00:06&lt;00:01, 22769580.84it/s]
 81%|  | 137265152/170498071 [00:06&lt;00:01, 22628963.30it/s]
 82%| | 139558912/170498071 [00:06&lt;00:01, 22154058.17it/s]
 83%| | 141885440/170498071 [00:06&lt;00:01, 22310546.25it/s]
 85%| | 144179200/170498071 [00:06&lt;00:01, 22479389.81it/s]
 86%| | 146440192/170498071 [00:07&lt;00:01, 22514432.03it/s]
 87%| | 148701184/170498071 [00:07&lt;00:00, 22406628.15it/s]
 89%| | 151060480/170498071 [00:07&lt;00:00, 22703935.91it/s]
 90%| | 153354240/170498071 [00:07&lt;00:00, 22640300.09it/s]
 91%|| 155713536/170498071 [00:07&lt;00:00, 22848139.72it/s]
 93%|| 158007296/170498071 [00:07&lt;00:00, 22644962.90it/s]
 94%|| 160301056/170498071 [00:07&lt;00:00, 22302305.46it/s]
 95%|| 162594816/170498071 [00:07&lt;00:00, 22389210.38it/s]
 97%|| 164888576/170498071 [00:07&lt;00:00, 22525669.39it/s]
 98%|| 167215104/170498071 [00:08&lt;00:00, 22738645.24it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20884540.23it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:19:46. Total running time: 25min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:20:16. Total running time: 25min 33s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:20:46. Total running time: 26min 3s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:21:16. Total running time: 26min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:21:46. Total running time: 27min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:22:17. Total running time: 27min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.4229999780654907, time = 127.65873551368713
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:22:47. Total running time: 28min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:23:17. Total running time: 28min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:23:47. Total running time: 29min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:24:17. Total running time: 29min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:24:47. Total running time: 30min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:25:17. Total running time: 30min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:25:47. Total running time: 31min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.49333998560905457, time = 128.40579271316528
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:26:17. Total running time: 31min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:26:47. Total running time: 32min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:27:17. Total running time: 32min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:27:47. Total running time: 33min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:28:17. Total running time: 33min 34s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:28:47. Total running time: 34min 4s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5471999645233154, time = 129.01236128807068
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:29:17. Total running time: 34min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:29:47. Total running time: 35min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:30:17. Total running time: 35min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:30:48. Total running time: 36min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:31:18. Total running time: 36min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:31:48. Total running time: 37min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5875200033187866, time = 127.10114216804504
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:32:18. Total running time: 37min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:32:48. Total running time: 38min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:33:18. Total running time: 38min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:33:48. Total running time: 39min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:34:18. Total running time: 39min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING
Current time: 2023-10-31 16:34:48. Total running time: 40min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00003   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00004   PENDING                0.001              4                                          |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.6218799948692322, time = 127.78192067146301
(train_cifar pid=3909) train time = 933.7627487182617

Trial train_cifar_ce6f6_00003 completed after 1 iterations at 2023-10-31 16:35:08. Total running time: 40min 25s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00003 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 945.97803 |
| time_total_s                                     945.97803 |
| training_iteration                                       1 |
| accuracy                                           0.62188 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00004 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00004 config           |
+------------------------------------------------+
| batch_size                                   4 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00003_3_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:55, 358355.82it/s]
  0%|          | 196608/170498071 [00:00&lt;03:39, 775522.39it/s]
  0%|          | 327680/170498071 [00:00&lt;02:53, 980584.28it/s]
  1%|          | 917504/170498071 [00:00&lt;01:16, 2212208.86it/s]
  1%|         | 2162688/170498071 [00:00&lt;00:32, 5173222.05it/s]
  3%|         | 4554752/170498071 [00:00&lt;00:15, 10657919.44it/s]
  5%|         | 7766016/170498071 [00:00&lt;00:09, 16980632.51it/s]
  7%|         | 11272192/170498071 [00:00&lt;00:07, 22299149.25it/s]
  9%|         | 15073280/170498071 [00:01&lt;00:05, 26964064.07it/s]
 11%|         | 18120704/170498071 [00:01&lt;00:05, 27997765.62it/s]
 13%|        | 21331968/170498071 [00:01&lt;00:05, 28941267.43it/s]
 14%|        | 24510464/170498071 [00:01&lt;00:04, 29517516.37it/s]
 17%|        | 28475392/170498071 [00:01&lt;00:04, 31944281.61it/s]
 19%|        | 31850496/170498071 [00:01&lt;00:04, 32472738.84it/s]
 21%|        | 35520512/170498071 [00:01&lt;00:04, 33711131.37it/s]
 23%|       | 39256064/170498071 [00:01&lt;00:04, 30193849.42it/s]
 25%|       | 42958848/170498071 [00:01&lt;00:04, 31864110.09it/s]
 27%|       | 46858240/170498071 [00:02&lt;00:03, 33831096.25it/s]
 30%|       | 50364416/170498071 [00:02&lt;00:03, 32745718.96it/s]
 32%|      | 54263808/170498071 [00:02&lt;00:03, 34478257.16it/s]
 34%|      | 57769984/170498071 [00:02&lt;00:03, 34526436.52it/s]
 36%|      | 61276160/170498071 [00:02&lt;00:03, 30108834.61it/s]
 38%|      | 64520192/170498071 [00:02&lt;00:03, 30682201.49it/s]
 40%|      | 67731456/170498071 [00:02&lt;00:03, 31057740.59it/s]
 42%|     | 70909952/170498071 [00:02&lt;00:03, 31209196.63it/s]
 43%|     | 74088448/170498071 [00:02&lt;00:03, 31347810.32it/s]
 45%|     | 77266944/170498071 [00:02&lt;00:02, 31336066.19it/s]
 47%|     | 80510976/170498071 [00:03&lt;00:02, 31483268.53it/s]
 49%|     | 83755008/170498071 [00:03&lt;00:02, 31650869.22it/s]
 51%|     | 86933504/170498071 [00:03&lt;00:02, 31603153.39it/s]
 53%|    | 90112000/170498071 [00:03&lt;00:02, 31637983.57it/s]
 55%|    | 93290496/170498071 [00:03&lt;00:02, 31610447.62it/s]
 57%|    | 96534528/170498071 [00:03&lt;00:02, 31791124.72it/s]
 59%|    | 99745792/170498071 [00:03&lt;00:02, 31839603.05it/s]
 60%|    | 102957056/170498071 [00:03&lt;00:02, 28461435.26it/s]
 62%|   | 106135552/170498071 [00:03&lt;00:02, 29310649.89it/s]
 64%|   | 109314048/170498071 [00:04&lt;00:02, 29965092.08it/s]
 66%|   | 112361472/170498071 [00:04&lt;00:01, 30073381.10it/s]
 68%|   | 115572736/170498071 [00:04&lt;00:01, 30616977.91it/s]
 70%|   | 118784000/170498071 [00:04&lt;00:01, 31029471.68it/s]
 72%|  | 121995264/170498071 [00:04&lt;00:01, 31340954.79it/s]
 73%|  | 125173760/170498071 [00:04&lt;00:01, 31184111.72it/s]
 76%|  | 128745472/170498071 [00:04&lt;00:01, 32516143.67it/s]
 78%|  | 132513792/170498071 [00:04&lt;00:01, 34029133.99it/s]
 80%|  | 135954432/170498071 [00:04&lt;00:01, 29940752.57it/s]
 82%| | 139689984/170498071 [00:04&lt;00:00, 31938066.23it/s]
 84%| | 143458304/170498071 [00:05&lt;00:00, 33528655.04it/s]
 86%| | 146898944/170498071 [00:05&lt;00:00, 32524673.33it/s]
 88%| | 150568960/170498071 [00:05&lt;00:00, 33682677.33it/s]
 91%| | 154337280/170498071 [00:05&lt;00:00, 34816496.13it/s]
 93%|| 157876224/170498071 [00:05&lt;00:00, 34915346.75it/s]
 95%|| 161415168/170498071 [00:05&lt;00:00, 30895319.20it/s]
 97%|| 165183488/170498071 [00:05&lt;00:00, 32701603.94it/s]
100%|| 170498071/170498071 [00:05&lt;00:00, 28985460.92it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:35:18. Total running time: 40min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:35:48. Total running time: 41min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:36:18. Total running time: 41min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:36:48. Total running time: 42min 5s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:37:18. Total running time: 42min 35s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:37:48. Total running time: 43min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:38:18. Total running time: 43min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.4318599998950958, time = 126.27536916732788
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:38:49. Total running time: 44min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:39:19. Total running time: 44min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:39:49. Total running time: 45min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:40:19. Total running time: 45min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:40:49. Total running time: 46min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:41:19. Total running time: 46min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4997199773788452, time = 125.57496285438538
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:41:49. Total running time: 47min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:42:19. Total running time: 47min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:42:49. Total running time: 48min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:43:19. Total running time: 48min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:43:49. Total running time: 49min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:44:19. Total running time: 49min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5493999719619751, time = 128.38473892211914
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:44:49. Total running time: 50min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:45:19. Total running time: 50min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:45:49. Total running time: 51min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:46:19. Total running time: 51min 36s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:46:49. Total running time: 52min 6s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:47:19. Total running time: 52min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5884599685668945, time = 128.52797555923462
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:47:49. Total running time: 53min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:48:20. Total running time: 53min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:48:50. Total running time: 54min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:49:20. Total running time: 54min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:49:50. Total running time: 55min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 4 TERMINATED | 1 RUNNING | 5 PENDING
Current time: 2023-10-31 16:50:20. Total running time: 55min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00004   RUNNING                0.001              4                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00005   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.6177799701690674, time = 127.81025004386902
(train_cifar pid=3909) train time = 930.2581012248993

Trial train_cifar_ce6f6_00004 completed after 1 iterations at 2023-10-31 16:50:49. Total running time: 56min 6s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00004 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 940.36313 |
| time_total_s                                     940.36313 |
| training_iteration                                       1 |
| accuracy                                           0.61778 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00005 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00005 config         |
+----------------------------------------------+
| batch_size                                16 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00004_4_batch_size=4,learning_rate=0.0010_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:52, 360445.51it/s]
  0%|          | 229376/170498071 [00:00&lt;04:10, 680851.57it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:50:50. Total running time: 56min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 589824/170498071 [00:00&lt;01:45, 1605238.32it/s]
  1%|          | 1441792/170498071 [00:00&lt;00:45, 3749338.56it/s]
  2%|         | 3276800/170498071 [00:00&lt;00:20, 8222975.15it/s]
  3%|         | 5668864/170498071 [00:00&lt;00:12, 13005952.96it/s]
  5%|         | 7831552/170498071 [00:00&lt;00:11, 13663453.41it/s]
  6%|         | 10354688/170498071 [00:01&lt;00:09, 16836990.60it/s]
  7%|         | 12648448/170498071 [00:01&lt;00:08, 18554203.03it/s]
  9%|         | 15073280/170498071 [00:01&lt;00:07, 20177789.30it/s]
 12%|        | 19693568/170498071 [00:01&lt;00:06, 21670248.20it/s]
 13%|        | 22151168/170498071 [00:01&lt;00:06, 22440894.36it/s]
 14%|        | 24444928/170498071 [00:01&lt;00:07, 20681322.15it/s]
 16%|        | 26771456/170498071 [00:01&lt;00:06, 21387499.18it/s]
 17%|        | 29032448/170498071 [00:01&lt;00:06, 21638554.75it/s]
 18%|        | 31391744/170498071 [00:01&lt;00:06, 22124984.56it/s]
 20%|        | 33816576/170498071 [00:02&lt;00:06, 22737567.36it/s]
 21%|        | 36208640/170498071 [00:02&lt;00:05, 23033669.22it/s]
 23%|       | 38600704/170498071 [00:02&lt;00:06, 21346483.57it/s]
 24%|       | 40992768/170498071 [00:02&lt;00:05, 21959022.74it/s]
 25%|       | 43220992/170498071 [00:02&lt;00:05, 22015747.93it/s]
 27%|       | 45547520/170498071 [00:02&lt;00:05, 22338010.77it/s]
 28%|       | 47874048/170498071 [00:02&lt;00:05, 22530785.42it/s]
 30%|       | 50298880/170498071 [00:02&lt;00:05, 22967759.98it/s]
 31%|       | 52625408/170498071 [00:02&lt;00:05, 22852431.17it/s]
 32%|      | 54919168/170498071 [00:03&lt;00:05, 21375343.92it/s]
 34%|      | 57442304/170498071 [00:03&lt;00:05, 21953001.98it/s]
 35%|      | 59736064/170498071 [00:03&lt;00:05, 22043982.59it/s]
 36%|      | 62193664/170498071 [00:03&lt;00:04, 22719640.22it/s]
 38%|      | 64520192/170498071 [00:03&lt;00:04, 22837065.33it/s]
 39%|      | 66912256/170498071 [00:03&lt;00:04, 23090827.25it/s]
 41%|      | 69238784/170498071 [00:03&lt;00:04, 21434941.30it/s]
 42%|     | 71598080/170498071 [00:03&lt;00:04, 21528987.90it/s]
 43%|     | 73957376/170498071 [00:03&lt;00:04, 22101318.84it/s]
 45%|     | 76382208/170498071 [00:03&lt;00:04, 22713612.13it/s]
 46%|     | 78774272/170498071 [00:04&lt;00:03, 23049555.63it/s]
 48%|     | 81100800/170498071 [00:04&lt;00:03, 23046054.13it/s]
 49%|     | 83427328/170498071 [00:04&lt;00:03, 23002801.25it/s]
 50%|     | 85753856/170498071 [00:04&lt;00:03, 21761537.60it/s]
 52%|    | 87982080/170498071 [00:04&lt;00:03, 21454970.80it/s]
 53%|    | 90406912/170498071 [00:04&lt;00:03, 22146708.92it/s]
 54%|    | 92897280/170498071 [00:04&lt;00:03, 22928036.79it/s]
 56%|    | 95322112/170498071 [00:04&lt;00:03, 23196248.01it/s]
 57%|    | 97681408/170498071 [00:04&lt;00:03, 23203079.75it/s]
 59%|    | 100007936/170498071 [00:05&lt;00:03, 21703958.06it/s]
 60%|    | 102268928/170498071 [00:05&lt;00:03, 21474369.40it/s]
 61%|   | 104693760/170498071 [00:05&lt;00:02, 22042067.62it/s]
 63%|   | 107216896/170498071 [00:05&lt;00:02, 22817615.92it/s]
 64%|   | 109838336/170498071 [00:05&lt;00:02, 23578482.77it/s]
 66%|   | 112230400/170498071 [00:05&lt;00:02, 23636152.47it/s]
 67%|   | 114622464/170498071 [00:05&lt;00:02, 21407779.89it/s]
 69%|   | 116916224/170498071 [00:05&lt;00:02, 21745182.05it/s]
 70%|   | 119308288/170498071 [00:05&lt;00:02, 22347013.98it/s]
 71%|  | 121831424/170498071 [00:05&lt;00:02, 23131587.05it/s]
 73%|  | 124321792/170498071 [00:06&lt;00:01, 23519837.97it/s]
 74%|  | 126713856/170498071 [00:06&lt;00:01, 22001305.88it/s]
 76%|  | 129007616/170498071 [00:06&lt;00:01, 21674334.72it/s]
 77%|  | 131203072/170498071 [00:06&lt;00:01, 21650529.79it/s]
 78%|  | 133660672/170498071 [00:06&lt;00:01, 22444809.69it/s]
 80%|  | 136118272/170498071 [00:06&lt;00:01, 22830470.54it/s]
 81%| | 138674176/170498071 [00:06&lt;00:01, 23545263.81it/s]
 83%| | 141066240/170498071 [00:06&lt;00:01, 22174805.43it/s]
 84%| | 143425536/170498071 [00:06&lt;00:01, 22091060.77it/s]
 85%| | 145653760/170498071 [00:07&lt;00:01, 22042161.68it/s]
 87%| | 148144128/170498071 [00:07&lt;00:00, 22749145.89it/s]
 88%| | 150437888/170498071 [00:07&lt;00:00, 22703274.68it/s]
 90%| | 152928256/170498071 [00:07&lt;00:00, 23342719.37it/s]
 91%| | 155287552/170498071 [00:07&lt;00:00, 22224249.66it/s]
 92%|| 157614080/170498071 [00:07&lt;00:00, 22502247.83it/s]
 94%|| 159907840/170498071 [00:07&lt;00:00, 22347711.68it/s]
 95%|| 162168832/170498071 [00:07&lt;00:00, 22128943.30it/s]
 97%|| 164593664/170498071 [00:07&lt;00:00, 22694314.74it/s]
 98%|| 166952960/170498071 [00:07&lt;00:00, 22888572.63it/s]
 99%|| 169246720/170498071 [00:08&lt;00:00, 22820177.59it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20912709.36it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:51:20. Total running time: 56min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:51:50. Total running time: 57min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6728399991989136, time = 38.56611657142639
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:52:20. Total running time: 57min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:52:50. Total running time: 58min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7697199583053589, time = 38.52308917045593
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:53:20. Total running time: 58min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:53:50. Total running time: 59min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8376999497413635, time = 38.70884370803833
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:54:20. Total running time: 59min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:54:50. Total running time: 1hr 0min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8943399786949158, time = 38.13762617111206
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:55:20. Total running time: 1hr 0min 37s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 5 TERMINATED | 1 RUNNING | 4 PENDING
Current time: 2023-10-31 16:55:50. Total running time: 1hr 1min 7s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00005   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00006   PENDING                0.1               16                                          |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9262599945068359, time = 38.1430881023407
(train_cifar pid=3909) train time = 294.1284604072571

Trial train_cifar_ce6f6_00005 completed after 1 iterations at 2023-10-31 16:55:56. Total running time: 1hr 1min 13s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00005 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                   306.802 |
| time_total_s                                       306.802 |
| training_iteration                                       1 |
| accuracy                                           0.92626 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00006 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00006 config         |
+----------------------------------------------+
| batch_size                                16 |
| learning_rate                            0.1 |
+----------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00005_5_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:11, 346711.28it/s]
  0%|          | 229376/170498071 [00:00&lt;04:16, 664266.42it/s]
  0%|          | 524288/170498071 [00:00&lt;02:03, 1373333.00it/s]
  1%|          | 1179648/170498071 [00:00&lt;00:57, 2955091.49it/s]
  2%|         | 2588672/170498071 [00:00&lt;00:26, 6336068.42it/s]
  2%|         | 4194304/170498071 [00:00&lt;00:17, 9243817.70it/s]
  4%|         | 6094848/170498071 [00:00&lt;00:13, 12147564.06it/s]
  5%|         | 8126464/170498071 [00:00&lt;00:11, 14518639.14it/s]
  6%|         | 10289152/170498071 [00:01&lt;00:09, 16613194.38it/s]
  7%|         | 12517376/170498071 [00:01&lt;00:08, 18298009.88it/s]
  9%|         | 14876672/170498071 [00:01&lt;00:07, 19848420.90it/s]
 10%|         | 17301504/170498071 [00:01&lt;00:07, 21034951.98it/s]
 12%|        | 19857408/170498071 [00:01&lt;00:06, 22378407.94it/s]
 13%|        | 22347776/170498071 [00:01&lt;00:06, 23099905.42it/s]
 15%|        | 24969216/170498071 [00:01&lt;00:06, 23987200.96it/s]
 16%|        | 27852800/170498071 [00:01&lt;00:05, 25409362.42it/s]
 18%|        | 30703616/170498071 [00:01&lt;00:05, 26291190.57it/s]
 20%|        | 33816576/170498071 [00:01&lt;00:05, 27113097.52it/s]
 22%|       | 36765696/170498071 [00:02&lt;00:04, 27787793.68it/s]
 23%|       | 39682048/170498071 [00:02&lt;00:04, 28170878.06it/s]
 25%|       | 42663936/170498071 [00:02&lt;00:04, 28558349.58it/s]
 27%|       | 45711360/170498071 [00:02&lt;00:04, 29098606.90it/s]
 29%|       | 48627712/170498071 [00:02&lt;00:04, 28847065.76it/s]
 30%|       | 51707904/170498071 [00:02&lt;00:04, 29373595.04it/s]
 32%|      | 54657024/170498071 [00:02&lt;00:03, 29012066.31it/s]
 34%|      | 57638912/170498071 [00:02&lt;00:03, 29235303.01it/s]
 36%|      | 60588032/170498071 [00:02&lt;00:03, 28979583.25it/s]
 37%|      | 63635456/170498071 [00:03&lt;00:03, 29404352.04it/s]
 39%|      | 66682880/170498071 [00:03&lt;00:03, 29581090.29it/s]
 41%|      | 69664768/170498071 [00:03&lt;00:03, 29231924.85it/s]
 43%|     | 72810496/170498071 [00:03&lt;00:03, 29871574.01it/s]
 44%|     | 75825152/170498071 [00:03&lt;00:03, 29116610.42it/s]
 46%|     | 78872576/170498071 [00:03&lt;00:03, 29504892.50it/s]
 48%|     | 81854464/170498071 [00:03&lt;00:03, 29182797.22it/s]
 50%|     | 84869120/170498071 [00:03&lt;00:02, 29413801.85it/s]
 52%|    | 87818240/170498071 [00:03&lt;00:02, 29201838.98it/s]
 53%|    | 90963968/170498071 [00:03&lt;00:02, 29815137.83it/s]
 55%|    | 93978624/170498071 [00:04&lt;00:02, 29035858.79it/s]
 57%|    | 97255424/170498071 [00:04&lt;00:02, 29315981.54it/s]
 59%|    | 100401152/170498071 [00:04&lt;00:02, 29817251.46it/s]
 61%|    | 104071168/170498071 [00:04&lt;00:02, 31808054.49it/s]
 63%|   | 107577344/170498071 [00:04&lt;00:01, 32731909.73it/s]
 65%|   | 111181824/170498071 [00:04&lt;00:01, 33707190.04it/s]
 67%|   | 114589696/170498071 [00:04&lt;00:01, 31136599.38it/s]
 69%|   | 118226944/170498071 [00:04&lt;00:01, 32581356.88it/s]
 71%|  | 121831424/170498071 [00:04&lt;00:01, 33552054.14it/s]
 74%|  | 125468672/170498071 [00:04&lt;00:01, 34367773.27it/s]
 76%|  | 128942080/170498071 [00:05&lt;00:01, 30644919.11it/s]
 77%|  | 132120576/170498071 [00:05&lt;00:01, 30289500.90it/s]
 79%|  | 135233536/170498071 [00:05&lt;00:01, 30167712.94it/s]
 81%|  | 138313728/170498071 [00:05&lt;00:01, 29979602.45it/s]
 83%| | 141361152/170498071 [00:05&lt;00:01, 27098135.46it/s]
 85%| | 144146432/170498071 [00:05&lt;00:01, 25444842.72it/s]
 86%| | 147030016/170498071 [00:05&lt;00:00, 26251754.18it/s]
 88%| | 149913600/170498071 [00:05&lt;00:00, 26911313.79it/s]
 90%| | 152829952/170498071 [00:06&lt;00:00, 27518604.86it/s]
 91%|| 155779072/170498071 [00:06&lt;00:00, 27991563.19it/s]
 93%|| 158629888/170498071 [00:06&lt;00:00, 27659531.01it/s]
 95%|| 161546240/170498071 [00:06&lt;00:00, 28070539.61it/s]
 96%|| 164528128/170498071 [00:06&lt;00:00, 28433514.75it/s]
 98%|| 167542784/170498071 [00:06&lt;00:00, 28592581.48it/s]
100%|| 170498071/170498071 [00:06&lt;00:00, 25736246.04it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:56:20. Total running time: 1hr 1min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:56:50. Total running time: 1hr 2min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6760199666023254, time = 38.34031796455383
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:57:21. Total running time: 1hr 2min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:57:51. Total running time: 1hr 3min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.795740008354187, time = 38.32409405708313
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:58:21. Total running time: 1hr 3min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:58:51. Total running time: 1hr 4min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8547999858856201, time = 38.247161626815796
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:59:21. Total running time: 1hr 4min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 16:59:51. Total running time: 1hr 5min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.89301997423172, time = 38.3633337020874
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 17:00:21. Total running time: 1hr 5min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING
Current time: 2023-10-31 17:00:51. Total running time: 1hr 6min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00006   RUNNING                0.1               16                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00007   PENDING                0.1               32                                          |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9261999726295471, time = 38.08350658416748
(train_cifar pid=3909) train time = 293.47102522850037

Trial train_cifar_ce6f6_00006 completed after 1 iterations at 2023-10-31 17:01:02. Total running time: 1hr 6min 19s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00006 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 306.49549 |
| time_total_s                                     306.49549 |
| training_iteration                                       1 |
| accuracy                                            0.9262 |
+------------------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00006_6_batch_size=16,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>
Trial train_cifar_ce6f6_00007 started with configuration:
+----------------------------------------------+
| Trial train_cifar_ce6f6_00007 config         |
+----------------------------------------------+
| batch_size                                32 |
| learning_rate                            0.1 |
+----------------------------------------------+
(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:18, 341950.89it/s]
  0%|          | 163840/170498071 [00:00&lt;04:41, 605128.12it/s]
  0%|          | 327680/170498071 [00:00&lt;02:51, 991297.23it/s]
  0%|          | 688128/170498071 [00:00&lt;01:30, 1877488.85it/s]
  1%|          | 1507328/170498071 [00:00&lt;00:42, 3950781.62it/s]
  1%|         | 2326528/170498071 [00:00&lt;00:31, 5277338.20it/s]
  2%|         | 3375104/170498071 [00:00&lt;00:24, 6864874.22it/s]
  3%|         | 4456448/170498071 [00:00&lt;00:20, 8054150.65it/s]
  3%|         | 5603328/170498071 [00:01&lt;00:18, 9074364.07it/s]
  4%|         | 6782976/170498071 [00:01&lt;00:16, 9832425.73it/s]
  5%|         | 8028160/170498071 [00:01&lt;00:15, 10503844.85it/s]
  5%|         | 9338880/170498071 [00:01&lt;00:14, 11278126.79it/s]
  7%|         | 12222464/170498071 [00:01&lt;00:12, 12818100.18it/s]
  8%|         | 13828096/170498071 [00:01&lt;00:11, 13706684.84it/s]
  9%|         | 15499264/170498071 [00:01&lt;00:10, 14526489.04it/s]
 10%|         | 17268736/170498071 [00:01&lt;00:09, 15433871.04it/s]
 11%|         | 19103744/170498071 [00:01&lt;00:09, 16284655.57it/s]
 12%|        | 21102592/170498071 [00:02&lt;00:08, 17293217.71it/s]
 14%|        | 23265280/170498071 [00:02&lt;00:07, 18580041.62it/s]
 15%|        | 25427968/170498071 [00:02&lt;00:07, 19454433.99it/s]
 16%|        | 27656192/170498071 [00:02&lt;00:07, 20265518.01it/s]
 18%|        | 29851648/170498071 [00:02&lt;00:06, 20672994.70it/s]
 19%|        | 32243712/170498071 [00:02&lt;00:06, 21498794.32it/s]
 20%|        | 34471936/170498071 [00:02&lt;00:06, 21560172.30it/s]
 22%|       | 36798464/170498071 [00:02&lt;00:06, 22003604.22it/s]
 23%|       | 39026688/170498071 [00:02&lt;00:06, 21908635.34it/s]
 24%|       | 41353216/170498071 [00:02&lt;00:05, 22289581.83it/s]
 26%|       | 43614208/170498071 [00:03&lt;00:05, 22282043.46it/s]
 27%|       | 46006272/170498071 [00:03&lt;00:05, 22656842.90it/s]
 28%|       | 48300032/170498071 [00:03&lt;00:05, 22464528.28it/s]
 30%|       | 50561024/170498071 [00:03&lt;00:05, 22262691.14it/s]
 31%|       | 52920320/170498071 [00:03&lt;00:05, 22604518.98it/s]
 32%|      | 55214080/170498071 [00:03&lt;00:05, 22494815.87it/s]
 34%|      | 57507840/170498071 [00:03&lt;00:04, 22599574.37it/s]
 35%|      | 59768832/170498071 [00:03&lt;00:04, 22359610.01it/s]
 36%|      | 62029824/170498071 [00:03&lt;00:04, 22255573.10it/s]
 38%|      | 64454656/170498071 [00:03&lt;00:04, 22721876.93it/s]
 39%|      | 66813952/170498071 [00:04&lt;00:04, 22708960.92it/s]
 41%|      | 69107712/170498071 [00:04&lt;00:04, 22319060.65it/s]
 42%|     | 71401472/170498071 [00:04&lt;00:04, 22498574.83it/s]
 43%|     | 73728000/170498071 [00:04&lt;00:04, 22499820.38it/s]
 45%|     | 76054528/170498071 [00:04&lt;00:04, 22683785.47it/s]
 46%|     | 78348288/170498071 [00:04&lt;00:04, 22317410.47it/s]
 47%|     | 80609280/170498071 [00:04&lt;00:04, 22399209.52it/s]
 49%|     | 82968576/170498071 [00:04&lt;00:03, 22709612.93it/s]
 50%|     | 85295104/170498071 [00:04&lt;00:03, 22863086.57it/s]
 51%|    | 87588864/170498071 [00:04&lt;00:03, 22673926.36it/s]
 53%|    | 89882624/170498071 [00:05&lt;00:03, 22273675.59it/s]
 54%|    | 92241920/170498071 [00:05&lt;00:03, 22608579.10it/s]
 55%|    | 94535680/170498071 [00:05&lt;00:03, 22589400.61it/s]
 57%|    | 96796672/170498071 [00:05&lt;00:03, 22408893.44it/s]
 58%|    | 99057664/170498071 [00:05&lt;00:03, 22211364.76it/s]
 59%|    | 101384192/170498071 [00:05&lt;00:03, 22450359.96it/s]
 61%|    | 103710720/170498071 [00:05&lt;00:02, 22631720.93it/s]
 62%|   | 106135552/170498071 [00:05&lt;00:02, 22689527.00it/s]
 64%|   | 108429312/170498071 [00:05&lt;00:02, 22452584.13it/s]
 65%|   | 110723072/170498071 [00:06&lt;00:02, 22534450.72it/s]
 66%|   | 112984064/170498071 [00:06&lt;00:02, 22303210.59it/s]
 68%|   | 115343360/170498071 [00:06&lt;00:02, 22517136.40it/s]
 69%|   | 117604352/170498071 [00:06&lt;00:02, 22369658.34it/s]
 70%|   | 119930880/170498071 [00:06&lt;00:02, 22554342.26it/s]
 72%|  | 122257408/170498071 [00:06&lt;00:02, 22484156.17it/s]
 73%|  | 124747776/170498071 [00:06&lt;00:02, 22870653.59it/s]
 75%|  | 127041536/170498071 [00:06&lt;00:01, 22714295.03it/s]
 76%|  | 129335296/170498071 [00:06&lt;00:01, 22499903.36it/s]
 77%|  | 131661824/170498071 [00:06&lt;00:01, 22315995.39it/s]
 79%|  | 133955584/170498071 [00:07&lt;00:01, 22472700.00it/s]
 80%|  | 136216576/170498071 [00:07&lt;00:01, 22469442.70it/s]
 81%| | 138543104/170498071 [00:07&lt;00:01, 22698894.20it/s]
 83%| | 140836864/170498071 [00:07&lt;00:01, 22596046.47it/s]
 84%| | 143163392/170498071 [00:07&lt;00:01, 22742213.61it/s]
 85%| | 145457152/170498071 [00:07&lt;00:01, 22666320.95it/s]
 87%| | 147750912/170498071 [00:07&lt;00:01, 22149240.53it/s]
 88%| | 150208512/170498071 [00:07&lt;00:00, 22469857.25it/s]
 89%| | 152469504/170498071 [00:07&lt;00:00, 22471052.47it/s]
 91%| | 154730496/170498071 [00:07&lt;00:00, 22308527.49it/s]
 92%|| 157089792/170498071 [00:08&lt;00:00, 22664646.46it/s]
 93%|| 159383552/170498071 [00:08&lt;00:00, 22561223.67it/s]
 95%|| 161710080/170498071 [00:08&lt;00:00, 22765948.27it/s]
 96%|| 164003840/170498071 [00:08&lt;00:00, 22612324.60it/s]
 98%|| 166363136/170498071 [00:08&lt;00:00, 22346221.23it/s]
 99%|| 168722432/170498071 [00:08&lt;00:00, 22670104.62it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 19698302.19it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:01:21. Total running time: 1hr 6min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:01:51. Total running time: 1hr 7min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.6528399586677551, time = 25.4039409160614
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:02:21. Total running time: 1hr 7min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.7628999948501587, time = 25.41545343399048
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:02:51. Total running time: 1hr 8min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.8079800009727478, time = 24.244602918624878
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:03:21. Total running time: 1hr 8min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:03:51. Total running time: 1hr 9min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.8719599843025208, time = 25.373770713806152
Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING
Current time: 2023-10-31 17:04:21. Total running time: 1hr 9min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00007   RUNNING                0.1               32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00008   PENDING                0.001              8                                          |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.9140599966049194, time = 25.575221300125122
(train_cifar pid=3909) train time = 196.36088514328003

Trial train_cifar_ce6f6_00007 completed after 1 iterations at 2023-10-31 17:04:31. Total running time: 1hr 9min 48s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00007 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 209.19587 |
| time_total_s                                     209.19587 |
| training_iteration                                       1 |
| accuracy                                           0.91406 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00008 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00008 config           |
+------------------------------------------------+
| batch_size                                   8 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00007_7_batch_size=32,learning_rate=0.1000_2023-10-31_15-54-42/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>  0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;07:54, 358835.79it/s]
  0%|          | 229376/170498071 [00:00&lt;04:11, 675900.94it/s]
  1%|          | 917504/170498071 [00:00&lt;01:21, 2088807.00it/s]
  2%|         | 2850816/170498071 [00:00&lt;00:25, 6624503.17it/s]
  3%|         | 5570560/170498071 [00:00&lt;00:13, 12244628.32it/s]
  5%|         | 7831552/170498071 [00:00&lt;00:12, 12761101.87it/s]
  6%|         | 10715136/170498071 [00:01&lt;00:09, 16807657.55it/s]
  8%|         | 13631488/170498071 [00:01&lt;00:07, 20071712.02it/s]
  9%|         | 16121856/170498071 [00:01&lt;00:08, 18095516.64it/s]
 11%|         | 18874368/170498071 [00:01&lt;00:07, 20434859.21it/s]
 13%|        | 21790720/170498071 [00:01&lt;00:06, 22707220.59it/s]
 14%|        | 24444928/170498071 [00:01&lt;00:07, 20119115.37it/s]
 16%|        | 26968064/170498071 [00:01&lt;00:06, 21383503.43it/s]
 18%|        | 29851648/170498071 [00:01&lt;00:06, 23327973.43it/s]
 19%|        | 32636928/170498071 [00:02&lt;00:06, 20885335.42it/s]
 20%|        | 34930688/170498071 [00:02&lt;00:06, 21386289.56it/s]
 22%|       | 37879808/170498071 [00:02&lt;00:05, 23493592.50it/s]
 24%|       | 40632320/170498071 [00:02&lt;00:05, 24570431.96it/s]
 25%|       | 43188224/170498071 [00:02&lt;00:06, 20943747.47it/s]
 27%|       | 45907968/170498071 [00:02&lt;00:05, 22451815.41it/s]
 29%|       | 48758784/170498071 [00:02&lt;00:05, 24015259.85it/s]
 30%|       | 51347456/170498071 [00:02&lt;00:05, 21363518.97it/s]
 31%|      | 53641216/170498071 [00:02&lt;00:05, 21447517.27it/s]
 33%|      | 56393728/170498071 [00:03&lt;00:04, 23036104.67it/s]
 35%|      | 59146240/170498071 [00:03&lt;00:04, 24254726.55it/s]
 36%|      | 61734912/170498071 [00:03&lt;00:05, 21408053.34it/s]
 38%|      | 63995904/170498071 [00:03&lt;00:04, 21511048.73it/s]
 39%|      | 66617344/170498071 [00:03&lt;00:04, 22752792.01it/s]
 41%|      | 69238784/170498071 [00:03&lt;00:04, 23700266.29it/s]
 42%|     | 72122368/170498071 [00:03&lt;00:04, 21742157.34it/s]
 44%|     | 74383360/170498071 [00:03&lt;00:04, 21694347.84it/s]
 45%|     | 77037568/170498071 [00:03&lt;00:04, 22987188.65it/s]
 47%|     | 79691776/170498071 [00:04&lt;00:03, 23968397.53it/s]
 48%|     | 82444288/170498071 [00:04&lt;00:03, 24969491.04it/s]
 50%|     | 85000192/170498071 [00:04&lt;00:04, 20972072.41it/s]
 52%|    | 87916544/170498071 [00:04&lt;00:03, 23040881.72it/s]
 53%|    | 90669056/170498071 [00:04&lt;00:03, 21779773.74it/s]
 55%|    | 92962816/170498071 [00:04&lt;00:03, 21298237.55it/s]
 56%|    | 95223808/170498071 [00:04&lt;00:03, 21591417.34it/s]
 58%|    | 98041856/170498071 [00:04&lt;00:03, 23360106.37it/s]
 59%|    | 100761600/170498071 [00:04&lt;00:02, 24425038.78it/s]
 61%|    | 103251968/170498071 [00:05&lt;00:03, 21515878.24it/s]
 62%|   | 105512960/170498071 [00:05&lt;00:03, 21386879.35it/s]
 63%|   | 107970560/170498071 [00:05&lt;00:02, 22192961.66it/s]
 65%|   | 110788608/170498071 [00:05&lt;00:02, 23840756.84it/s]
 66%|   | 113311744/170498071 [00:05&lt;00:02, 22128381.07it/s]
 68%|   | 115605504/170498071 [00:05&lt;00:02, 22158915.21it/s]
 69%|   | 117866496/170498071 [00:05&lt;00:02, 21646303.13it/s]
 70%|   | 120061952/170498071 [00:05&lt;00:02, 21667477.74it/s]
 72%|  | 122847232/170498071 [00:05&lt;00:02, 23307109.95it/s]
 74%|  | 125566976/170498071 [00:06&lt;00:02, 22362548.75it/s]
 75%|  | 127926272/170498071 [00:06&lt;00:01, 22620793.50it/s]
 76%|  | 130220032/170498071 [00:06&lt;00:01, 22220263.14it/s]
 78%|  | 132481024/170498071 [00:06&lt;00:01, 21699726.85it/s]
 79%|  | 135168000/170498071 [00:06&lt;00:01, 23150806.60it/s]
 81%|  | 137854976/170498071 [00:06&lt;00:01, 22256185.19it/s]
 82%| | 140279808/170498071 [00:06&lt;00:01, 22721405.41it/s]
 84%| | 142573568/170498071 [00:06&lt;00:01, 22310664.99it/s]
 85%| | 144834560/170498071 [00:06&lt;00:01, 22033462.21it/s]
 86%| | 147062784/170498071 [00:07&lt;00:01, 21981825.19it/s]
 88%| | 149815296/170498071 [00:07&lt;00:00, 23427596.63it/s]
 89%| | 152174592/170498071 [00:07&lt;00:00, 21992940.27it/s]
 91%| | 154664960/170498071 [00:07&lt;00:00, 22609216.75it/s]
 92%|| 156958720/170498071 [00:07&lt;00:00, 22272476.34it/s]
 93%|| 159219712/170498071 [00:07&lt;00:00, 22028772.30it/s]
 95%|| 161480704/170498071 [00:07&lt;00:00, 22164199.77it/s]
 96%|| 164036608/170498071 [00:07&lt;00:00, 23110420.51it/s]
 98%|| 166363136/170498071 [00:07&lt;00:00, 21616590.25it/s]
 99%|| 168853504/170498071 [00:08&lt;00:00, 22523841.75it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20928341.58it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43
(train_cifar pid=3909) Files already downloaded and verified

Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:04:51. Total running time: 1hr 10min 8s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:05:21. Total running time: 1hr 10min 38s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:05:51. Total running time: 1hr 11min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:06:21. Total running time: 1hr 11min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.42377999424934387, time = 68.82069039344788
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:06:52. Total running time: 1hr 12min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:07:22. Total running time: 1hr 12min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:07:52. Total running time: 1hr 13min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.4830999970436096, time = 66.91194462776184
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:08:22. Total running time: 1hr 13min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:08:52. Total running time: 1hr 14min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:09:22. Total running time: 1hr 14min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.5298599600791931, time = 67.32850790023804
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:09:52. Total running time: 1hr 15min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:10:22. Total running time: 1hr 15min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:10:52. Total running time: 1hr 16min 9s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:11:22. Total running time: 1hr 16min 39s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.5626800060272217, time = 67.79275965690613
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:11:52. Total running time: 1hr 17min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:12:23. Total running time: 1hr 17min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING
Current time: 2023-10-31 17:12:53. Total running time: 1hr 18min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00008   RUNNING                0.001              8                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00009   PENDING                0.001             32                                          |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.5894399881362915, time = 68.66705393791199
(train_cifar pid=3909) train time = 506.94449043273926

Trial train_cifar_ce6f6_00008 completed after 1 iterations at 2023-10-31 17:13:13. Total running time: 1hr 18min 30s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00008 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 521.16806 |
| time_total_s                                     521.16806 |
| training_iteration                                       1 |
| accuracy                                           0.58944 |
+------------------------------------------------------------+

Trial train_cifar_ce6f6_00009 started with configuration:
+------------------------------------------------+
| Trial train_cifar_ce6f6_00009 config           |
+------------------------------------------------+
| batch_size                                  32 |
| learning_rate                            0.001 |
+------------------------------------------------+
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00008_8_batch_size=8,learning_rate=0.0010_2023-10-31_15-54-43/checkpoint_000000)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909)   0%|          | 0/170498071 [00:00&lt;?, ?it/s]
  0%|          | 65536/170498071 [00:00&lt;08:01, 353619.85it/s]
  0%|          | 229376/170498071 [00:00&lt;04:15, 666647.34it/s]
  1%|          | 917504/170498071 [00:00&lt;01:22, 2059248.40it/s]
  2%|         | 2686976/170498071 [00:00&lt;00:27, 6130577.27it/s]
  3%|         | 5373952/170498071 [00:00&lt;00:14, 11748726.54it/s]
  5%|         | 7864320/170498071 [00:00&lt;00:12, 12850168.67it/s]
  6%|         | 10682368/170498071 [00:01&lt;00:09, 16672401.06it/s]
  8%|         | 13434880/170498071 [00:01&lt;00:08, 19504725.39it/s]
  9%|         | 16089088/170498071 [00:01&lt;00:08, 17869413.49it/s]
 11%|         | 18644992/170498071 [00:01&lt;00:07, 19750394.28it/s]
 12%|        | 20905984/170498071 [00:01&lt;00:07, 20485877.74it/s]
 14%|        | 23396352/170498071 [00:01&lt;00:06, 21631590.02it/s]
 15%|        | 25722880/170498071 [00:01&lt;00:06, 22021057.60it/s]
 16%|        | 28016640/170498071 [00:01&lt;00:06, 22262444.81it/s]
 18%|        | 30310400/170498071 [00:01&lt;00:06, 22420356.19it/s]
 19%|        | 32604160/170498071 [00:02&lt;00:06, 19775429.76it/s]
 20%|        | 34865152/170498071 [00:02&lt;00:06, 20505874.30it/s]
 22%|       | 37191680/170498071 [00:02&lt;00:06, 21236598.43it/s]
 23%|       | 39419904/170498071 [00:02&lt;00:06, 21493101.45it/s]
 25%|       | 41844736/170498071 [00:02&lt;00:05, 22240237.14it/s]
 26%|       | 44105728/170498071 [00:02&lt;00:05, 22086064.34it/s]
 27%|       | 46366720/170498071 [00:02&lt;00:05, 22205633.13it/s]
 29%|       | 48627712/170498071 [00:02&lt;00:05, 22111374.09it/s]
 30%|       | 50855936/170498071 [00:02&lt;00:05, 20395518.94it/s]
 31%|       | 53084160/170498071 [00:02&lt;00:05, 20871937.53it/s]
 32%|      | 55214080/170498071 [00:03&lt;00:05, 20964288.40it/s]
 34%|      | 57507840/170498071 [00:03&lt;00:05, 21485354.47it/s]
 35%|      | 59834368/170498071 [00:03&lt;00:05, 21898294.70it/s]
 36%|      | 62226432/170498071 [00:03&lt;00:04, 22435279.58it/s]
 38%|      | 64520192/170498071 [00:03&lt;00:04, 22397453.81it/s]
 39%|      | 66781184/170498071 [00:03&lt;00:04, 22445840.78it/s]
 40%|      | 69042176/170498071 [00:03&lt;00:04, 22171862.25it/s]
 42%|     | 71335936/170498071 [00:03&lt;00:04, 22336013.54it/s]
 43%|     | 73596928/170498071 [00:03&lt;00:04, 20596893.57it/s]
 44%|     | 75825152/170498071 [00:04&lt;00:04, 21010612.21it/s]
 46%|     | 77987840/170498071 [00:04&lt;00:04, 21176061.94it/s]
 47%|     | 80248832/170498071 [00:04&lt;00:04, 21444456.96it/s]
 48%|     | 82509824/170498071 [00:04&lt;00:04, 21669137.31it/s]
 50%|     | 84803584/170498071 [00:04&lt;00:03, 21998277.38it/s]
 51%|     | 87031808/170498071 [00:04&lt;00:03, 21994612.23it/s]
 52%|    | 89358336/170498071 [00:04&lt;00:03, 22346146.02it/s]
 54%|    | 91619328/170498071 [00:04&lt;00:03, 22344373.82it/s]
 55%|    | 93880320/170498071 [00:04&lt;00:03, 21668811.17it/s]
 56%|    | 96206848/170498071 [00:04&lt;00:03, 22022511.24it/s]
 58%|    | 98435072/170498071 [00:05&lt;00:03, 21397504.90it/s]
 59%|    | 100663296/170498071 [00:05&lt;00:03, 21582123.29it/s]
 60%|    | 102924288/170498071 [00:05&lt;00:03, 21844894.53it/s]
 62%|   | 105316352/170498071 [00:05&lt;00:02, 22318129.01it/s]
 63%|   | 107577344/170498071 [00:05&lt;00:02, 22318644.75it/s]
 64%|   | 109838336/170498071 [00:05&lt;00:02, 22340196.41it/s]
 66%|   | 112099328/170498071 [00:05&lt;00:02, 22184402.16it/s]
 67%|   | 114327552/170498071 [00:05&lt;00:02, 21496748.24it/s]
 68%|   | 116523008/170498071 [00:05&lt;00:02, 21562131.63it/s]
 70%|   | 118718464/170498071 [00:05&lt;00:02, 21628121.36it/s]
 71%|   | 120946688/170498071 [00:06&lt;00:02, 21817917.60it/s]
 72%|  | 123142144/170498071 [00:06&lt;00:02, 21750978.62it/s]
 74%|  | 125337600/170498071 [00:06&lt;00:02, 21722475.83it/s]
 75%|  | 127533056/170498071 [00:06&lt;00:01, 21767365.90it/s]
 76%|  | 129761280/170498071 [00:06&lt;00:01, 21683953.86it/s]
 77%|  | 131956736/170498071 [00:06&lt;00:01, 21667175.20it/s]
 79%|  | 134184960/170498071 [00:06&lt;00:01, 21827517.20it/s]
 80%|  | 136445952/170498071 [00:06&lt;00:01, 21689376.14it/s]
 81%| | 138641408/170498071 [00:06&lt;00:01, 21554304.45it/s]
 83%| | 140869632/170498071 [00:07&lt;00:01, 21641762.75it/s]
 84%| | 143097856/170498071 [00:07&lt;00:01, 21762844.68it/s]
 85%| | 145358848/170498071 [00:07&lt;00:01, 21983352.82it/s]
 87%| | 147587072/170498071 [00:07&lt;00:01, 21896074.64it/s]
 88%| | 149782528/170498071 [00:07&lt;00:00, 21825676.58it/s]
 89%| | 151977984/170498071 [00:07&lt;00:00, 21690523.84it/s]
 90%| | 154173440/170498071 [00:07&lt;00:00, 20864522.98it/s]
 92%|| 156368896/170498071 [00:07&lt;00:00, 21178034.14it/s]
 93%|| 158629888/170498071 [00:07&lt;00:00, 21518891.27it/s]
 94%|| 160890880/170498071 [00:07&lt;00:00, 21658020.88it/s]
 96%|| 163151872/170498071 [00:08&lt;00:00, 21915841.79it/s]
 97%|| 165380096/170498071 [00:08&lt;00:00, 21969873.99it/s]
 98%|| 167641088/170498071 [00:08&lt;00:00, 21980070.34it/s]
100%|| 170498071/170498071 [00:08&lt;00:00, 20365777.70it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>(train_cifar pid=3909) Extracting /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/cifar-10-python.tar.gz to /root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:13:23. Total running time: 1hr 18min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Files already downloaded and verified
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:13:53. Total running time: 1hr 19min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[0]: accuracy = 0.350380003452301, time = 25.939082860946655
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:14:23. Total running time: 1hr 19min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[1]: accuracy = 0.41301998496055603, time = 25.695386171340942
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:14:53. Total running time: 1hr 20min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:15:23. Total running time: 1hr 20min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[2]: accuracy = 0.45009997487068176, time = 25.19807529449463
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:15:53. Total running time: 1hr 21min 10s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[3]: accuracy = 0.47495999932289124, time = 25.620651483535767
Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2023-10-31 17:16:23. Total running time: 1hr 21min 40s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00001 with accuracy=0.5552600026130676 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 16}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00009   RUNNING                0.001             32                                          |
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
+----------------------------------------------------------------------------------------------------------------+
(train_cifar pid=3909) Epoch[4]: accuracy = 0.4984799921512604, time = 25.44916582107544
(train_cifar pid=3909) train time = 197.71213483810425

Trial train_cifar_ce6f6_00009 completed after 1 iterations at 2023-10-31 17:16:43. Total running time: 1hr 22min 0s
+------------------------------------------------------------+
| Trial train_cifar_ce6f6_00009 result                       |
+------------------------------------------------------------+
| checkpoint_dir_name                      checkpoint_000000 |
| time_this_iter_s                                 210.39063 |
| time_total_s                                     210.39063 |
| training_iteration                                       1 |
| accuracy                                           0.49848 |
+------------------------------------------------------------+

Trial status: 10 TERMINATED
Current time: 2023-10-31 17:16:43. Total running time: 1hr 22min 0s
Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)
Current best trial: ce6f6_00009 with accuracy=0.4984799921512604 and params={&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 32}
+----------------------------------------------------------------------------------------------------------------+
| Trial name                status         learning_rate     batch_size     iter     total time (s)     accuracy |
+----------------------------------------------------------------------------------------------------------------+
| train_cifar_ce6f6_00000   TERMINATED             0.1                4        1            959.957      0.90646 |
| train_cifar_ce6f6_00001   TERMINATED             0.001             16        1            306.709      0.55526 |
| train_cifar_ce6f6_00002   TERMINATED             0.1               32        1            207.598      0.91994 |
| train_cifar_ce6f6_00003   TERMINATED             0.001              4        1            945.978      0.62188 |
| train_cifar_ce6f6_00004   TERMINATED             0.001              4        1            940.363      0.61778 |
| train_cifar_ce6f6_00005   TERMINATED             0.1               16        1            306.802      0.92626 |
| train_cifar_ce6f6_00006   TERMINATED             0.1               16        1            306.495      0.9262  |
| train_cifar_ce6f6_00007   TERMINATED             0.1               32        1            209.196      0.91406 |
| train_cifar_ce6f6_00008   TERMINATED             0.001              8        1            521.168      0.58944 |
| train_cifar_ce6f6_00009   TERMINATED             0.001             32        1            210.391      0.49848 |
+----------------------------------------------------------------------------------------------------------------+

Best trial config: {&#39;learning_rate&#39;: 0.001, &#39;batch_size&#39;: 32}
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>(train_cifar pid=3909) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2023-10-31_15-54-42/train_cifar_ce6f6_00009_9_batch_size=32,learning_rate=0.0010_2023-10-31_15-54-43/checkpoint_000000)
</code></pre>
</div>
<div class="output error" data-ename="AttributeError"
data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-6c7a25d68bfa&gt; in &lt;cell line: 33&gt;()
     32 print(&quot;Best trial config: {}&quot;.format(best_result .config))
     33 print(&quot;Best trial final validation accuracy: {}&quot;.format(
---&gt; 34     best_result .last_result[&quot;accuracy&quot;]))
     35 print(&quot;Best trial final validation accuracy: {}&quot;.format(
     36     best_result .last_result[&quot;accuracy&quot;]))

AttributeError: &#39;Result&#39; object has no attribute &#39;last_result&#39;
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="16"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="iHjugoqJF37p" data-outputId="69353edd-dc93-4b4b-f562-cd5425f5ae07">
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>dir_name <span class="op">=</span> os.getcwd()</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">True</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> dir_name, train <span class="op">=</span> <span class="va">False</span>, download <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> torchvision.transforms.ToTensor()</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of train samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(train_dataset)))</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>show_images(train_dataset, <span class="st">&#39;Train samples&#39;</span>)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of test samples: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(<span class="bu">len</span>(test_dataset)))</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>show_images(test_dataset, <span class="st">&#39;Test samples&#39;</span>)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>    test_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|| 170498071/170498071 [00:03&lt;00:00, 48707918.18it/s]
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting /content/cifar-10-python.tar.gz to /content
Files already downloaded and verified
Number of train samples: 50000
Number of test samples: 10000
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_34886d2bd42949ffa43c5041e94c6532/3127bb28c3d62b8eefc5e5b54343ce73394d0e10.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_34886d2bd42949ffa43c5041e94c6532/b794e07d0a5589cdfdb5489c4a96832e783400b2.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="17"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="rq2CeJz01Ym6" data-outputId="023d8c1a-dcfb-4996-c667-87c0b7ee6ee4">
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="17">
<pre><code>MySENet(
  (features): Sequential(
    (init_block): SEInitBlock(
      (conv1): ConvBlock(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv2): ConvBlock(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv3): ConvBlock(
        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (stage1): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage2): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage3): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (final_pool): AdaptiveAvgPool2d(output_size=1)
  )
  (output): Sequential(
    (dropout): Dropout(p=0.2, inplace=False)
    (fc): Linear(in_features=1024, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="18"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="LkQlqZeF1Pq4" data-outputId="b38b7d33-ce76-445c-a26d-e1dc18a09e8c">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6185199618339539, time = 30.925504207611084
Epoch[1]: accuracy = 0.7169399857521057, time = 22.91165590286255
Epoch[2]: accuracy = 0.8411200046539307, time = 23.687257289886475
Epoch[3]: accuracy = 0.8774999976158142, time = 23.178725004196167
Epoch[4]: accuracy = 0.9222999811172485, time = 23.38175058364868
Epoch[5]: accuracy = 0.9480599761009216, time = 23.5803005695343
Epoch[6]: accuracy = 0.9622799754142761, time = 23.58870840072632
Epoch[7]: accuracy = 0.9548400044441223, time = 23.31749963760376
Epoch[8]: accuracy = 0.971780002117157, time = 23.381746768951416
Epoch[9]: accuracy = 0.9805600047111511, time = 23.53693127632141
Epoch[10]: accuracy = 0.9869599938392639, time = 23.716327905654907
Epoch[11]: accuracy = 0.9875999689102173, time = 23.601375341415405
Epoch[12]: accuracy = 0.9893800020217896, time = 23.627362489700317
Epoch[13]: accuracy = 0.9886599779129028, time = 23.62673258781433
Epoch[14]: accuracy = 0.9909399747848511, time = 23.595299005508423
Epoch[15]: accuracy = 0.99263995885849, time = 24.11177349090576
Epoch[16]: accuracy = 0.9919999837875366, time = 23.65166664123535
Epoch[17]: accuracy = 0.9910399913787842, time = 23.771889686584473
Epoch[18]: accuracy = 0.9924999475479126, time = 23.617604970932007
Epoch[19]: accuracy = 0.9962999820709229, time = 23.714627504348755
train time = 750.3111162185669
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="19"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="DKvNV2IC1w6L" data-outputId="48b1b3a9-e74d-404c-8be1-642c49e57665">
<div class="sourceCode" id="cb88"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7657999992370605
</code></pre>
</div>
</div>
<div class="cell markdown" id="H0G35h49ZQD1">
<p>,       ().
  optimizer:</p>
</div>
<div class="cell code" data-execution_count="20"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="VYyGB5yEbYu0" data-outputId="50bf4342-8871-425e-ac0e-25d2bbd8fe55">
<div class="sourceCode" id="cb90"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr<span class="op">=</span>learning_rate, momentum<span class="op">=</span><span class="fl">0.9</span>, weight_decay<span class="op">=</span><span class="fl">5e-4</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.7418599724769592, time = 25.27243399620056
Epoch[1]: accuracy = 0.7615799903869629, time = 25.251456260681152
Epoch[2]: accuracy = 0.7592200040817261, time = 31.700586557388306
Epoch[3]: accuracy = 0.7815399765968323, time = 25.970283269882202
Epoch[4]: accuracy = 0.773099958896637, time = 25.522578239440918
Epoch[5]: accuracy = 0.7827000021934509, time = 26.07141399383545
Epoch[6]: accuracy = 0.7940799593925476, time = 25.713838815689087
Epoch[7]: accuracy = 0.7894399762153625, time = 25.81398034095764
Epoch[8]: accuracy = 0.7859599590301514, time = 25.73969268798828
Epoch[9]: accuracy = 0.8053999543190002, time = 25.93813681602478
Epoch[10]: accuracy = 0.7809999585151672, time = 25.848584175109863
Epoch[11]: accuracy = 0.7598199844360352, time = 25.881882190704346
Epoch[12]: accuracy = 0.7791000008583069, time = 25.79347586631775
Epoch[13]: accuracy = 0.7839799523353577, time = 25.950156211853027
Epoch[14]: accuracy = 0.7773999571800232, time = 26.057410955429077
Epoch[15]: accuracy = 0.8060599565505981, time = 25.96229338645935
Epoch[16]: accuracy = 0.7967000007629395, time = 25.80086874961853
Epoch[17]: accuracy = 0.7843199968338013, time = 25.855061531066895
Epoch[18]: accuracy = 0.8054400086402893, time = 26.51353907585144
Epoch[19]: accuracy = 0.7841199636459351, time = 25.706406116485596
train time = 809.5836207866669
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="21"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="II58EgxZd7BF" data-outputId="6dd28ea2-3771-48b2-dd85-65dcba3b21df">
<div class="sourceCode" id="cb92"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.7361999750137329
</code></pre>
</div>
</div>
<div class="cell markdown" id="ZXt73XeZbcqa">
<p> :</p>
</div>
<div class="cell code" data-execution_count="25" id="u8tBl0LzZPTW">
<div class="sourceCode" id="cb94"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicBlock(torch.nn.Module):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_planes, out_planes, stride, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BasicBlock, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm2d(in_planes)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span>stride,</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm2d(out_planes)</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv2d(out_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.droprate <span class="op">=</span> dropRate</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.equalInOut <span class="op">=</span> (in_planes <span class="op">==</span> out_planes)</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.convShortcut <span class="op">=</span> (<span class="kw">not</span> <span class="va">self</span>.equalInOut) <span class="kw">and</span> torch.nn.Conv2d(in_planes, out_planes, kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span>stride,</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>                               padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>) <span class="kw">or</span> <span class="va">None</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.equalInOut:</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.relu1(<span class="va">self</span>.bn1(x))</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.relu1(<span class="va">self</span>.bn1(x))</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu2(<span class="va">self</span>.bn2(<span class="va">self</span>.conv1(out <span class="cf">if</span> <span class="va">self</span>.equalInOut <span class="cf">else</span> x)))</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.droprate <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span><span class="va">self</span>.droprate)(out)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.add(x <span class="cf">if</span> <span class="va">self</span>.equalInOut <span class="cf">else</span> <span class="va">self</span>.convShortcut(x), out)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="26" id="3H2QKuNHZ7z_">
<div class="sourceCode" id="cb95"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyCNN(torch.nn.Module):</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, depth<span class="op">=</span><span class="dv">28</span>, widen_factor<span class="op">=</span><span class="dv">10</span>, num_classes<span class="op">=</span><span class="dv">10</span>, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MyCNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>        nChannels <span class="op">=</span> [<span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span>widen_factor, <span class="dv">32</span><span class="op">*</span>widen_factor, <span class="dv">64</span><span class="op">*</span>widen_factor]</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>((depth <span class="op">-</span> <span class="dv">4</span>) <span class="op">%</span> <span class="dv">6</span> <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> (depth <span class="op">-</span> <span class="dv">4</span>) <span class="op">//</span> <span class="dv">6</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>        block <span class="op">=</span> BasicBlock</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st conv before any network block</span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">3</span>, nChannels[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st block</span></span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">0</span>], nChannels[<span class="dv">1</span>], n, stride<span class="op">=</span><span class="dv">1</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd block</span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">1</span>], nChannels[<span class="dv">2</span>], n, stride<span class="op">=</span><span class="dv">2</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3rd block</span></span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3 <span class="op">=</span> <span class="va">self</span>._make_layer(block, nChannels[<span class="dv">2</span>], nChannels[<span class="dv">3</span>], n, stride<span class="op">=</span><span class="dv">2</span>, dropRate<span class="op">=</span>dropRate)</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># global average pooling and classifier</span></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm2d(nChannels[<span class="dv">3</span>])</span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> torch.nn.Linear(nChannels[<span class="dv">3</span>], num_classes)</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nChannels <span class="op">=</span> nChannels[<span class="dv">3</span>]</span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _make_layer(<span class="va">self</span>, block, in_planes, out_planes, nb_layers, stride, dropRate<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nb_layers):</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>            layers.append(block(i <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> in_planes <span class="kw">or</span> out_planes, out_planes, i <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> stride <span class="kw">or</span> <span class="dv">1</span>, dropRate))</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block1(out)</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block2(out)</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.block3(out)</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.bn1(out))</span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.nn.AvgPool2d(<span class="dv">8</span>)(out)</span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.nChannels)</span>
<span id="cb95-43"><a href="#cb95-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc(out)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="27"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Fra-pQZFZ3vp" data-outputId="3b16a9be-f2ab-49a5-ebea-372368ea2b77">
<div class="sourceCode" id="cb96"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>cnn_model <span class="op">=</span> MyCNN()</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="27">
<pre><code>MyCNN(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (block2): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (block3): Sequential(
    (0): BasicBlock(
      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): BasicBlock(
      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=640, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="28"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="JdGBwdbZbIZt" data-outputId="aafb91f7-61bc-47f6-d0dc-65a2aa88fb1e">
<div class="sourceCode" id="cb98"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.6646599769592285, time = 216.0471315383911
Epoch[1]: accuracy = 0.780019998550415, time = 224.1840682029724
Epoch[2]: accuracy = 0.8097800016403198, time = 224.2865493297577
Epoch[3]: accuracy = 0.8788599967956543, time = 224.24250078201294
Epoch[4]: accuracy = 0.8708999752998352, time = 224.06565022468567
Epoch[5]: accuracy = 0.9170599579811096, time = 223.96775150299072
Epoch[6]: accuracy = 0.943340003490448, time = 224.04663920402527
Epoch[7]: accuracy = 0.9487800002098083, time = 223.8983211517334
Epoch[8]: accuracy = 0.9468599557876587, time = 223.89434027671814
Epoch[9]: accuracy = 0.9655199646949768, time = 223.94078588485718
Epoch[10]: accuracy = 0.9796199798583984, time = 223.8154489994049
Epoch[11]: accuracy = 0.9723199605941772, time = 224.03929209709167
Epoch[12]: accuracy = 0.9829399585723877, time = 223.99121856689453
Epoch[13]: accuracy = 0.9848600029945374, time = 224.0068883895874
Epoch[14]: accuracy = 0.9912599921226501, time = 224.071857213974
Epoch[15]: accuracy = 0.9803199768066406, time = 224.23546528816223
Epoch[16]: accuracy = 0.9924799799919128, time = 224.06210255622864
Epoch[17]: accuracy = 0.9927999973297119, time = 224.21832489967346
Epoch[18]: accuracy = 0.9984399676322937, time = 224.24001669883728
Epoch[19]: accuracy = 0.9944799542427063, time = 224.643981218338
train time = 5946.710683584213
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="29"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="KGWbK-21HA7H" data-outputId="d0040274-5246-4b2e-f1b7-466c9d856f26">
<div class="sourceCode" id="cb100"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test accuracy: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(get_accuracy(test_data_loader, cnn_model)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Test accuracy: 0.8708999752998352
</code></pre>
</div>
</div>
<div class="cell markdown" id="3I5eUPFI5tM6">
<p>  - :</p>
</div>
<div class="cell code" data-execution_count="30"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ZAsANYm95wDw" data-outputId="c7820c30-e64a-432d-aabc-8aaccbfa0a9c">
<div class="sourceCode" id="cb102"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>cnn_model<span class="op">=</span>senet16()</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>cnn_model.to(device)</span></code></pre></div>
<div class="output execute_result" data-execution_count="30">
<pre><code>MySENet(
  (features): Sequential(
    (init_block): SEInitBlock(
      (conv1): ConvBlock(
        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv2): ConvBlock(
        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (conv3): ConvBlock(
        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activ): ReLU()
      )
      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (stage1): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage2): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (stage3): Sequential(
      (unit1): SENetUnit(
        (body): SENetBottleneck(
          (conv1): ConvBlock(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv2): ConvBlock(
            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activ): ReLU()
          )
          (conv3): ConvBlock(
            (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (conv1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))
          (activ): ReLU()
          (conv2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
          (sigmoid): Sigmoid()
        )
        (identity_conv): ConvBlock(
          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activ): ReLU()
      )
    )
    (final_pool): AdaptiveAvgPool2d(output_size=1)
  )
  (output): Sequential(
    (dropout): Dropout(p=0.2, inplace=False)
    (fc): Linear(in_features=1024, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="hZr2PDc25Vtk" data-outputId="41a707c3-af5d-41f4-dc8a-fc86e7bcb80a">
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(cnn_model.parameters(), lr <span class="op">=</span> learning_rate)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>start_all <span class="op">=</span> time.time()</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_data_loader):</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> cnn_model(images)</span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, labels)</span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb104-18"><a href="#cb104-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb104-19"><a href="#cb104-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb104-20"><a href="#cb104-20" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> time.time()</span>
<span id="cb104-21"><a href="#cb104-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Epoch[</span><span class="sc">{}</span><span class="st">]: accuracy = </span><span class="sc">{}</span><span class="st">, time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, get_accuracy(train_data_loader, cnn_model), (end <span class="op">-</span> start)))</span>
<span id="cb104-22"><a href="#cb104-22" aria-hidden="true" tabindex="-1"></a>end_all <span class="op">=</span> time.time()</span>
<span id="cb104-23"><a href="#cb104-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;train time = </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>((end_all <span class="op">-</span> start_all)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch[0]: accuracy = 0.661899983882904, time = 24.76333785057068
Epoch[1]: accuracy = 0.7913599610328674, time = 24.308416604995728
Epoch[2]: accuracy = 0.8473399877548218, time = 24.198498964309692
Epoch[3]: accuracy = 0.8970399498939514, time = 23.879033088684082
Epoch[4]: accuracy = 0.9146199822425842, time = 24.080710649490356
Epoch[5]: accuracy = 0.9434399604797363, time = 24.004873991012573
Epoch[6]: accuracy = 0.9443599581718445, time = 24.08707284927368
Epoch[7]: accuracy = 0.9714399576187134, time = 24.04041600227356
Epoch[8]: accuracy = 0.9749599695205688, time = 24.16765522956848
Epoch[9]: accuracy = 0.980959951877594, time = 24.123506546020508
Epoch[10]: accuracy = 0.9806399941444397, time = 24.156297206878662
Epoch[11]: accuracy = 0.9861399531364441, time = 23.972455263137817
Epoch[12]: accuracy = 0.9911399483680725, time = 23.984041213989258
Epoch[13]: accuracy = 0.9877399802207947, time = 24.09603476524353
Epoch[14]: accuracy = 0.9938199520111084, time = 23.980199098587036
Epoch[15]: accuracy = 0.9930399656295776, time = 24.030192852020264
Epoch[16]: accuracy = 0.9923399686813354, time = 24.097229957580566
Epoch[17]: accuracy = 0.9942799806594849, time = 23.979227781295776
Epoch[18]: accuracy = 0.9948599934577942, time = 24.116578578948975
Epoch[19]: accuracy = 0.9938599467277527, time = 24.081641912460327
Epoch[20]: accuracy = 0.9956799745559692, time = 23.87823987007141
Epoch[21]: accuracy = 0.992139995098114, time = 24.008548736572266
Epoch[22]: accuracy = 0.9973599910736084, time = 24.177753925323486
Epoch[23]: accuracy = 0.9977399706840515, time = 24.041317224502563
Epoch[24]: accuracy = 0.9973999857902527, time = 24.02409815788269
Epoch[25]: accuracy = 0.9967999458312988, time = 24.066431999206543
Epoch[26]: accuracy = 0.9965399503707886, time = 23.96722173690796
Epoch[27]: accuracy = 0.9968599677085876, time = 24.01838183403015
Epoch[28]: accuracy = 0.996999979019165, time = 24.040329456329346
Epoch[29]: accuracy = 0.9953999519348145, time = 23.96444272994995
Epoch[30]: accuracy = 0.9979199767112732, time = 23.938584566116333
Epoch[31]: accuracy = 0.9977200031280518, time = 24.18357491493225
Epoch[32]: accuracy = 0.9979999661445618, time = 24.19944477081299
Epoch[33]: accuracy = 0.9985399842262268, time = 24.302847862243652
Epoch[34]: accuracy = 0.9992199540138245, time = 24.077566862106323
Epoch[35]: accuracy = 0.9991599917411804, time = 24.090834617614746
Epoch[36]: accuracy = 0.9993199706077576, time = 24.122657299041748
Epoch[37]: accuracy = 0.9995599985122681, time = 23.940995454788208
Epoch[38]: accuracy = 0.999459981918335, time = 23.936484575271606
Epoch[39]: accuracy = 0.9990999698638916, time = 23.93556785583496
Epoch[40]: accuracy = 0.9996199607849121, time = 24.2277889251709
Epoch[41]: accuracy = 0.9996999502182007, time = 24.089961290359497
Epoch[42]: accuracy = 0.9997599720954895, time = 23.999568223953247
Epoch[43]: accuracy = 0.9998999834060669, time = 24.008137941360474
Epoch[44]: accuracy = 0.9999599456787109, time = 24.08919405937195
Epoch[45]: accuracy = 0.9999199509620667, time = 24.013184785842896
Epoch[46]: accuracy = 0.9991799592971802, time = 24.04884624481201
Epoch[47]: accuracy = 0.9998999834060669, time = 23.948622226715088
Epoch[48]: accuracy = 0.9999799728393555, time = 23.98257827758789
Epoch[49]: accuracy = 0.9999399781227112, time = 24.43471097946167
Epoch[50]: accuracy = 0.9998399615287781, time = 23.993987560272217
Epoch[51]: accuracy = 0.9998399615287781, time = 24.095747232437134
Epoch[52]: accuracy = 0.9997199773788452, time = 23.9491126537323
Epoch[53]: accuracy = 0.9998399615287781, time = 23.95765495300293
Epoch[54]: accuracy = 0.9999599456787109, time = 23.893803596496582
Epoch[55]: accuracy = 0.9997199773788452, time = 24.051599264144897
Epoch[56]: accuracy = 0.9999199509620667, time = 23.921846389770508
Epoch[57]: accuracy = 0.9997400045394897, time = 24.063940286636353
Epoch[58]: accuracy = 0.999779999256134, time = 24.11956763267517
Epoch[59]: accuracy = 0.9999199509620667, time = 23.87937569618225
Epoch[60]: accuracy = 0.9999599456787109, time = 23.92638659477234
Epoch[61]: accuracy = 0.9999399781227112, time = 23.94841694831848
Epoch[62]: accuracy = 0.9999199509620667, time = 23.882050275802612
Epoch[63]: accuracy = 0.9998399615287781, time = 23.955415725708008
Epoch[64]: accuracy = 0.9998199939727783, time = 24.055267810821533
Epoch[65]: accuracy = 0.9997999668121338, time = 23.94260835647583
Epoch[66]: accuracy = 0.9999399781227112, time = 23.943598985671997
Epoch[67]: accuracy = 0.9997999668121338, time = 24.132544994354248
Epoch[68]: accuracy = 0.9998799562454224, time = 23.932313919067383
Epoch[69]: accuracy = 0.9998599886894226, time = 23.983975410461426
Epoch[70]: accuracy = 0.9999399781227112, time = 24.012935400009155
Epoch[71]: accuracy = 0.9999199509620667, time = 24.03549098968506
Epoch[72]: accuracy = 0.9998599886894226, time = 23.903950214385986
Epoch[73]: accuracy = 0.9998799562454224, time = 23.932905435562134
Epoch[74]: accuracy = 0.9998799562454224, time = 23.90555453300476
Epoch[75]: accuracy = 0.9998999834060669, time = 23.97638487815857
Epoch[76]: accuracy = 0.9998599886894226, time = 23.798442363739014
Epoch[77]: accuracy = 0.9998199939727783, time = 23.880855798721313
Epoch[78]: accuracy = 0.9999799728393555, time = 23.990347385406494
Epoch[79]: accuracy = 0.9999399781227112, time = 24.015867471694946
Epoch[80]: accuracy = 0.999779999256134, time = 23.979984521865845
Epoch[81]: accuracy = 0.9998399615287781, time = 23.941874504089355
Epoch[82]: accuracy = 0.9998799562454224, time = 23.974444150924683
Epoch[83]: accuracy = 0.9999799728393555, time = 23.933456897735596
Epoch[84]: accuracy = 0.9999199509620667, time = 23.94545030593872
Epoch[85]: accuracy = 0.9999599456787109, time = 24.025662422180176
Epoch[86]: accuracy = 0.9982999563217163, time = 23.949324131011963
Epoch[87]: accuracy = 0.9997199773788452, time = 23.93287420272827
Epoch[88]: accuracy = 0.9999599456787109, time = 24.118873834609985
Epoch[89]: accuracy = 0.9998199939727783, time = 24.19706916809082
Epoch[90]: accuracy = 0.9998199939727783, time = 24.009126663208008
Epoch[91]: accuracy = 0.9998599886894226, time = 23.97919726371765
Epoch[92]: accuracy = 0.9998799562454224, time = 24.060304880142212
Epoch[93]: accuracy = 0.9999599456787109, time = 23.921614408493042
Epoch[94]: accuracy = 0.9999599456787109, time = 23.973186492919922
Epoch[95]: accuracy = 0.9999799728393555, time = 23.91923975944519
Epoch[96]: accuracy = 0.9999599456787109, time = 23.88985848426819
Epoch[97]: accuracy = 0.9999399781227112, time = 23.912118434906006
Epoch[98]: accuracy = 1.0, time = 23.96862483024597
Epoch[99]: accuracy = 0.9999199509620667, time = 23.993123054504395
Epoch[100]: accuracy = 0.9999199509620667, time = 24.015195846557617
Epoch[101]: accuracy = 0.9999599456787109, time = 23.975151300430298
Epoch[102]: accuracy = 0.9999799728393555, time = 24.8198664188385
Epoch[103]: accuracy = 0.9999199509620667, time = 24.30082416534424
Epoch[104]: accuracy = 0.9999399781227112, time = 24.915534019470215
Epoch[105]: accuracy = 0.9999799728393555, time = 24.032864809036255
Epoch[106]: accuracy = 0.9999799728393555, time = 23.93260145187378
Epoch[107]: accuracy = 0.9998999834060669, time = 24.07957148551941
Epoch[108]: accuracy = 0.9999599456787109, time = 24.100292205810547
Epoch[109]: accuracy = 0.9995399713516235, time = 24.76988697052002
Epoch[110]: accuracy = 0.9999599456787109, time = 24.038285493850708
Epoch[111]: accuracy = 0.9999399781227112, time = 24.056800365447998
Epoch[112]: accuracy = 0.9999599456787109, time = 24.194685220718384
Epoch[113]: accuracy = 0.9999799728393555, time = 24.054330825805664
Epoch[114]: accuracy = 0.9999799728393555, time = 24.898550033569336
Epoch[115]: accuracy = 1.0, time = 23.9685115814209
Epoch[116]: accuracy = 0.9999199509620667, time = 24.355198860168457
Epoch[117]: accuracy = 0.9999799728393555, time = 24.355379819869995
Epoch[118]: accuracy = 1.0, time = 24.208831787109375
Epoch[119]: accuracy = 0.9999599456787109, time = 23.98014998435974
Epoch[120]: accuracy = 0.9999399781227112, time = 24.024555444717407
Epoch[121]: accuracy = 0.9998999834060669, time = 24.109285593032837
Epoch[122]: accuracy = 0.9999599456787109, time = 23.983426570892334
Epoch[123]: accuracy = 1.0, time = 24.074538230895996
Epoch[124]: accuracy = 1.0, time = 24.85045313835144
Epoch[125]: accuracy = 0.9999799728393555, time = 24.029072523117065
Epoch[126]: accuracy = 0.9999399781227112, time = 23.963019371032715
Epoch[127]: accuracy = 1.0, time = 23.96269679069519
Epoch[128]: accuracy = 1.0, time = 23.983736753463745
Epoch[129]: accuracy = 1.0, time = 23.975033283233643
Epoch[130]: accuracy = 1.0, time = 24.156365156173706
Epoch[131]: accuracy = 1.0, time = 24.03429937362671
Epoch[132]: accuracy = 1.0, time = 24.05022621154785
Epoch[133]: accuracy = 0.9999799728393555, time = 23.976776361465454
Epoch[134]: accuracy = 0.9999799728393555, time = 24.00331425666809
Epoch[135]: accuracy = 0.9999799728393555, time = 24.042723417282104
Epoch[136]: accuracy = 1.0, time = 23.97308111190796
Epoch[137]: accuracy = 0.9999799728393555, time = 24.02052640914917
Epoch[138]: accuracy = 1.0, time = 23.9253089427948
Epoch[139]: accuracy = 0.9999799728393555, time = 23.960351943969727
Epoch[140]: accuracy = 1.0, time = 23.963252544403076
Epoch[141]: accuracy = 1.0, time = 23.948593854904175
Epoch[142]: accuracy = 1.0, time = 23.970805883407593
Epoch[143]: accuracy = 0.9999799728393555, time = 24.05373215675354
Epoch[144]: accuracy = 0.9999799728393555, time = 23.976948499679565
Epoch[145]: accuracy = 1.0, time = 24.012725591659546
Epoch[146]: accuracy = 1.0, time = 23.854395389556885
Epoch[147]: accuracy = 0.9999799728393555, time = 23.89655351638794
Epoch[148]: accuracy = 1.0, time = 24.188517808914185
Epoch[149]: accuracy = 1.0, time = 24.058602809906006
Epoch[150]: accuracy = 1.0, time = 23.98647665977478
Epoch[151]: accuracy = 1.0, time = 24.0036518573761
Epoch[152]: accuracy = 0.9999799728393555, time = 23.955833196640015
Epoch[153]: accuracy = 1.0, time = 24.70493793487549
Epoch[154]: accuracy = 1.0, time = 24.001108407974243
Epoch[155]: accuracy = 1.0, time = 23.965216875076294
Epoch[156]: accuracy = 1.0, time = 24.033264636993408
Epoch[157]: accuracy = 1.0, time = 24.06483006477356
Epoch[158]: accuracy = 1.0, time = 24.446363925933838
Epoch[159]: accuracy = 1.0, time = 24.39970111846924
Epoch[160]: accuracy = 1.0, time = 24.04589533805847
</code></pre>
</div>
</div>
<div class="cell code" id="DKMUE6YJ55S6">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
